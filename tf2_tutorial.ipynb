{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf2-start.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5lWdfkbocQx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "3e3ba9b6-9832-4701-da4c-cc09404fe28d"
      },
      "source": [
        "%tensorflow_version 2.x  #选择tensorflow 2.0\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: `1.x` or `2.x`.\n",
            "You set: `2.x  #选择tensorflow 2.0`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0-rc2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-eHKGpBo3PC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "91d912c4-dc10-4961-af08-8c242c43c953"
      },
      "source": [
        "print(\"GPU Available \", tf.test.is_gpu_available)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU Available  <function is_gpu_available at 0x7ff0a476f400>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q39i0_N9qIU4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "e5256c92-af6b-4b2c-bf37-77032b734045"
      },
      "source": [
        "!nvidia-smi  #gpu信息"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Oct  8 13:57:12 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.40       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlEwESpmqMxy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c45a1978-e2cb-43c6-8aed-950142bc0d96"
      },
      "source": [
        "A = tf.constant([[1,2],[3,4]])\n",
        "B = tf.constant([[5,6],[7,8]])\n",
        "C = tf.matmul(A,B)\n",
        "print(C)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[19 22]\n",
            " [43 50]], shape=(2, 2), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDDe7bF0q2Um",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "4c017fda-a2fc-4bd8-bcc5-624e7bfad901"
      },
      "source": [
        "import numpy as np\n",
        "random_float = tf.random.uniform(shape=())\n",
        "zero_vector = tf.zeros(shape=(2))\n",
        "\n",
        "print(A.shape)\n",
        "print(A.dtype)\n",
        "print(A.numpy())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 2)\n",
            "<dtype: 'int32'>\n",
            "[[1 2]\n",
            " [3 4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2kXm5oqsLDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "D = tf.add(A,B)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Orhr9pAqsVpU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "be106392-e06b-4fae-f25b-6c73cc8266e0"
      },
      "source": [
        "x = tf.Variable(initial_value=3.)\n",
        "with tf.GradientTape() as tape:   # 在tf.GradientTape()的上下文内，所有计算步骤都会被记录以用于求导\n",
        "    y = tf.square(x)\n",
        "y_grad = tape.gradient(y,x)     # 计算y=x^2关于x在x=3的导数\n",
        "print([y,y_grad])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor: id=23, shape=(), dtype=float32, numpy=9.0>, <tf.Tensor: id=27, shape=(), dtype=float32, numpy=6.0>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWocGuSDsz4Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "f40e90d2-e6f8-4faa-8924-59c01ba2e7b7"
      },
      "source": [
        "X = tf.constant([[1., 2.], [3., 4.]])\n",
        "y = tf.constant([[1.], [2.]])\n",
        "w = tf.Variable(initial_value=[[1.], [2.]])\n",
        "b = tf.Variable(initial_value=1.)\n",
        "with tf.GradientTape() as tape:\n",
        "    L = 0.5 * tf.reduce_sum(tf.square(tf.matmul(X, w) + b - y))\n",
        "w_grad, b_grad = tape.gradient(L, [w, b])        # 计算L(w,b)关于w,b的偏导数\n",
        "print([L.numpy(), w_grad.numpy(), b_grad.numpy()])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[62.5, array([[35.],\n",
            "       [50.]], dtype=float32), 15.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vplf8EHRwxTg",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "# **基础示例：线性回归**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWc9NxFFtllg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_raw = np.array([2013, 2014, 2015, 2016, 2017], dtype=np.float32)\n",
        "y_raw = np.array([12000, 14000, 15000, 16500, 17500], dtype=np.float32)\n",
        "\n",
        "X = (X_raw - X_raw.min()) / (X_raw.max() - X_raw.min())  #正则化x\n",
        "y = (y_raw - y_raw.min()) / (y_raw.max() - y_raw.min())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu2RrXatwrQf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "56996252-2c72-44de-f475-d139bf53a795"
      },
      "source": [
        "# numpy 版本\n",
        "a, b = 0, 0\n",
        "\n",
        "num_epoch = 10000\n",
        "learning_rate = 1e-3\n",
        "for e in range(num_epoch):\n",
        "    # 手动计算损失函数关于自变量（模型参数）的梯度\n",
        "    y_pred = a * X + b\n",
        "    grad_a, grad_b = (y_pred - y).dot(X), (y_pred - y).sum()\n",
        "\n",
        "    # 更新参数\n",
        "    a, b = a - learning_rate * grad_a, b - learning_rate * grad_b\n",
        "\n",
        "print(a, b)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9763702027872221 0.057564988311377796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRm1sxhHuM0T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "65eb72eb-1a53-4f5f-931c-ccea6223daa4"
      },
      "source": [
        "# tf2.0 版本\n",
        "X = tf.constant(X)\n",
        "y = tf.constant(y)\n",
        "\n",
        "a = tf.Variable(initial_value=0.)\n",
        "b = tf.Variable(initial_value=0.)\n",
        "variables = [a,b]\n",
        "\n",
        "num_epoch = 10000\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)     #tf.keras.optimizer\n",
        "for e in range(num_epoch):\n",
        "    # 使用tf.GradientTape()记录损失函数的梯度信息\n",
        "    with tf.GradientTape() as tape:  \n",
        "        y_pred = a*X + b   #以后会用model()类代替, tf.keras.Model, tf.keras.layers\n",
        "        loss = 0.5*tf.reduce_sum(tf.square(y_pred-y))    # tf.keras.losses\n",
        "    # TensorFlow自动计算损失函数关于自变量（模型参数）的梯度\n",
        "    grads = tape.gradient(loss, variables)\n",
        "    # TensorFlow自动根据梯度更新参数\n",
        "    optimizer.apply_gradients(grads_and_vars=zip(grads,variables))\n",
        "\n",
        "print(a,b)\n",
        "#模型的评估 tf.keras.metrics"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.97637> <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.057565063>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRECMhJ1y_M2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()     # Python 2 下使用 super(MyModel, self).__init__()\n",
        "        # 此处添加初始化代码（包含 call 方法中会用到的层），例如\n",
        "        # layer1 = tf.keras.layers.BuiltInLayer(...)\n",
        "        # layer2 = MyCustomLayer(...)\n",
        "\n",
        "    def call(self, input):\n",
        "        # 此处添加模型调用的代码（处理输入并返回输出），例如\n",
        "        # x = layer1(input)\n",
        "        # output = layer2(x)\n",
        "        return output\n",
        "\n",
        "    # 还可以添加自定义的方法"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUqULK-yzOKa",
        "colab_type": "text"
      },
      "source": [
        "![Keras模型类定义示意图](https://tf.wiki/_images/model.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EUERpvGz-pD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "abdc0b99-b0da-4b27-ffd6-c754cf5b422e"
      },
      "source": [
        "# 通过模型类的方式改写前一个代码\n",
        "X = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "y = tf.constant([[10.0], [20.0]])\n",
        "\n",
        "class Linear(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.dense = tf.keras.layers.Dense(  #全连接层\n",
        "            units=1,   #units ：输出张量的维度\n",
        "            activation=None,  #activation：激活函数，默认为无激活函数。常用的激活函数包括tf.nn.relu、tf.nn.tanh和tf.nn.sigmoid\n",
        "            kernel_initializer=tf.zeros_initializer(),  # W kernel_initializer 、 bias_initializer ：权重矩阵kernel和偏置向量bias两个变量的初始化器。默认为tf.glorot_uniform_initializer。设置为tf.zeros_initializer表示将两个变量均初始化为全0\n",
        "            bias_initializer=tf.zeros_initializer()   # b use_bias：是否加入偏置向量bias,默认为True \n",
        "        )\n",
        "\n",
        "    def call(self,input):\n",
        "        output = self.dense(input)\n",
        "        return output\n",
        "\n",
        "# 以下代码结构与前节类似\n",
        "model = Linear()   #实例化网络Linear\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)  #确定optimizer优化器\n",
        "for i in range(100):\n",
        "    with tf.GradientTape() as tape:      #打开运算记录\n",
        "        y_pred = model(X)      # 调用模型 y_pred = model(X) 而不是显式写出 y_pred = a * X + b\n",
        "        loss = tf.reduce_mean(tf.square(y_pred - y))\n",
        "    grads = tape.gradient(loss, model.variables)    # 使用 model.variables 这一属性直接获得模型中的所有变量\n",
        "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
        "print(model.variables)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Variable 'linear/dense/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
            "array([[0.40784496],\n",
            "       [1.191065  ],\n",
            "       [1.9742855 ]], dtype=float32)>, <tf.Variable 'linear/dense/bias:0' shape=(1,) dtype=float32, numpy=array([0.78322077], dtype=float32)>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wDpfC59b4nZ-"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "# **基础示例：多层感知机（MLP）**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FD3Ebwk1471w",
        "colab_type": "text"
      },
      "source": [
        "多层感知机或多层全连接神经网络 流程：\n",
        "1. 使用 **tf.keras.datasets** 获得数据集并预处理\n",
        "\n",
        "2. 使用 **tf.keras.Model** 和 **tf.keras.layers** 构建模型\n",
        "\n",
        "3. 构建模型训练流程，使用 **tf.keras.losses** 计算损失函数，并使用 **tf.keras.optimizer** 优化模型\n",
        "\n",
        "4. 构建模型评估流程，使用 **tf.keras.metrics** 计算评估指标"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2lAyQNp50S1",
        "colab_type": "text"
      },
      "source": [
        "## 数据获取及预处理： **tf.keras.datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zLqU3_A56Cr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNISTLoader():\n",
        "    def __init__(self):\n",
        "        mnist=tf.keras.datasets.mnist\n",
        "        (self.train_data,self.train_label),(self.test_data,self.test_label)=mnist.load_data()\n",
        "        # MNIST中的图像默认为uint8（0-255的数字）。以下代码将其归一化到0-1之间的浮点数，并在最后增加一维作为颜色通道\n",
        "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]  np.expand_dims(axis=-1)在最后增加一个维度\n",
        "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
        "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
        "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
        "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
        "\n",
        "    def get_batch(self,batch_size):\n",
        "       # 从数据集中随机取出batch_size个元素并返回\n",
        "        index = np.random.randint(0,np.shape(self.train_data)[0],batch_size)\n",
        "        return self.train_data[index,:],self.train_label[index] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VbZ0Eynt9aF1"
      },
      "source": [
        "## 模型的构建： tf.keras.Model 和 tf.keras.layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Clik7Co49d9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = tf.keras.layers.Flatten() # Flatten层将除第一维（batch_size）以外的维度展平\n",
        "        self.dense1 = tf.keras.layers.Dense(units=100,activation=tf.nn.relu)\n",
        "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
        "\n",
        "    def call(self,inputs):          # [batch_size, 28, 28, 1]\n",
        "        x = self.flatten(inputs)    # [batch_size, 784]\n",
        "        x = self.dense1(x)       # [batch_size, 100]\n",
        "        x = self.dense2(x)\n",
        "        output = tf.nn.softmax(x)    # [batch_size, 10]\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kBDBoiNT-9EE"
      },
      "source": [
        "## 模型的训练： tf.keras.losses 和 tf.keras.optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcQ992Te_CJS",
        "colab_type": "text"
      },
      "source": [
        "先定义模型超参数：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6HGC0Ye_AEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 5\n",
        "batch_size = 50\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZjdEm3V_Sz3",
        "colab_type": "text"
      },
      "source": [
        "实例化模型和数据读取类，并实例化一个 tf.keras.optimizer 的优化器（这里使用常用的 Adam 优化器）："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ChwuqY__PdZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "6ff13872-915b-47c3-8319-d6f7d8caf4f4"
      },
      "source": [
        "model = MLP()\n",
        "data_loader = MNISTLoader()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9T_bWmo_dSD",
        "colab_type": "text"
      },
      "source": [
        "然后迭代进行以下步骤：\n",
        "\n",
        "1. 从 DataLoader 中随机取一批训练数据；\n",
        "\n",
        "2. 将这批数据送入模型，计算出模型的预测值；\n",
        "\n",
        "3. 将模型预测值与真实值进行比较，计算损失函数（loss）。这里使用 tf.keras.losses 中的交叉熵函数作为损失函数；\n",
        "\n",
        "4. 计算损失函数关于模型变量的导数；\n",
        "\n",
        "5. 将求出的导数值传入优化器，使用优化器的 apply_gradients 方法更新模型参数以最小化损失函数。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiQFfI1N_lJ2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5fae8fa7-bcf5-4dae-b63b-cd663b2ede35"
      },
      "source": [
        "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
        "for batch_index in range(num_batches):\n",
        "    X,y=data_loader.get_batch(batch_size)\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = model(X)\n",
        "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y,y_pred=y_pred)  #sparse_categorical_crossentropy可直接传入标签，不用one-hot\n",
        "        loss = tf.reduce_mean(loss)\n",
        "        print(\"batch %d: loss %f\" % (batch_index,loss.numpy()))\n",
        "    grads = tape.gradient(loss,model.variables)\n",
        "    optimizer.apply_gradients(grads_and_vars=zip(grads,model.variables))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch 0: loss 2.250197\n",
            "WARNING:tensorflow:From /tensorflow-2.0.0-rc2/python3.6/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "batch 1: loss 2.288867\n",
            "batch 2: loss 2.222971\n",
            "batch 3: loss 2.150999\n",
            "batch 4: loss 1.906977\n",
            "batch 5: loss 2.006940\n",
            "batch 6: loss 1.782133\n",
            "batch 7: loss 1.877084\n",
            "batch 8: loss 1.842059\n",
            "batch 9: loss 1.605330\n",
            "batch 10: loss 1.647323\n",
            "batch 11: loss 1.551296\n",
            "batch 12: loss 1.536149\n",
            "batch 13: loss 1.481353\n",
            "batch 14: loss 1.503345\n",
            "batch 15: loss 1.419806\n",
            "batch 16: loss 1.449546\n",
            "batch 17: loss 1.210968\n",
            "batch 18: loss 1.265529\n",
            "batch 19: loss 1.255735\n",
            "batch 20: loss 1.198387\n",
            "batch 21: loss 1.225810\n",
            "batch 22: loss 0.989654\n",
            "batch 23: loss 0.973528\n",
            "batch 24: loss 0.984867\n",
            "batch 25: loss 0.937330\n",
            "batch 26: loss 1.048070\n",
            "batch 27: loss 0.929489\n",
            "batch 28: loss 0.896890\n",
            "batch 29: loss 0.941883\n",
            "batch 30: loss 0.878296\n",
            "batch 31: loss 0.837311\n",
            "batch 32: loss 0.770499\n",
            "batch 33: loss 0.804016\n",
            "batch 34: loss 0.700481\n",
            "batch 35: loss 0.727899\n",
            "batch 36: loss 0.695580\n",
            "batch 37: loss 0.989866\n",
            "batch 38: loss 0.671808\n",
            "batch 39: loss 0.588949\n",
            "batch 40: loss 0.659633\n",
            "batch 41: loss 0.800832\n",
            "batch 42: loss 0.794211\n",
            "batch 43: loss 0.691123\n",
            "batch 44: loss 0.551466\n",
            "batch 45: loss 0.646474\n",
            "batch 46: loss 0.576146\n",
            "batch 47: loss 0.594046\n",
            "batch 48: loss 0.540733\n",
            "batch 49: loss 0.561436\n",
            "batch 50: loss 0.369208\n",
            "batch 51: loss 0.473605\n",
            "batch 52: loss 0.552509\n",
            "batch 53: loss 0.443998\n",
            "batch 54: loss 0.429206\n",
            "batch 55: loss 0.460023\n",
            "batch 56: loss 0.603105\n",
            "batch 57: loss 0.734279\n",
            "batch 58: loss 0.510993\n",
            "batch 59: loss 0.477554\n",
            "batch 60: loss 0.588037\n",
            "batch 61: loss 0.366301\n",
            "batch 62: loss 0.611876\n",
            "batch 63: loss 0.403604\n",
            "batch 64: loss 0.579983\n",
            "batch 65: loss 0.460113\n",
            "batch 66: loss 0.510953\n",
            "batch 67: loss 0.505537\n",
            "batch 68: loss 0.645046\n",
            "batch 69: loss 0.464092\n",
            "batch 70: loss 0.548796\n",
            "batch 71: loss 0.547765\n",
            "batch 72: loss 0.607104\n",
            "batch 73: loss 0.338362\n",
            "batch 74: loss 0.469795\n",
            "batch 75: loss 0.471706\n",
            "batch 76: loss 0.452555\n",
            "batch 77: loss 0.388313\n",
            "batch 78: loss 0.555439\n",
            "batch 79: loss 0.482819\n",
            "batch 80: loss 0.397293\n",
            "batch 81: loss 0.437807\n",
            "batch 82: loss 0.589094\n",
            "batch 83: loss 0.381020\n",
            "batch 84: loss 0.441991\n",
            "batch 85: loss 0.523433\n",
            "batch 86: loss 0.560957\n",
            "batch 87: loss 0.522602\n",
            "batch 88: loss 0.396904\n",
            "batch 89: loss 0.407523\n",
            "batch 90: loss 0.502266\n",
            "batch 91: loss 0.544426\n",
            "batch 92: loss 0.290426\n",
            "batch 93: loss 0.391888\n",
            "batch 94: loss 0.515262\n",
            "batch 95: loss 0.367053\n",
            "batch 96: loss 0.487902\n",
            "batch 97: loss 0.329605\n",
            "batch 98: loss 0.342016\n",
            "batch 99: loss 0.526827\n",
            "batch 100: loss 0.360527\n",
            "batch 101: loss 0.385593\n",
            "batch 102: loss 0.408452\n",
            "batch 103: loss 0.383166\n",
            "batch 104: loss 0.357928\n",
            "batch 105: loss 0.454748\n",
            "batch 106: loss 0.634357\n",
            "batch 107: loss 0.435593\n",
            "batch 108: loss 0.458359\n",
            "batch 109: loss 0.529903\n",
            "batch 110: loss 0.407514\n",
            "batch 111: loss 0.361922\n",
            "batch 112: loss 0.567349\n",
            "batch 113: loss 0.430903\n",
            "batch 114: loss 0.383382\n",
            "batch 115: loss 0.426604\n",
            "batch 116: loss 0.601912\n",
            "batch 117: loss 0.396782\n",
            "batch 118: loss 0.397341\n",
            "batch 119: loss 0.459583\n",
            "batch 120: loss 0.466117\n",
            "batch 121: loss 0.494174\n",
            "batch 122: loss 0.347109\n",
            "batch 123: loss 0.305208\n",
            "batch 124: loss 0.562704\n",
            "batch 125: loss 0.314398\n",
            "batch 126: loss 0.387516\n",
            "batch 127: loss 0.281353\n",
            "batch 128: loss 0.290138\n",
            "batch 129: loss 0.592880\n",
            "batch 130: loss 0.349365\n",
            "batch 131: loss 0.376780\n",
            "batch 132: loss 0.397335\n",
            "batch 133: loss 0.426619\n",
            "batch 134: loss 0.290622\n",
            "batch 135: loss 0.639205\n",
            "batch 136: loss 0.537738\n",
            "batch 137: loss 0.287154\n",
            "batch 138: loss 0.280990\n",
            "batch 139: loss 0.355827\n",
            "batch 140: loss 0.307789\n",
            "batch 141: loss 0.266764\n",
            "batch 142: loss 0.483194\n",
            "batch 143: loss 0.327415\n",
            "batch 144: loss 0.274761\n",
            "batch 145: loss 0.527389\n",
            "batch 146: loss 0.287476\n",
            "batch 147: loss 0.485065\n",
            "batch 148: loss 0.321757\n",
            "batch 149: loss 0.363418\n",
            "batch 150: loss 0.317250\n",
            "batch 151: loss 0.496820\n",
            "batch 152: loss 0.122006\n",
            "batch 153: loss 0.212909\n",
            "batch 154: loss 0.287095\n",
            "batch 155: loss 0.351618\n",
            "batch 156: loss 0.342608\n",
            "batch 157: loss 0.431896\n",
            "batch 158: loss 0.294114\n",
            "batch 159: loss 0.682711\n",
            "batch 160: loss 0.457727\n",
            "batch 161: loss 0.302648\n",
            "batch 162: loss 0.398060\n",
            "batch 163: loss 0.192811\n",
            "batch 164: loss 0.407293\n",
            "batch 165: loss 0.417796\n",
            "batch 166: loss 0.304419\n",
            "batch 167: loss 0.470441\n",
            "batch 168: loss 0.344541\n",
            "batch 169: loss 0.281057\n",
            "batch 170: loss 0.402634\n",
            "batch 171: loss 0.246429\n",
            "batch 172: loss 0.204663\n",
            "batch 173: loss 0.495869\n",
            "batch 174: loss 0.405905\n",
            "batch 175: loss 0.299471\n",
            "batch 176: loss 0.295806\n",
            "batch 177: loss 0.298712\n",
            "batch 178: loss 0.341095\n",
            "batch 179: loss 0.449552\n",
            "batch 180: loss 0.339423\n",
            "batch 181: loss 0.330928\n",
            "batch 182: loss 0.240478\n",
            "batch 183: loss 0.451126\n",
            "batch 184: loss 0.325376\n",
            "batch 185: loss 0.336057\n",
            "batch 186: loss 0.361763\n",
            "batch 187: loss 0.236545\n",
            "batch 188: loss 0.345095\n",
            "batch 189: loss 0.202438\n",
            "batch 190: loss 0.264667\n",
            "batch 191: loss 0.335533\n",
            "batch 192: loss 0.280089\n",
            "batch 193: loss 0.318445\n",
            "batch 194: loss 0.393123\n",
            "batch 195: loss 0.569621\n",
            "batch 196: loss 0.210500\n",
            "batch 197: loss 0.156479\n",
            "batch 198: loss 0.449270\n",
            "batch 199: loss 0.287521\n",
            "batch 200: loss 0.361568\n",
            "batch 201: loss 0.293605\n",
            "batch 202: loss 0.229653\n",
            "batch 203: loss 0.260040\n",
            "batch 204: loss 0.258991\n",
            "batch 205: loss 0.354428\n",
            "batch 206: loss 0.333460\n",
            "batch 207: loss 0.258771\n",
            "batch 208: loss 0.125828\n",
            "batch 209: loss 0.353402\n",
            "batch 210: loss 0.303130\n",
            "batch 211: loss 0.258243\n",
            "batch 212: loss 0.283650\n",
            "batch 213: loss 0.209427\n",
            "batch 214: loss 0.442324\n",
            "batch 215: loss 0.496523\n",
            "batch 216: loss 0.405684\n",
            "batch 217: loss 0.349280\n",
            "batch 218: loss 0.353222\n",
            "batch 219: loss 0.468967\n",
            "batch 220: loss 0.455260\n",
            "batch 221: loss 0.454630\n",
            "batch 222: loss 0.226357\n",
            "batch 223: loss 0.233544\n",
            "batch 224: loss 0.294652\n",
            "batch 225: loss 0.229744\n",
            "batch 226: loss 0.259731\n",
            "batch 227: loss 0.499682\n",
            "batch 228: loss 0.353606\n",
            "batch 229: loss 0.167988\n",
            "batch 230: loss 0.306989\n",
            "batch 231: loss 0.472436\n",
            "batch 232: loss 0.169532\n",
            "batch 233: loss 0.267873\n",
            "batch 234: loss 0.243215\n",
            "batch 235: loss 0.416866\n",
            "batch 236: loss 0.273967\n",
            "batch 237: loss 0.253798\n",
            "batch 238: loss 0.409259\n",
            "batch 239: loss 0.342801\n",
            "batch 240: loss 0.167411\n",
            "batch 241: loss 0.432125\n",
            "batch 242: loss 0.404199\n",
            "batch 243: loss 0.196751\n",
            "batch 244: loss 0.382209\n",
            "batch 245: loss 0.224113\n",
            "batch 246: loss 0.427099\n",
            "batch 247: loss 0.329859\n",
            "batch 248: loss 0.497846\n",
            "batch 249: loss 0.413644\n",
            "batch 250: loss 0.186321\n",
            "batch 251: loss 0.497105\n",
            "batch 252: loss 0.441566\n",
            "batch 253: loss 0.281966\n",
            "batch 254: loss 0.499296\n",
            "batch 255: loss 0.145136\n",
            "batch 256: loss 0.215259\n",
            "batch 257: loss 0.213317\n",
            "batch 258: loss 0.288272\n",
            "batch 259: loss 0.444212\n",
            "batch 260: loss 0.172491\n",
            "batch 261: loss 0.209606\n",
            "batch 262: loss 0.369121\n",
            "batch 263: loss 0.476560\n",
            "batch 264: loss 0.425417\n",
            "batch 265: loss 0.412647\n",
            "batch 266: loss 0.234882\n",
            "batch 267: loss 0.220116\n",
            "batch 268: loss 0.189102\n",
            "batch 269: loss 0.262154\n",
            "batch 270: loss 0.166817\n",
            "batch 271: loss 0.225982\n",
            "batch 272: loss 0.374995\n",
            "batch 273: loss 0.243478\n",
            "batch 274: loss 0.275183\n",
            "batch 275: loss 0.257161\n",
            "batch 276: loss 0.119638\n",
            "batch 277: loss 0.257383\n",
            "batch 278: loss 0.360852\n",
            "batch 279: loss 0.185448\n",
            "batch 280: loss 0.178830\n",
            "batch 281: loss 0.221785\n",
            "batch 282: loss 0.303138\n",
            "batch 283: loss 0.544016\n",
            "batch 284: loss 0.295914\n",
            "batch 285: loss 0.182670\n",
            "batch 286: loss 0.165231\n",
            "batch 287: loss 0.305226\n",
            "batch 288: loss 0.267676\n",
            "batch 289: loss 0.307569\n",
            "batch 290: loss 0.327101\n",
            "batch 291: loss 0.376622\n",
            "batch 292: loss 0.460938\n",
            "batch 293: loss 0.293361\n",
            "batch 294: loss 0.455360\n",
            "batch 295: loss 0.147525\n",
            "batch 296: loss 0.375416\n",
            "batch 297: loss 0.288806\n",
            "batch 298: loss 0.373658\n",
            "batch 299: loss 0.251483\n",
            "batch 300: loss 0.242272\n",
            "batch 301: loss 0.321313\n",
            "batch 302: loss 0.420968\n",
            "batch 303: loss 0.189504\n",
            "batch 304: loss 0.232260\n",
            "batch 305: loss 0.268549\n",
            "batch 306: loss 0.365848\n",
            "batch 307: loss 0.371013\n",
            "batch 308: loss 0.456815\n",
            "batch 309: loss 0.283328\n",
            "batch 310: loss 0.496764\n",
            "batch 311: loss 0.335366\n",
            "batch 312: loss 0.201837\n",
            "batch 313: loss 0.487623\n",
            "batch 314: loss 0.261117\n",
            "batch 315: loss 0.210584\n",
            "batch 316: loss 0.350794\n",
            "batch 317: loss 0.325624\n",
            "batch 318: loss 0.240245\n",
            "batch 319: loss 0.296168\n",
            "batch 320: loss 0.173931\n",
            "batch 321: loss 0.242761\n",
            "batch 322: loss 0.171181\n",
            "batch 323: loss 0.221276\n",
            "batch 324: loss 0.215493\n",
            "batch 325: loss 0.305310\n",
            "batch 326: loss 0.185537\n",
            "batch 327: loss 0.224580\n",
            "batch 328: loss 0.162139\n",
            "batch 329: loss 0.254471\n",
            "batch 330: loss 0.372947\n",
            "batch 331: loss 0.415734\n",
            "batch 332: loss 0.215947\n",
            "batch 333: loss 0.234269\n",
            "batch 334: loss 0.176725\n",
            "batch 335: loss 0.292376\n",
            "batch 336: loss 0.289867\n",
            "batch 337: loss 0.320060\n",
            "batch 338: loss 0.459823\n",
            "batch 339: loss 0.398419\n",
            "batch 340: loss 0.270018\n",
            "batch 341: loss 0.220965\n",
            "batch 342: loss 0.388010\n",
            "batch 343: loss 0.105387\n",
            "batch 344: loss 0.186957\n",
            "batch 345: loss 0.271660\n",
            "batch 346: loss 0.307212\n",
            "batch 347: loss 0.107845\n",
            "batch 348: loss 0.080421\n",
            "batch 349: loss 0.439557\n",
            "batch 350: loss 0.332011\n",
            "batch 351: loss 0.178883\n",
            "batch 352: loss 0.386304\n",
            "batch 353: loss 0.107487\n",
            "batch 354: loss 0.387627\n",
            "batch 355: loss 0.241339\n",
            "batch 356: loss 0.256858\n",
            "batch 357: loss 0.288507\n",
            "batch 358: loss 0.285969\n",
            "batch 359: loss 0.329570\n",
            "batch 360: loss 0.261123\n",
            "batch 361: loss 0.295965\n",
            "batch 362: loss 0.271945\n",
            "batch 363: loss 0.254890\n",
            "batch 364: loss 0.281269\n",
            "batch 365: loss 0.154434\n",
            "batch 366: loss 0.147291\n",
            "batch 367: loss 0.427885\n",
            "batch 368: loss 0.184579\n",
            "batch 369: loss 0.120859\n",
            "batch 370: loss 0.260642\n",
            "batch 371: loss 0.260232\n",
            "batch 372: loss 0.312281\n",
            "batch 373: loss 0.417220\n",
            "batch 374: loss 0.301246\n",
            "batch 375: loss 0.181746\n",
            "batch 376: loss 0.348541\n",
            "batch 377: loss 0.225612\n",
            "batch 378: loss 0.280525\n",
            "batch 379: loss 0.188654\n",
            "batch 380: loss 0.314465\n",
            "batch 381: loss 0.212442\n",
            "batch 382: loss 0.281520\n",
            "batch 383: loss 0.474205\n",
            "batch 384: loss 0.414244\n",
            "batch 385: loss 0.230221\n",
            "batch 386: loss 0.367502\n",
            "batch 387: loss 0.134498\n",
            "batch 388: loss 0.284329\n",
            "batch 389: loss 0.386723\n",
            "batch 390: loss 0.237700\n",
            "batch 391: loss 0.280173\n",
            "batch 392: loss 0.366536\n",
            "batch 393: loss 0.220089\n",
            "batch 394: loss 0.329564\n",
            "batch 395: loss 0.353836\n",
            "batch 396: loss 0.190452\n",
            "batch 397: loss 0.388348\n",
            "batch 398: loss 0.163265\n",
            "batch 399: loss 0.213002\n",
            "batch 400: loss 0.283459\n",
            "batch 401: loss 0.158587\n",
            "batch 402: loss 0.184633\n",
            "batch 403: loss 0.231196\n",
            "batch 404: loss 0.307980\n",
            "batch 405: loss 0.381172\n",
            "batch 406: loss 0.288776\n",
            "batch 407: loss 0.283546\n",
            "batch 408: loss 0.249135\n",
            "batch 409: loss 0.149677\n",
            "batch 410: loss 0.334628\n",
            "batch 411: loss 0.215136\n",
            "batch 412: loss 0.207717\n",
            "batch 413: loss 0.266311\n",
            "batch 414: loss 0.319985\n",
            "batch 415: loss 0.169348\n",
            "batch 416: loss 0.226312\n",
            "batch 417: loss 0.141975\n",
            "batch 418: loss 0.261025\n",
            "batch 419: loss 0.195245\n",
            "batch 420: loss 0.224550\n",
            "batch 421: loss 0.171846\n",
            "batch 422: loss 0.179641\n",
            "batch 423: loss 0.128537\n",
            "batch 424: loss 0.167334\n",
            "batch 425: loss 0.427376\n",
            "batch 426: loss 0.349874\n",
            "batch 427: loss 0.239323\n",
            "batch 428: loss 0.241521\n",
            "batch 429: loss 0.267820\n",
            "batch 430: loss 0.407492\n",
            "batch 431: loss 0.198836\n",
            "batch 432: loss 0.271213\n",
            "batch 433: loss 0.367690\n",
            "batch 434: loss 0.458007\n",
            "batch 435: loss 0.185152\n",
            "batch 436: loss 0.181457\n",
            "batch 437: loss 0.154877\n",
            "batch 438: loss 0.198742\n",
            "batch 439: loss 0.392191\n",
            "batch 440: loss 0.277081\n",
            "batch 441: loss 0.204015\n",
            "batch 442: loss 0.092539\n",
            "batch 443: loss 0.314983\n",
            "batch 444: loss 0.225413\n",
            "batch 445: loss 0.245883\n",
            "batch 446: loss 0.183003\n",
            "batch 447: loss 0.515333\n",
            "batch 448: loss 0.088672\n",
            "batch 449: loss 0.203080\n",
            "batch 450: loss 0.210837\n",
            "batch 451: loss 0.414338\n",
            "batch 452: loss 0.164552\n",
            "batch 453: loss 0.122535\n",
            "batch 454: loss 0.404682\n",
            "batch 455: loss 0.087251\n",
            "batch 456: loss 0.171714\n",
            "batch 457: loss 0.240724\n",
            "batch 458: loss 0.210795\n",
            "batch 459: loss 0.263277\n",
            "batch 460: loss 0.131165\n",
            "batch 461: loss 0.164191\n",
            "batch 462: loss 0.229316\n",
            "batch 463: loss 0.250625\n",
            "batch 464: loss 0.185130\n",
            "batch 465: loss 0.301003\n",
            "batch 466: loss 0.285802\n",
            "batch 467: loss 0.147524\n",
            "batch 468: loss 0.138352\n",
            "batch 469: loss 0.118812\n",
            "batch 470: loss 0.224842\n",
            "batch 471: loss 0.398666\n",
            "batch 472: loss 0.155729\n",
            "batch 473: loss 0.376447\n",
            "batch 474: loss 0.308483\n",
            "batch 475: loss 0.382504\n",
            "batch 476: loss 0.645721\n",
            "batch 477: loss 0.211887\n",
            "batch 478: loss 0.163056\n",
            "batch 479: loss 0.280356\n",
            "batch 480: loss 0.386227\n",
            "batch 481: loss 0.144858\n",
            "batch 482: loss 0.303876\n",
            "batch 483: loss 0.217352\n",
            "batch 484: loss 0.206032\n",
            "batch 485: loss 0.357497\n",
            "batch 486: loss 0.416319\n",
            "batch 487: loss 0.172704\n",
            "batch 488: loss 0.095424\n",
            "batch 489: loss 0.429355\n",
            "batch 490: loss 0.334216\n",
            "batch 491: loss 0.203548\n",
            "batch 492: loss 0.256700\n",
            "batch 493: loss 0.306440\n",
            "batch 494: loss 0.359907\n",
            "batch 495: loss 0.233761\n",
            "batch 496: loss 0.171716\n",
            "batch 497: loss 0.179526\n",
            "batch 498: loss 0.243354\n",
            "batch 499: loss 0.253607\n",
            "batch 500: loss 0.111650\n",
            "batch 501: loss 0.432339\n",
            "batch 502: loss 0.368755\n",
            "batch 503: loss 0.297173\n",
            "batch 504: loss 0.326667\n",
            "batch 505: loss 0.207000\n",
            "batch 506: loss 0.107635\n",
            "batch 507: loss 0.084505\n",
            "batch 508: loss 0.219045\n",
            "batch 509: loss 0.158363\n",
            "batch 510: loss 0.325002\n",
            "batch 511: loss 0.138042\n",
            "batch 512: loss 0.246820\n",
            "batch 513: loss 0.457245\n",
            "batch 514: loss 0.309266\n",
            "batch 515: loss 0.206377\n",
            "batch 516: loss 0.185973\n",
            "batch 517: loss 0.270960\n",
            "batch 518: loss 0.162677\n",
            "batch 519: loss 0.277116\n",
            "batch 520: loss 0.201894\n",
            "batch 521: loss 0.350788\n",
            "batch 522: loss 0.245315\n",
            "batch 523: loss 0.240424\n",
            "batch 524: loss 0.317207\n",
            "batch 525: loss 0.281225\n",
            "batch 526: loss 0.300204\n",
            "batch 527: loss 0.179844\n",
            "batch 528: loss 0.165495\n",
            "batch 529: loss 0.078440\n",
            "batch 530: loss 0.331327\n",
            "batch 531: loss 0.202342\n",
            "batch 532: loss 0.167412\n",
            "batch 533: loss 0.569187\n",
            "batch 534: loss 0.206152\n",
            "batch 535: loss 0.235172\n",
            "batch 536: loss 0.232047\n",
            "batch 537: loss 0.136512\n",
            "batch 538: loss 0.147227\n",
            "batch 539: loss 0.186360\n",
            "batch 540: loss 0.246782\n",
            "batch 541: loss 0.303766\n",
            "batch 542: loss 0.165764\n",
            "batch 543: loss 0.451608\n",
            "batch 544: loss 0.156536\n",
            "batch 545: loss 0.409911\n",
            "batch 546: loss 0.222680\n",
            "batch 547: loss 0.168829\n",
            "batch 548: loss 0.268110\n",
            "batch 549: loss 0.231580\n",
            "batch 550: loss 0.245474\n",
            "batch 551: loss 0.148452\n",
            "batch 552: loss 0.192442\n",
            "batch 553: loss 0.168161\n",
            "batch 554: loss 0.261788\n",
            "batch 555: loss 0.390065\n",
            "batch 556: loss 0.179470\n",
            "batch 557: loss 0.118298\n",
            "batch 558: loss 0.397321\n",
            "batch 559: loss 0.283137\n",
            "batch 560: loss 0.186283\n",
            "batch 561: loss 0.131813\n",
            "batch 562: loss 0.201034\n",
            "batch 563: loss 0.222744\n",
            "batch 564: loss 0.190673\n",
            "batch 565: loss 0.163860\n",
            "batch 566: loss 0.254114\n",
            "batch 567: loss 0.228014\n",
            "batch 568: loss 0.275209\n",
            "batch 569: loss 0.270788\n",
            "batch 570: loss 0.447730\n",
            "batch 571: loss 0.206731\n",
            "batch 572: loss 0.218991\n",
            "batch 573: loss 0.303097\n",
            "batch 574: loss 0.318667\n",
            "batch 575: loss 0.190540\n",
            "batch 576: loss 0.188726\n",
            "batch 577: loss 0.110501\n",
            "batch 578: loss 0.413960\n",
            "batch 579: loss 0.245033\n",
            "batch 580: loss 0.272442\n",
            "batch 581: loss 0.491640\n",
            "batch 582: loss 0.427308\n",
            "batch 583: loss 0.209748\n",
            "batch 584: loss 0.231089\n",
            "batch 585: loss 0.304013\n",
            "batch 586: loss 0.245932\n",
            "batch 587: loss 0.221004\n",
            "batch 588: loss 0.196532\n",
            "batch 589: loss 0.354791\n",
            "batch 590: loss 0.103536\n",
            "batch 591: loss 0.325864\n",
            "batch 592: loss 0.347087\n",
            "batch 593: loss 0.248384\n",
            "batch 594: loss 0.264838\n",
            "batch 595: loss 0.126857\n",
            "batch 596: loss 0.182220\n",
            "batch 597: loss 0.120862\n",
            "batch 598: loss 0.258809\n",
            "batch 599: loss 0.153771\n",
            "batch 600: loss 0.192825\n",
            "batch 601: loss 0.147575\n",
            "batch 602: loss 0.322241\n",
            "batch 603: loss 0.185039\n",
            "batch 604: loss 0.478668\n",
            "batch 605: loss 0.351801\n",
            "batch 606: loss 0.230275\n",
            "batch 607: loss 0.254325\n",
            "batch 608: loss 0.082630\n",
            "batch 609: loss 0.235766\n",
            "batch 610: loss 0.380425\n",
            "batch 611: loss 0.123578\n",
            "batch 612: loss 0.352910\n",
            "batch 613: loss 0.130808\n",
            "batch 614: loss 0.121545\n",
            "batch 615: loss 0.194937\n",
            "batch 616: loss 0.295283\n",
            "batch 617: loss 0.365295\n",
            "batch 618: loss 0.188083\n",
            "batch 619: loss 0.120629\n",
            "batch 620: loss 0.222779\n",
            "batch 621: loss 0.250753\n",
            "batch 622: loss 0.241378\n",
            "batch 623: loss 0.095506\n",
            "batch 624: loss 0.212246\n",
            "batch 625: loss 0.151221\n",
            "batch 626: loss 0.124304\n",
            "batch 627: loss 0.150879\n",
            "batch 628: loss 0.184507\n",
            "batch 629: loss 0.252619\n",
            "batch 630: loss 0.266866\n",
            "batch 631: loss 0.392765\n",
            "batch 632: loss 0.159401\n",
            "batch 633: loss 0.372880\n",
            "batch 634: loss 0.429238\n",
            "batch 635: loss 0.127572\n",
            "batch 636: loss 0.195110\n",
            "batch 637: loss 0.248874\n",
            "batch 638: loss 0.198751\n",
            "batch 639: loss 0.116412\n",
            "batch 640: loss 0.282891\n",
            "batch 641: loss 0.212131\n",
            "batch 642: loss 0.197505\n",
            "batch 643: loss 0.195028\n",
            "batch 644: loss 0.254656\n",
            "batch 645: loss 0.382334\n",
            "batch 646: loss 0.270937\n",
            "batch 647: loss 0.123461\n",
            "batch 648: loss 0.105270\n",
            "batch 649: loss 0.134024\n",
            "batch 650: loss 0.251833\n",
            "batch 651: loss 0.186842\n",
            "batch 652: loss 0.285572\n",
            "batch 653: loss 0.208233\n",
            "batch 654: loss 0.128080\n",
            "batch 655: loss 0.201859\n",
            "batch 656: loss 0.093712\n",
            "batch 657: loss 0.497239\n",
            "batch 658: loss 0.138046\n",
            "batch 659: loss 0.174464\n",
            "batch 660: loss 0.087432\n",
            "batch 661: loss 0.361220\n",
            "batch 662: loss 0.249013\n",
            "batch 663: loss 0.168648\n",
            "batch 664: loss 0.164596\n",
            "batch 665: loss 0.205040\n",
            "batch 666: loss 0.148878\n",
            "batch 667: loss 0.220937\n",
            "batch 668: loss 0.225366\n",
            "batch 669: loss 0.139244\n",
            "batch 670: loss 0.228526\n",
            "batch 671: loss 0.165519\n",
            "batch 672: loss 0.264079\n",
            "batch 673: loss 0.229547\n",
            "batch 674: loss 0.201356\n",
            "batch 675: loss 0.397696\n",
            "batch 676: loss 0.244117\n",
            "batch 677: loss 0.218770\n",
            "batch 678: loss 0.175073\n",
            "batch 679: loss 0.150036\n",
            "batch 680: loss 0.181004\n",
            "batch 681: loss 0.199164\n",
            "batch 682: loss 0.319383\n",
            "batch 683: loss 0.454577\n",
            "batch 684: loss 0.362161\n",
            "batch 685: loss 0.379396\n",
            "batch 686: loss 0.286826\n",
            "batch 687: loss 0.237542\n",
            "batch 688: loss 0.092273\n",
            "batch 689: loss 0.251575\n",
            "batch 690: loss 0.251943\n",
            "batch 691: loss 0.299448\n",
            "batch 692: loss 0.219701\n",
            "batch 693: loss 0.058337\n",
            "batch 694: loss 0.104682\n",
            "batch 695: loss 0.224800\n",
            "batch 696: loss 0.158321\n",
            "batch 697: loss 0.186271\n",
            "batch 698: loss 0.379669\n",
            "batch 699: loss 0.196304\n",
            "batch 700: loss 0.126820\n",
            "batch 701: loss 0.215020\n",
            "batch 702: loss 0.139840\n",
            "batch 703: loss 0.097043\n",
            "batch 704: loss 0.131791\n",
            "batch 705: loss 0.220118\n",
            "batch 706: loss 0.205489\n",
            "batch 707: loss 0.137859\n",
            "batch 708: loss 0.161911\n",
            "batch 709: loss 0.112413\n",
            "batch 710: loss 0.322004\n",
            "batch 711: loss 0.192871\n",
            "batch 712: loss 0.119190\n",
            "batch 713: loss 0.156436\n",
            "batch 714: loss 0.215381\n",
            "batch 715: loss 0.165257\n",
            "batch 716: loss 0.088763\n",
            "batch 717: loss 0.311590\n",
            "batch 718: loss 0.309833\n",
            "batch 719: loss 0.118781\n",
            "batch 720: loss 0.144300\n",
            "batch 721: loss 0.165681\n",
            "batch 722: loss 0.211104\n",
            "batch 723: loss 0.286076\n",
            "batch 724: loss 0.261407\n",
            "batch 725: loss 0.171135\n",
            "batch 726: loss 0.228628\n",
            "batch 727: loss 0.379348\n",
            "batch 728: loss 0.259715\n",
            "batch 729: loss 0.166803\n",
            "batch 730: loss 0.120044\n",
            "batch 731: loss 0.136197\n",
            "batch 732: loss 0.193157\n",
            "batch 733: loss 0.462505\n",
            "batch 734: loss 0.194480\n",
            "batch 735: loss 0.120701\n",
            "batch 736: loss 0.200316\n",
            "batch 737: loss 0.192156\n",
            "batch 738: loss 0.429415\n",
            "batch 739: loss 0.151615\n",
            "batch 740: loss 0.130395\n",
            "batch 741: loss 0.067391\n",
            "batch 742: loss 0.239272\n",
            "batch 743: loss 0.191425\n",
            "batch 744: loss 0.088040\n",
            "batch 745: loss 0.133100\n",
            "batch 746: loss 0.178744\n",
            "batch 747: loss 0.105200\n",
            "batch 748: loss 0.336302\n",
            "batch 749: loss 0.268693\n",
            "batch 750: loss 0.106920\n",
            "batch 751: loss 0.238592\n",
            "batch 752: loss 0.348431\n",
            "batch 753: loss 0.381967\n",
            "batch 754: loss 0.138469\n",
            "batch 755: loss 0.148312\n",
            "batch 756: loss 0.159597\n",
            "batch 757: loss 0.269694\n",
            "batch 758: loss 0.218261\n",
            "batch 759: loss 0.183713\n",
            "batch 760: loss 0.131542\n",
            "batch 761: loss 0.088646\n",
            "batch 762: loss 0.131081\n",
            "batch 763: loss 0.213436\n",
            "batch 764: loss 0.113815\n",
            "batch 765: loss 0.328426\n",
            "batch 766: loss 0.165077\n",
            "batch 767: loss 0.178210\n",
            "batch 768: loss 0.150529\n",
            "batch 769: loss 0.178899\n",
            "batch 770: loss 0.139170\n",
            "batch 771: loss 0.405826\n",
            "batch 772: loss 0.083926\n",
            "batch 773: loss 0.460692\n",
            "batch 774: loss 0.258821\n",
            "batch 775: loss 0.202971\n",
            "batch 776: loss 0.237020\n",
            "batch 777: loss 0.162470\n",
            "batch 778: loss 0.342729\n",
            "batch 779: loss 0.189643\n",
            "batch 780: loss 0.261693\n",
            "batch 781: loss 0.354600\n",
            "batch 782: loss 0.136905\n",
            "batch 783: loss 0.174311\n",
            "batch 784: loss 0.139615\n",
            "batch 785: loss 0.422603\n",
            "batch 786: loss 0.153136\n",
            "batch 787: loss 0.264856\n",
            "batch 788: loss 0.225169\n",
            "batch 789: loss 0.110463\n",
            "batch 790: loss 0.101593\n",
            "batch 791: loss 0.138396\n",
            "batch 792: loss 0.156536\n",
            "batch 793: loss 0.152907\n",
            "batch 794: loss 0.407488\n",
            "batch 795: loss 0.227170\n",
            "batch 796: loss 0.263045\n",
            "batch 797: loss 0.106457\n",
            "batch 798: loss 0.158547\n",
            "batch 799: loss 0.130157\n",
            "batch 800: loss 0.230506\n",
            "batch 801: loss 0.165566\n",
            "batch 802: loss 0.066773\n",
            "batch 803: loss 0.081488\n",
            "batch 804: loss 0.112505\n",
            "batch 805: loss 0.122623\n",
            "batch 806: loss 0.080080\n",
            "batch 807: loss 0.119630\n",
            "batch 808: loss 0.372654\n",
            "batch 809: loss 0.088802\n",
            "batch 810: loss 0.183851\n",
            "batch 811: loss 0.181369\n",
            "batch 812: loss 0.191452\n",
            "batch 813: loss 0.081957\n",
            "batch 814: loss 0.081069\n",
            "batch 815: loss 0.308453\n",
            "batch 816: loss 0.162984\n",
            "batch 817: loss 0.198797\n",
            "batch 818: loss 0.284803\n",
            "batch 819: loss 0.254315\n",
            "batch 820: loss 0.098845\n",
            "batch 821: loss 0.360396\n",
            "batch 822: loss 0.147413\n",
            "batch 823: loss 0.134099\n",
            "batch 824: loss 0.206968\n",
            "batch 825: loss 0.124256\n",
            "batch 826: loss 0.075344\n",
            "batch 827: loss 0.187225\n",
            "batch 828: loss 0.385096\n",
            "batch 829: loss 0.076132\n",
            "batch 830: loss 0.184770\n",
            "batch 831: loss 0.093971\n",
            "batch 832: loss 0.116948\n",
            "batch 833: loss 0.480064\n",
            "batch 834: loss 0.270651\n",
            "batch 835: loss 0.254246\n",
            "batch 836: loss 0.232615\n",
            "batch 837: loss 0.241897\n",
            "batch 838: loss 0.303030\n",
            "batch 839: loss 0.256742\n",
            "batch 840: loss 0.091740\n",
            "batch 841: loss 0.368938\n",
            "batch 842: loss 0.101534\n",
            "batch 843: loss 0.091485\n",
            "batch 844: loss 0.186079\n",
            "batch 845: loss 0.260622\n",
            "batch 846: loss 0.222772\n",
            "batch 847: loss 0.150905\n",
            "batch 848: loss 0.248737\n",
            "batch 849: loss 0.242198\n",
            "batch 850: loss 0.103479\n",
            "batch 851: loss 0.332825\n",
            "batch 852: loss 0.182598\n",
            "batch 853: loss 0.124299\n",
            "batch 854: loss 0.346539\n",
            "batch 855: loss 0.283192\n",
            "batch 856: loss 0.229150\n",
            "batch 857: loss 0.332144\n",
            "batch 858: loss 0.326545\n",
            "batch 859: loss 0.212941\n",
            "batch 860: loss 0.278035\n",
            "batch 861: loss 0.136816\n",
            "batch 862: loss 0.294525\n",
            "batch 863: loss 0.337999\n",
            "batch 864: loss 0.165972\n",
            "batch 865: loss 0.232428\n",
            "batch 866: loss 0.375904\n",
            "batch 867: loss 0.052344\n",
            "batch 868: loss 0.129622\n",
            "batch 869: loss 0.189282\n",
            "batch 870: loss 0.246183\n",
            "batch 871: loss 0.185088\n",
            "batch 872: loss 0.273844\n",
            "batch 873: loss 0.231083\n",
            "batch 874: loss 0.444245\n",
            "batch 875: loss 0.138045\n",
            "batch 876: loss 0.281417\n",
            "batch 877: loss 0.173194\n",
            "batch 878: loss 0.187778\n",
            "batch 879: loss 0.082031\n",
            "batch 880: loss 0.187816\n",
            "batch 881: loss 0.324046\n",
            "batch 882: loss 0.146382\n",
            "batch 883: loss 0.212791\n",
            "batch 884: loss 0.175484\n",
            "batch 885: loss 0.105109\n",
            "batch 886: loss 0.137521\n",
            "batch 887: loss 0.350912\n",
            "batch 888: loss 0.134004\n",
            "batch 889: loss 0.096639\n",
            "batch 890: loss 0.236112\n",
            "batch 891: loss 0.213303\n",
            "batch 892: loss 0.204443\n",
            "batch 893: loss 0.316707\n",
            "batch 894: loss 0.119204\n",
            "batch 895: loss 0.149610\n",
            "batch 896: loss 0.205986\n",
            "batch 897: loss 0.054024\n",
            "batch 898: loss 0.373867\n",
            "batch 899: loss 0.107830\n",
            "batch 900: loss 0.303014\n",
            "batch 901: loss 0.298210\n",
            "batch 902: loss 0.239811\n",
            "batch 903: loss 0.081657\n",
            "batch 904: loss 0.151176\n",
            "batch 905: loss 0.168715\n",
            "batch 906: loss 0.162409\n",
            "batch 907: loss 0.138237\n",
            "batch 908: loss 0.269247\n",
            "batch 909: loss 0.201964\n",
            "batch 910: loss 0.227105\n",
            "batch 911: loss 0.145848\n",
            "batch 912: loss 0.456647\n",
            "batch 913: loss 0.251516\n",
            "batch 914: loss 0.332249\n",
            "batch 915: loss 0.158279\n",
            "batch 916: loss 0.188900\n",
            "batch 917: loss 0.212861\n",
            "batch 918: loss 0.144752\n",
            "batch 919: loss 0.105779\n",
            "batch 920: loss 0.136230\n",
            "batch 921: loss 0.242994\n",
            "batch 922: loss 0.178327\n",
            "batch 923: loss 0.136785\n",
            "batch 924: loss 0.118241\n",
            "batch 925: loss 0.197878\n",
            "batch 926: loss 0.273921\n",
            "batch 927: loss 0.090994\n",
            "batch 928: loss 0.080289\n",
            "batch 929: loss 0.264020\n",
            "batch 930: loss 0.250393\n",
            "batch 931: loss 0.209158\n",
            "batch 932: loss 0.149427\n",
            "batch 933: loss 0.161322\n",
            "batch 934: loss 0.309410\n",
            "batch 935: loss 0.175828\n",
            "batch 936: loss 0.114064\n",
            "batch 937: loss 0.291186\n",
            "batch 938: loss 0.122938\n",
            "batch 939: loss 0.158277\n",
            "batch 940: loss 0.247278\n",
            "batch 941: loss 0.211736\n",
            "batch 942: loss 0.172036\n",
            "batch 943: loss 0.155320\n",
            "batch 944: loss 0.283286\n",
            "batch 945: loss 0.055861\n",
            "batch 946: loss 0.138187\n",
            "batch 947: loss 0.099569\n",
            "batch 948: loss 0.114588\n",
            "batch 949: loss 0.165940\n",
            "batch 950: loss 0.158468\n",
            "batch 951: loss 0.144281\n",
            "batch 952: loss 0.228760\n",
            "batch 953: loss 0.110188\n",
            "batch 954: loss 0.134715\n",
            "batch 955: loss 0.177538\n",
            "batch 956: loss 0.121746\n",
            "batch 957: loss 0.148320\n",
            "batch 958: loss 0.156589\n",
            "batch 959: loss 0.266998\n",
            "batch 960: loss 0.358975\n",
            "batch 961: loss 0.117211\n",
            "batch 962: loss 0.223431\n",
            "batch 963: loss 0.324820\n",
            "batch 964: loss 0.102497\n",
            "batch 965: loss 0.115700\n",
            "batch 966: loss 0.143268\n",
            "batch 967: loss 0.110392\n",
            "batch 968: loss 0.091357\n",
            "batch 969: loss 0.386956\n",
            "batch 970: loss 0.366155\n",
            "batch 971: loss 0.138953\n",
            "batch 972: loss 0.078476\n",
            "batch 973: loss 0.381980\n",
            "batch 974: loss 0.237336\n",
            "batch 975: loss 0.310699\n",
            "batch 976: loss 0.255909\n",
            "batch 977: loss 0.190113\n",
            "batch 978: loss 0.128842\n",
            "batch 979: loss 0.490231\n",
            "batch 980: loss 0.117207\n",
            "batch 981: loss 0.152495\n",
            "batch 982: loss 0.178150\n",
            "batch 983: loss 0.062736\n",
            "batch 984: loss 0.207027\n",
            "batch 985: loss 0.153286\n",
            "batch 986: loss 0.102366\n",
            "batch 987: loss 0.206578\n",
            "batch 988: loss 0.247806\n",
            "batch 989: loss 0.170794\n",
            "batch 990: loss 0.065410\n",
            "batch 991: loss 0.225616\n",
            "batch 992: loss 0.191554\n",
            "batch 993: loss 0.328937\n",
            "batch 994: loss 0.260859\n",
            "batch 995: loss 0.107933\n",
            "batch 996: loss 0.149759\n",
            "batch 997: loss 0.139096\n",
            "batch 998: loss 0.242818\n",
            "batch 999: loss 0.137285\n",
            "batch 1000: loss 0.287610\n",
            "batch 1001: loss 0.240831\n",
            "batch 1002: loss 0.222429\n",
            "batch 1003: loss 0.247821\n",
            "batch 1004: loss 0.092017\n",
            "batch 1005: loss 0.187642\n",
            "batch 1006: loss 0.159406\n",
            "batch 1007: loss 0.466153\n",
            "batch 1008: loss 0.208077\n",
            "batch 1009: loss 0.329706\n",
            "batch 1010: loss 0.438521\n",
            "batch 1011: loss 0.180183\n",
            "batch 1012: loss 0.129646\n",
            "batch 1013: loss 0.080241\n",
            "batch 1014: loss 0.222254\n",
            "batch 1015: loss 0.062716\n",
            "batch 1016: loss 0.195022\n",
            "batch 1017: loss 0.080307\n",
            "batch 1018: loss 0.058030\n",
            "batch 1019: loss 0.255315\n",
            "batch 1020: loss 0.136901\n",
            "batch 1021: loss 0.224680\n",
            "batch 1022: loss 0.163065\n",
            "batch 1023: loss 0.173243\n",
            "batch 1024: loss 0.254313\n",
            "batch 1025: loss 0.167482\n",
            "batch 1026: loss 0.179161\n",
            "batch 1027: loss 0.108108\n",
            "batch 1028: loss 0.147646\n",
            "batch 1029: loss 0.154089\n",
            "batch 1030: loss 0.272354\n",
            "batch 1031: loss 0.324066\n",
            "batch 1032: loss 0.179905\n",
            "batch 1033: loss 0.211925\n",
            "batch 1034: loss 0.245150\n",
            "batch 1035: loss 0.219373\n",
            "batch 1036: loss 0.139194\n",
            "batch 1037: loss 0.182004\n",
            "batch 1038: loss 0.102834\n",
            "batch 1039: loss 0.183904\n",
            "batch 1040: loss 0.161382\n",
            "batch 1041: loss 0.107381\n",
            "batch 1042: loss 0.249992\n",
            "batch 1043: loss 0.064434\n",
            "batch 1044: loss 0.325336\n",
            "batch 1045: loss 0.103303\n",
            "batch 1046: loss 0.157886\n",
            "batch 1047: loss 0.062011\n",
            "batch 1048: loss 0.052705\n",
            "batch 1049: loss 0.334349\n",
            "batch 1050: loss 0.407097\n",
            "batch 1051: loss 0.282231\n",
            "batch 1052: loss 0.111510\n",
            "batch 1053: loss 0.221454\n",
            "batch 1054: loss 0.263605\n",
            "batch 1055: loss 0.114107\n",
            "batch 1056: loss 0.092366\n",
            "batch 1057: loss 0.338452\n",
            "batch 1058: loss 0.102594\n",
            "batch 1059: loss 0.063453\n",
            "batch 1060: loss 0.107768\n",
            "batch 1061: loss 0.092841\n",
            "batch 1062: loss 0.147367\n",
            "batch 1063: loss 0.269492\n",
            "batch 1064: loss 0.189989\n",
            "batch 1065: loss 0.069833\n",
            "batch 1066: loss 0.179656\n",
            "batch 1067: loss 0.088612\n",
            "batch 1068: loss 0.199888\n",
            "batch 1069: loss 0.147905\n",
            "batch 1070: loss 0.157295\n",
            "batch 1071: loss 0.078558\n",
            "batch 1072: loss 0.092149\n",
            "batch 1073: loss 0.171366\n",
            "batch 1074: loss 0.108801\n",
            "batch 1075: loss 0.319465\n",
            "batch 1076: loss 0.212894\n",
            "batch 1077: loss 0.168428\n",
            "batch 1078: loss 0.171473\n",
            "batch 1079: loss 0.194087\n",
            "batch 1080: loss 0.154834\n",
            "batch 1081: loss 0.087226\n",
            "batch 1082: loss 0.327244\n",
            "batch 1083: loss 0.157041\n",
            "batch 1084: loss 0.146947\n",
            "batch 1085: loss 0.192076\n",
            "batch 1086: loss 0.071909\n",
            "batch 1087: loss 0.091010\n",
            "batch 1088: loss 0.055776\n",
            "batch 1089: loss 0.068580\n",
            "batch 1090: loss 0.057781\n",
            "batch 1091: loss 0.106048\n",
            "batch 1092: loss 0.165874\n",
            "batch 1093: loss 0.249932\n",
            "batch 1094: loss 0.091789\n",
            "batch 1095: loss 0.126265\n",
            "batch 1096: loss 0.108243\n",
            "batch 1097: loss 0.279924\n",
            "batch 1098: loss 0.121689\n",
            "batch 1099: loss 0.073583\n",
            "batch 1100: loss 0.215764\n",
            "batch 1101: loss 0.134286\n",
            "batch 1102: loss 0.174852\n",
            "batch 1103: loss 0.195627\n",
            "batch 1104: loss 0.322693\n",
            "batch 1105: loss 0.103442\n",
            "batch 1106: loss 0.117735\n",
            "batch 1107: loss 0.092486\n",
            "batch 1108: loss 0.167453\n",
            "batch 1109: loss 0.072287\n",
            "batch 1110: loss 0.068875\n",
            "batch 1111: loss 0.272942\n",
            "batch 1112: loss 0.215650\n",
            "batch 1113: loss 0.289055\n",
            "batch 1114: loss 0.231615\n",
            "batch 1115: loss 0.286097\n",
            "batch 1116: loss 0.426897\n",
            "batch 1117: loss 0.103288\n",
            "batch 1118: loss 0.113883\n",
            "batch 1119: loss 0.258251\n",
            "batch 1120: loss 0.147385\n",
            "batch 1121: loss 0.124886\n",
            "batch 1122: loss 0.216439\n",
            "batch 1123: loss 0.205901\n",
            "batch 1124: loss 0.061745\n",
            "batch 1125: loss 0.530007\n",
            "batch 1126: loss 0.476759\n",
            "batch 1127: loss 0.178194\n",
            "batch 1128: loss 0.109276\n",
            "batch 1129: loss 0.110618\n",
            "batch 1130: loss 0.141938\n",
            "batch 1131: loss 0.091796\n",
            "batch 1132: loss 0.090625\n",
            "batch 1133: loss 0.051714\n",
            "batch 1134: loss 0.119903\n",
            "batch 1135: loss 0.101843\n",
            "batch 1136: loss 0.257633\n",
            "batch 1137: loss 0.143367\n",
            "batch 1138: loss 0.171259\n",
            "batch 1139: loss 0.057594\n",
            "batch 1140: loss 0.205549\n",
            "batch 1141: loss 0.127571\n",
            "batch 1142: loss 0.176739\n",
            "batch 1143: loss 0.166264\n",
            "batch 1144: loss 0.133878\n",
            "batch 1145: loss 0.295597\n",
            "batch 1146: loss 0.093821\n",
            "batch 1147: loss 0.177287\n",
            "batch 1148: loss 0.187648\n",
            "batch 1149: loss 0.264647\n",
            "batch 1150: loss 0.151646\n",
            "batch 1151: loss 0.218156\n",
            "batch 1152: loss 0.319582\n",
            "batch 1153: loss 0.074021\n",
            "batch 1154: loss 0.085205\n",
            "batch 1155: loss 0.137912\n",
            "batch 1156: loss 0.099332\n",
            "batch 1157: loss 0.123826\n",
            "batch 1158: loss 0.176292\n",
            "batch 1159: loss 0.137943\n",
            "batch 1160: loss 0.117650\n",
            "batch 1161: loss 0.191346\n",
            "batch 1162: loss 0.045847\n",
            "batch 1163: loss 0.083111\n",
            "batch 1164: loss 0.105928\n",
            "batch 1165: loss 0.280000\n",
            "batch 1166: loss 0.102128\n",
            "batch 1167: loss 0.135393\n",
            "batch 1168: loss 0.070535\n",
            "batch 1169: loss 0.387547\n",
            "batch 1170: loss 0.237031\n",
            "batch 1171: loss 0.027067\n",
            "batch 1172: loss 0.302285\n",
            "batch 1173: loss 0.061499\n",
            "batch 1174: loss 0.070979\n",
            "batch 1175: loss 0.225266\n",
            "batch 1176: loss 0.275560\n",
            "batch 1177: loss 0.074270\n",
            "batch 1178: loss 0.148317\n",
            "batch 1179: loss 0.139538\n",
            "batch 1180: loss 0.284845\n",
            "batch 1181: loss 0.134055\n",
            "batch 1182: loss 0.134778\n",
            "batch 1183: loss 0.190963\n",
            "batch 1184: loss 0.208224\n",
            "batch 1185: loss 0.143467\n",
            "batch 1186: loss 0.102004\n",
            "batch 1187: loss 0.068690\n",
            "batch 1188: loss 0.130685\n",
            "batch 1189: loss 0.206383\n",
            "batch 1190: loss 0.073019\n",
            "batch 1191: loss 0.058372\n",
            "batch 1192: loss 0.131698\n",
            "batch 1193: loss 0.058582\n",
            "batch 1194: loss 0.205968\n",
            "batch 1195: loss 0.197927\n",
            "batch 1196: loss 0.150121\n",
            "batch 1197: loss 0.368679\n",
            "batch 1198: loss 0.039886\n",
            "batch 1199: loss 0.086828\n",
            "batch 1200: loss 0.398496\n",
            "batch 1201: loss 0.306039\n",
            "batch 1202: loss 0.275784\n",
            "batch 1203: loss 0.208350\n",
            "batch 1204: loss 0.199872\n",
            "batch 1205: loss 0.148930\n",
            "batch 1206: loss 0.098109\n",
            "batch 1207: loss 0.071266\n",
            "batch 1208: loss 0.203594\n",
            "batch 1209: loss 0.113406\n",
            "batch 1210: loss 0.026985\n",
            "batch 1211: loss 0.189062\n",
            "batch 1212: loss 0.146502\n",
            "batch 1213: loss 0.199465\n",
            "batch 1214: loss 0.025133\n",
            "batch 1215: loss 0.206073\n",
            "batch 1216: loss 0.126111\n",
            "batch 1217: loss 0.099880\n",
            "batch 1218: loss 0.160274\n",
            "batch 1219: loss 0.129986\n",
            "batch 1220: loss 0.118653\n",
            "batch 1221: loss 0.061238\n",
            "batch 1222: loss 0.229207\n",
            "batch 1223: loss 0.101188\n",
            "batch 1224: loss 0.136140\n",
            "batch 1225: loss 0.167736\n",
            "batch 1226: loss 0.138204\n",
            "batch 1227: loss 0.145734\n",
            "batch 1228: loss 0.065554\n",
            "batch 1229: loss 0.183232\n",
            "batch 1230: loss 0.119818\n",
            "batch 1231: loss 0.162687\n",
            "batch 1232: loss 0.155451\n",
            "batch 1233: loss 0.198348\n",
            "batch 1234: loss 0.340052\n",
            "batch 1235: loss 0.051617\n",
            "batch 1236: loss 0.111592\n",
            "batch 1237: loss 0.083468\n",
            "batch 1238: loss 0.285588\n",
            "batch 1239: loss 0.313872\n",
            "batch 1240: loss 0.076467\n",
            "batch 1241: loss 0.261374\n",
            "batch 1242: loss 0.113322\n",
            "batch 1243: loss 0.070615\n",
            "batch 1244: loss 0.175935\n",
            "batch 1245: loss 0.116053\n",
            "batch 1246: loss 0.327901\n",
            "batch 1247: loss 0.060814\n",
            "batch 1248: loss 0.152250\n",
            "batch 1249: loss 0.129919\n",
            "batch 1250: loss 0.122442\n",
            "batch 1251: loss 0.102318\n",
            "batch 1252: loss 0.111080\n",
            "batch 1253: loss 0.114126\n",
            "batch 1254: loss 0.170800\n",
            "batch 1255: loss 0.038434\n",
            "batch 1256: loss 0.165805\n",
            "batch 1257: loss 0.164230\n",
            "batch 1258: loss 0.201104\n",
            "batch 1259: loss 0.098283\n",
            "batch 1260: loss 0.084275\n",
            "batch 1261: loss 0.147279\n",
            "batch 1262: loss 0.131320\n",
            "batch 1263: loss 0.075387\n",
            "batch 1264: loss 0.225587\n",
            "batch 1265: loss 0.259033\n",
            "batch 1266: loss 0.139219\n",
            "batch 1267: loss 0.241389\n",
            "batch 1268: loss 0.262569\n",
            "batch 1269: loss 0.167699\n",
            "batch 1270: loss 0.249782\n",
            "batch 1271: loss 0.174772\n",
            "batch 1272: loss 0.175012\n",
            "batch 1273: loss 0.182243\n",
            "batch 1274: loss 0.129153\n",
            "batch 1275: loss 0.241576\n",
            "batch 1276: loss 0.151165\n",
            "batch 1277: loss 0.253425\n",
            "batch 1278: loss 0.115418\n",
            "batch 1279: loss 0.085945\n",
            "batch 1280: loss 0.087929\n",
            "batch 1281: loss 0.062155\n",
            "batch 1282: loss 0.256266\n",
            "batch 1283: loss 0.224586\n",
            "batch 1284: loss 0.319615\n",
            "batch 1285: loss 0.064415\n",
            "batch 1286: loss 0.110068\n",
            "batch 1287: loss 0.230561\n",
            "batch 1288: loss 0.165361\n",
            "batch 1289: loss 0.281954\n",
            "batch 1290: loss 0.087723\n",
            "batch 1291: loss 0.183081\n",
            "batch 1292: loss 0.111898\n",
            "batch 1293: loss 0.320132\n",
            "batch 1294: loss 0.276569\n",
            "batch 1295: loss 0.161013\n",
            "batch 1296: loss 0.175281\n",
            "batch 1297: loss 0.219129\n",
            "batch 1298: loss 0.173551\n",
            "batch 1299: loss 0.140789\n",
            "batch 1300: loss 0.101707\n",
            "batch 1301: loss 0.301014\n",
            "batch 1302: loss 0.087795\n",
            "batch 1303: loss 0.077503\n",
            "batch 1304: loss 0.130524\n",
            "batch 1305: loss 0.104943\n",
            "batch 1306: loss 0.317728\n",
            "batch 1307: loss 0.240317\n",
            "batch 1308: loss 0.169164\n",
            "batch 1309: loss 0.105801\n",
            "batch 1310: loss 0.086439\n",
            "batch 1311: loss 0.109836\n",
            "batch 1312: loss 0.121391\n",
            "batch 1313: loss 0.179823\n",
            "batch 1314: loss 0.033450\n",
            "batch 1315: loss 0.208852\n",
            "batch 1316: loss 0.183325\n",
            "batch 1317: loss 0.178338\n",
            "batch 1318: loss 0.283915\n",
            "batch 1319: loss 0.059600\n",
            "batch 1320: loss 0.076613\n",
            "batch 1321: loss 0.115665\n",
            "batch 1322: loss 0.176987\n",
            "batch 1323: loss 0.334054\n",
            "batch 1324: loss 0.145889\n",
            "batch 1325: loss 0.111103\n",
            "batch 1326: loss 0.159793\n",
            "batch 1327: loss 0.152688\n",
            "batch 1328: loss 0.174950\n",
            "batch 1329: loss 0.261193\n",
            "batch 1330: loss 0.055669\n",
            "batch 1331: loss 0.088117\n",
            "batch 1332: loss 0.317442\n",
            "batch 1333: loss 0.203945\n",
            "batch 1334: loss 0.108730\n",
            "batch 1335: loss 0.113309\n",
            "batch 1336: loss 0.274982\n",
            "batch 1337: loss 0.151791\n",
            "batch 1338: loss 0.282848\n",
            "batch 1339: loss 0.219362\n",
            "batch 1340: loss 0.110663\n",
            "batch 1341: loss 0.263462\n",
            "batch 1342: loss 0.096388\n",
            "batch 1343: loss 0.104224\n",
            "batch 1344: loss 0.090651\n",
            "batch 1345: loss 0.074675\n",
            "batch 1346: loss 0.164715\n",
            "batch 1347: loss 0.157941\n",
            "batch 1348: loss 0.280117\n",
            "batch 1349: loss 0.038222\n",
            "batch 1350: loss 0.145823\n",
            "batch 1351: loss 0.108346\n",
            "batch 1352: loss 0.065123\n",
            "batch 1353: loss 0.078585\n",
            "batch 1354: loss 0.182649\n",
            "batch 1355: loss 0.031297\n",
            "batch 1356: loss 0.222324\n",
            "batch 1357: loss 0.179584\n",
            "batch 1358: loss 0.187378\n",
            "batch 1359: loss 0.119499\n",
            "batch 1360: loss 0.365781\n",
            "batch 1361: loss 0.218540\n",
            "batch 1362: loss 0.170001\n",
            "batch 1363: loss 0.106357\n",
            "batch 1364: loss 0.057551\n",
            "batch 1365: loss 0.098922\n",
            "batch 1366: loss 0.123045\n",
            "batch 1367: loss 0.103993\n",
            "batch 1368: loss 0.064606\n",
            "batch 1369: loss 0.222544\n",
            "batch 1370: loss 0.195882\n",
            "batch 1371: loss 0.099635\n",
            "batch 1372: loss 0.062893\n",
            "batch 1373: loss 0.056266\n",
            "batch 1374: loss 0.251509\n",
            "batch 1375: loss 0.073804\n",
            "batch 1376: loss 0.043797\n",
            "batch 1377: loss 0.339104\n",
            "batch 1378: loss 0.118213\n",
            "batch 1379: loss 0.121013\n",
            "batch 1380: loss 0.097739\n",
            "batch 1381: loss 0.088023\n",
            "batch 1382: loss 0.308111\n",
            "batch 1383: loss 0.070164\n",
            "batch 1384: loss 0.172030\n",
            "batch 1385: loss 0.105447\n",
            "batch 1386: loss 0.140603\n",
            "batch 1387: loss 0.171527\n",
            "batch 1388: loss 0.265889\n",
            "batch 1389: loss 0.104236\n",
            "batch 1390: loss 0.054671\n",
            "batch 1391: loss 0.123709\n",
            "batch 1392: loss 0.081720\n",
            "batch 1393: loss 0.125016\n",
            "batch 1394: loss 0.158992\n",
            "batch 1395: loss 0.152437\n",
            "batch 1396: loss 0.076025\n",
            "batch 1397: loss 0.318423\n",
            "batch 1398: loss 0.138524\n",
            "batch 1399: loss 0.272603\n",
            "batch 1400: loss 0.067430\n",
            "batch 1401: loss 0.098560\n",
            "batch 1402: loss 0.157625\n",
            "batch 1403: loss 0.489996\n",
            "batch 1404: loss 0.082763\n",
            "batch 1405: loss 0.113474\n",
            "batch 1406: loss 0.168122\n",
            "batch 1407: loss 0.376683\n",
            "batch 1408: loss 0.095518\n",
            "batch 1409: loss 0.254719\n",
            "batch 1410: loss 0.053606\n",
            "batch 1411: loss 0.131419\n",
            "batch 1412: loss 0.085785\n",
            "batch 1413: loss 0.212800\n",
            "batch 1414: loss 0.061986\n",
            "batch 1415: loss 0.171069\n",
            "batch 1416: loss 0.167503\n",
            "batch 1417: loss 0.104133\n",
            "batch 1418: loss 0.144937\n",
            "batch 1419: loss 0.214774\n",
            "batch 1420: loss 0.195122\n",
            "batch 1421: loss 0.186440\n",
            "batch 1422: loss 0.170001\n",
            "batch 1423: loss 0.145186\n",
            "batch 1424: loss 0.167095\n",
            "batch 1425: loss 0.113248\n",
            "batch 1426: loss 0.121752\n",
            "batch 1427: loss 0.117455\n",
            "batch 1428: loss 0.139913\n",
            "batch 1429: loss 0.191004\n",
            "batch 1430: loss 0.140140\n",
            "batch 1431: loss 0.095218\n",
            "batch 1432: loss 0.045237\n",
            "batch 1433: loss 0.149396\n",
            "batch 1434: loss 0.032006\n",
            "batch 1435: loss 0.222741\n",
            "batch 1436: loss 0.034334\n",
            "batch 1437: loss 0.060692\n",
            "batch 1438: loss 0.084337\n",
            "batch 1439: loss 0.103165\n",
            "batch 1440: loss 0.163242\n",
            "batch 1441: loss 0.228531\n",
            "batch 1442: loss 0.119634\n",
            "batch 1443: loss 0.270238\n",
            "batch 1444: loss 0.129718\n",
            "batch 1445: loss 0.125081\n",
            "batch 1446: loss 0.238008\n",
            "batch 1447: loss 0.103622\n",
            "batch 1448: loss 0.069539\n",
            "batch 1449: loss 0.241371\n",
            "batch 1450: loss 0.267123\n",
            "batch 1451: loss 0.106386\n",
            "batch 1452: loss 0.109804\n",
            "batch 1453: loss 0.105256\n",
            "batch 1454: loss 0.043108\n",
            "batch 1455: loss 0.208151\n",
            "batch 1456: loss 0.218779\n",
            "batch 1457: loss 0.137737\n",
            "batch 1458: loss 0.234706\n",
            "batch 1459: loss 0.203495\n",
            "batch 1460: loss 0.180256\n",
            "batch 1461: loss 0.158159\n",
            "batch 1462: loss 0.032568\n",
            "batch 1463: loss 0.395250\n",
            "batch 1464: loss 0.183204\n",
            "batch 1465: loss 0.055139\n",
            "batch 1466: loss 0.234886\n",
            "batch 1467: loss 0.321713\n",
            "batch 1468: loss 0.123155\n",
            "batch 1469: loss 0.155446\n",
            "batch 1470: loss 0.196597\n",
            "batch 1471: loss 0.205202\n",
            "batch 1472: loss 0.211120\n",
            "batch 1473: loss 0.137489\n",
            "batch 1474: loss 0.106504\n",
            "batch 1475: loss 0.085397\n",
            "batch 1476: loss 0.271234\n",
            "batch 1477: loss 0.059280\n",
            "batch 1478: loss 0.110328\n",
            "batch 1479: loss 0.130649\n",
            "batch 1480: loss 0.190791\n",
            "batch 1481: loss 0.087822\n",
            "batch 1482: loss 0.104791\n",
            "batch 1483: loss 0.067963\n",
            "batch 1484: loss 0.182605\n",
            "batch 1485: loss 0.264023\n",
            "batch 1486: loss 0.115394\n",
            "batch 1487: loss 0.235428\n",
            "batch 1488: loss 0.048370\n",
            "batch 1489: loss 0.121081\n",
            "batch 1490: loss 0.079964\n",
            "batch 1491: loss 0.256764\n",
            "batch 1492: loss 0.060362\n",
            "batch 1493: loss 0.087005\n",
            "batch 1494: loss 0.199580\n",
            "batch 1495: loss 0.252122\n",
            "batch 1496: loss 0.154462\n",
            "batch 1497: loss 0.162039\n",
            "batch 1498: loss 0.117276\n",
            "batch 1499: loss 0.062517\n",
            "batch 1500: loss 0.113697\n",
            "batch 1501: loss 0.243172\n",
            "batch 1502: loss 0.060303\n",
            "batch 1503: loss 0.143342\n",
            "batch 1504: loss 0.071116\n",
            "batch 1505: loss 0.103985\n",
            "batch 1506: loss 0.132913\n",
            "batch 1507: loss 0.128098\n",
            "batch 1508: loss 0.117815\n",
            "batch 1509: loss 0.079554\n",
            "batch 1510: loss 0.132953\n",
            "batch 1511: loss 0.147344\n",
            "batch 1512: loss 0.075385\n",
            "batch 1513: loss 0.233310\n",
            "batch 1514: loss 0.107981\n",
            "batch 1515: loss 0.278409\n",
            "batch 1516: loss 0.092571\n",
            "batch 1517: loss 0.048378\n",
            "batch 1518: loss 0.055347\n",
            "batch 1519: loss 0.085896\n",
            "batch 1520: loss 0.279083\n",
            "batch 1521: loss 0.105206\n",
            "batch 1522: loss 0.166070\n",
            "batch 1523: loss 0.082771\n",
            "batch 1524: loss 0.110232\n",
            "batch 1525: loss 0.102894\n",
            "batch 1526: loss 0.249487\n",
            "batch 1527: loss 0.098070\n",
            "batch 1528: loss 0.164741\n",
            "batch 1529: loss 0.142498\n",
            "batch 1530: loss 0.210323\n",
            "batch 1531: loss 0.123648\n",
            "batch 1532: loss 0.080210\n",
            "batch 1533: loss 0.143142\n",
            "batch 1534: loss 0.122699\n",
            "batch 1535: loss 0.147799\n",
            "batch 1536: loss 0.070224\n",
            "batch 1537: loss 0.068081\n",
            "batch 1538: loss 0.281211\n",
            "batch 1539: loss 0.166648\n",
            "batch 1540: loss 0.043291\n",
            "batch 1541: loss 0.066642\n",
            "batch 1542: loss 0.060033\n",
            "batch 1543: loss 0.222983\n",
            "batch 1544: loss 0.140948\n",
            "batch 1545: loss 0.044658\n",
            "batch 1546: loss 0.123568\n",
            "batch 1547: loss 0.054503\n",
            "batch 1548: loss 0.244707\n",
            "batch 1549: loss 0.195696\n",
            "batch 1550: loss 0.043431\n",
            "batch 1551: loss 0.183927\n",
            "batch 1552: loss 0.098726\n",
            "batch 1553: loss 0.151189\n",
            "batch 1554: loss 0.172466\n",
            "batch 1555: loss 0.067500\n",
            "batch 1556: loss 0.064388\n",
            "batch 1557: loss 0.043169\n",
            "batch 1558: loss 0.156724\n",
            "batch 1559: loss 0.052881\n",
            "batch 1560: loss 0.354280\n",
            "batch 1561: loss 0.147982\n",
            "batch 1562: loss 0.309179\n",
            "batch 1563: loss 0.254655\n",
            "batch 1564: loss 0.193736\n",
            "batch 1565: loss 0.115213\n",
            "batch 1566: loss 0.293264\n",
            "batch 1567: loss 0.258972\n",
            "batch 1568: loss 0.107633\n",
            "batch 1569: loss 0.153627\n",
            "batch 1570: loss 0.156887\n",
            "batch 1571: loss 0.170584\n",
            "batch 1572: loss 0.203526\n",
            "batch 1573: loss 0.279916\n",
            "batch 1574: loss 0.172795\n",
            "batch 1575: loss 0.150469\n",
            "batch 1576: loss 0.086154\n",
            "batch 1577: loss 0.100269\n",
            "batch 1578: loss 0.162224\n",
            "batch 1579: loss 0.034951\n",
            "batch 1580: loss 0.111976\n",
            "batch 1581: loss 0.029568\n",
            "batch 1582: loss 0.206589\n",
            "batch 1583: loss 0.105329\n",
            "batch 1584: loss 0.125878\n",
            "batch 1585: loss 0.295543\n",
            "batch 1586: loss 0.150782\n",
            "batch 1587: loss 0.094471\n",
            "batch 1588: loss 0.084747\n",
            "batch 1589: loss 0.172581\n",
            "batch 1590: loss 0.218276\n",
            "batch 1591: loss 0.135891\n",
            "batch 1592: loss 0.157386\n",
            "batch 1593: loss 0.165269\n",
            "batch 1594: loss 0.213036\n",
            "batch 1595: loss 0.229938\n",
            "batch 1596: loss 0.093433\n",
            "batch 1597: loss 0.114342\n",
            "batch 1598: loss 0.064206\n",
            "batch 1599: loss 0.135585\n",
            "batch 1600: loss 0.111476\n",
            "batch 1601: loss 0.060810\n",
            "batch 1602: loss 0.228336\n",
            "batch 1603: loss 0.176482\n",
            "batch 1604: loss 0.046190\n",
            "batch 1605: loss 0.272214\n",
            "batch 1606: loss 0.114451\n",
            "batch 1607: loss 0.165190\n",
            "batch 1608: loss 0.123729\n",
            "batch 1609: loss 0.073900\n",
            "batch 1610: loss 0.074190\n",
            "batch 1611: loss 0.259877\n",
            "batch 1612: loss 0.127274\n",
            "batch 1613: loss 0.091605\n",
            "batch 1614: loss 0.153385\n",
            "batch 1615: loss 0.185196\n",
            "batch 1616: loss 0.121048\n",
            "batch 1617: loss 0.183795\n",
            "batch 1618: loss 0.087593\n",
            "batch 1619: loss 0.209184\n",
            "batch 1620: loss 0.143122\n",
            "batch 1621: loss 0.150515\n",
            "batch 1622: loss 0.057726\n",
            "batch 1623: loss 0.166359\n",
            "batch 1624: loss 0.109768\n",
            "batch 1625: loss 0.136249\n",
            "batch 1626: loss 0.070262\n",
            "batch 1627: loss 0.097660\n",
            "batch 1628: loss 0.104426\n",
            "batch 1629: loss 0.243308\n",
            "batch 1630: loss 0.140035\n",
            "batch 1631: loss 0.198209\n",
            "batch 1632: loss 0.169381\n",
            "batch 1633: loss 0.139953\n",
            "batch 1634: loss 0.155279\n",
            "batch 1635: loss 0.117805\n",
            "batch 1636: loss 0.291366\n",
            "batch 1637: loss 0.195649\n",
            "batch 1638: loss 0.066984\n",
            "batch 1639: loss 0.171310\n",
            "batch 1640: loss 0.210405\n",
            "batch 1641: loss 0.188834\n",
            "batch 1642: loss 0.054866\n",
            "batch 1643: loss 0.288828\n",
            "batch 1644: loss 0.137391\n",
            "batch 1645: loss 0.052173\n",
            "batch 1646: loss 0.150987\n",
            "batch 1647: loss 0.150815\n",
            "batch 1648: loss 0.039105\n",
            "batch 1649: loss 0.070371\n",
            "batch 1650: loss 0.131536\n",
            "batch 1651: loss 0.116380\n",
            "batch 1652: loss 0.166179\n",
            "batch 1653: loss 0.312966\n",
            "batch 1654: loss 0.162133\n",
            "batch 1655: loss 0.173129\n",
            "batch 1656: loss 0.190838\n",
            "batch 1657: loss 0.107804\n",
            "batch 1658: loss 0.171376\n",
            "batch 1659: loss 0.124155\n",
            "batch 1660: loss 0.317378\n",
            "batch 1661: loss 0.241053\n",
            "batch 1662: loss 0.180637\n",
            "batch 1663: loss 0.099170\n",
            "batch 1664: loss 0.144306\n",
            "batch 1665: loss 0.197919\n",
            "batch 1666: loss 0.140767\n",
            "batch 1667: loss 0.105819\n",
            "batch 1668: loss 0.046357\n",
            "batch 1669: loss 0.057370\n",
            "batch 1670: loss 0.071804\n",
            "batch 1671: loss 0.327097\n",
            "batch 1672: loss 0.265446\n",
            "batch 1673: loss 0.080052\n",
            "batch 1674: loss 0.073251\n",
            "batch 1675: loss 0.079434\n",
            "batch 1676: loss 0.058651\n",
            "batch 1677: loss 0.424426\n",
            "batch 1678: loss 0.182527\n",
            "batch 1679: loss 0.156515\n",
            "batch 1680: loss 0.092779\n",
            "batch 1681: loss 0.117877\n",
            "batch 1682: loss 0.144498\n",
            "batch 1683: loss 0.127730\n",
            "batch 1684: loss 0.039250\n",
            "batch 1685: loss 0.089434\n",
            "batch 1686: loss 0.127699\n",
            "batch 1687: loss 0.354326\n",
            "batch 1688: loss 0.274738\n",
            "batch 1689: loss 0.089694\n",
            "batch 1690: loss 0.070117\n",
            "batch 1691: loss 0.206415\n",
            "batch 1692: loss 0.221896\n",
            "batch 1693: loss 0.170888\n",
            "batch 1694: loss 0.173871\n",
            "batch 1695: loss 0.069248\n",
            "batch 1696: loss 0.148501\n",
            "batch 1697: loss 0.224699\n",
            "batch 1698: loss 0.092377\n",
            "batch 1699: loss 0.124336\n",
            "batch 1700: loss 0.116493\n",
            "batch 1701: loss 0.137001\n",
            "batch 1702: loss 0.153381\n",
            "batch 1703: loss 0.208934\n",
            "batch 1704: loss 0.070481\n",
            "batch 1705: loss 0.086878\n",
            "batch 1706: loss 0.513018\n",
            "batch 1707: loss 0.125270\n",
            "batch 1708: loss 0.073977\n",
            "batch 1709: loss 0.175755\n",
            "batch 1710: loss 0.103972\n",
            "batch 1711: loss 0.069751\n",
            "batch 1712: loss 0.131773\n",
            "batch 1713: loss 0.167005\n",
            "batch 1714: loss 0.118123\n",
            "batch 1715: loss 0.144601\n",
            "batch 1716: loss 0.102287\n",
            "batch 1717: loss 0.037325\n",
            "batch 1718: loss 0.142715\n",
            "batch 1719: loss 0.068946\n",
            "batch 1720: loss 0.085383\n",
            "batch 1721: loss 0.167476\n",
            "batch 1722: loss 0.101013\n",
            "batch 1723: loss 0.085366\n",
            "batch 1724: loss 0.078917\n",
            "batch 1725: loss 0.040463\n",
            "batch 1726: loss 0.208657\n",
            "batch 1727: loss 0.155231\n",
            "batch 1728: loss 0.034677\n",
            "batch 1729: loss 0.222097\n",
            "batch 1730: loss 0.162387\n",
            "batch 1731: loss 0.323018\n",
            "batch 1732: loss 0.121255\n",
            "batch 1733: loss 0.154692\n",
            "batch 1734: loss 0.069137\n",
            "batch 1735: loss 0.162623\n",
            "batch 1736: loss 0.183059\n",
            "batch 1737: loss 0.105678\n",
            "batch 1738: loss 0.164763\n",
            "batch 1739: loss 0.138849\n",
            "batch 1740: loss 0.073440\n",
            "batch 1741: loss 0.183759\n",
            "batch 1742: loss 0.063927\n",
            "batch 1743: loss 0.275554\n",
            "batch 1744: loss 0.136294\n",
            "batch 1745: loss 0.106294\n",
            "batch 1746: loss 0.052520\n",
            "batch 1747: loss 0.127479\n",
            "batch 1748: loss 0.283562\n",
            "batch 1749: loss 0.053033\n",
            "batch 1750: loss 0.046979\n",
            "batch 1751: loss 0.102212\n",
            "batch 1752: loss 0.088991\n",
            "batch 1753: loss 0.171735\n",
            "batch 1754: loss 0.054834\n",
            "batch 1755: loss 0.324463\n",
            "batch 1756: loss 0.167892\n",
            "batch 1757: loss 0.089309\n",
            "batch 1758: loss 0.137296\n",
            "batch 1759: loss 0.217274\n",
            "batch 1760: loss 0.053781\n",
            "batch 1761: loss 0.162137\n",
            "batch 1762: loss 0.196272\n",
            "batch 1763: loss 0.106202\n",
            "batch 1764: loss 0.260106\n",
            "batch 1765: loss 0.133112\n",
            "batch 1766: loss 0.050591\n",
            "batch 1767: loss 0.157606\n",
            "batch 1768: loss 0.216150\n",
            "batch 1769: loss 0.204463\n",
            "batch 1770: loss 0.093107\n",
            "batch 1771: loss 0.159622\n",
            "batch 1772: loss 0.061847\n",
            "batch 1773: loss 0.227843\n",
            "batch 1774: loss 0.071754\n",
            "batch 1775: loss 0.126148\n",
            "batch 1776: loss 0.258938\n",
            "batch 1777: loss 0.093886\n",
            "batch 1778: loss 0.129005\n",
            "batch 1779: loss 0.178045\n",
            "batch 1780: loss 0.121031\n",
            "batch 1781: loss 0.311041\n",
            "batch 1782: loss 0.152560\n",
            "batch 1783: loss 0.062883\n",
            "batch 1784: loss 0.096818\n",
            "batch 1785: loss 0.062698\n",
            "batch 1786: loss 0.065647\n",
            "batch 1787: loss 0.184046\n",
            "batch 1788: loss 0.212188\n",
            "batch 1789: loss 0.169034\n",
            "batch 1790: loss 0.192115\n",
            "batch 1791: loss 0.107098\n",
            "batch 1792: loss 0.055098\n",
            "batch 1793: loss 0.242169\n",
            "batch 1794: loss 0.272020\n",
            "batch 1795: loss 0.076051\n",
            "batch 1796: loss 0.192354\n",
            "batch 1797: loss 0.062904\n",
            "batch 1798: loss 0.171324\n",
            "batch 1799: loss 0.366727\n",
            "batch 1800: loss 0.080360\n",
            "batch 1801: loss 0.287459\n",
            "batch 1802: loss 0.040298\n",
            "batch 1803: loss 0.098521\n",
            "batch 1804: loss 0.136350\n",
            "batch 1805: loss 0.259142\n",
            "batch 1806: loss 0.152907\n",
            "batch 1807: loss 0.088187\n",
            "batch 1808: loss 0.111726\n",
            "batch 1809: loss 0.078814\n",
            "batch 1810: loss 0.124016\n",
            "batch 1811: loss 0.291935\n",
            "batch 1812: loss 0.067690\n",
            "batch 1813: loss 0.188587\n",
            "batch 1814: loss 0.037794\n",
            "batch 1815: loss 0.110064\n",
            "batch 1816: loss 0.054609\n",
            "batch 1817: loss 0.081796\n",
            "batch 1818: loss 0.035226\n",
            "batch 1819: loss 0.068531\n",
            "batch 1820: loss 0.045457\n",
            "batch 1821: loss 0.114738\n",
            "batch 1822: loss 0.328964\n",
            "batch 1823: loss 0.080907\n",
            "batch 1824: loss 0.153921\n",
            "batch 1825: loss 0.077598\n",
            "batch 1826: loss 0.062255\n",
            "batch 1827: loss 0.256601\n",
            "batch 1828: loss 0.296345\n",
            "batch 1829: loss 0.138831\n",
            "batch 1830: loss 0.088686\n",
            "batch 1831: loss 0.135659\n",
            "batch 1832: loss 0.120726\n",
            "batch 1833: loss 0.097449\n",
            "batch 1834: loss 0.095450\n",
            "batch 1835: loss 0.093248\n",
            "batch 1836: loss 0.064562\n",
            "batch 1837: loss 0.105992\n",
            "batch 1838: loss 0.273966\n",
            "batch 1839: loss 0.024674\n",
            "batch 1840: loss 0.150215\n",
            "batch 1841: loss 0.119721\n",
            "batch 1842: loss 0.237735\n",
            "batch 1843: loss 0.112567\n",
            "batch 1844: loss 0.103810\n",
            "batch 1845: loss 0.123619\n",
            "batch 1846: loss 0.038472\n",
            "batch 1847: loss 0.143144\n",
            "batch 1848: loss 0.110665\n",
            "batch 1849: loss 0.090617\n",
            "batch 1850: loss 0.063690\n",
            "batch 1851: loss 0.100279\n",
            "batch 1852: loss 0.048392\n",
            "batch 1853: loss 0.177302\n",
            "batch 1854: loss 0.144284\n",
            "batch 1855: loss 0.114929\n",
            "batch 1856: loss 0.173198\n",
            "batch 1857: loss 0.117804\n",
            "batch 1858: loss 0.084170\n",
            "batch 1859: loss 0.118607\n",
            "batch 1860: loss 0.037024\n",
            "batch 1861: loss 0.304072\n",
            "batch 1862: loss 0.055727\n",
            "batch 1863: loss 0.040413\n",
            "batch 1864: loss 0.162612\n",
            "batch 1865: loss 0.154978\n",
            "batch 1866: loss 0.153519\n",
            "batch 1867: loss 0.120799\n",
            "batch 1868: loss 0.279478\n",
            "batch 1869: loss 0.170311\n",
            "batch 1870: loss 0.260659\n",
            "batch 1871: loss 0.203117\n",
            "batch 1872: loss 0.063011\n",
            "batch 1873: loss 0.063726\n",
            "batch 1874: loss 0.054584\n",
            "batch 1875: loss 0.235785\n",
            "batch 1876: loss 0.197457\n",
            "batch 1877: loss 0.094568\n",
            "batch 1878: loss 0.124533\n",
            "batch 1879: loss 0.187348\n",
            "batch 1880: loss 0.031157\n",
            "batch 1881: loss 0.115810\n",
            "batch 1882: loss 0.349863\n",
            "batch 1883: loss 0.086340\n",
            "batch 1884: loss 0.160207\n",
            "batch 1885: loss 0.191361\n",
            "batch 1886: loss 0.086459\n",
            "batch 1887: loss 0.144536\n",
            "batch 1888: loss 0.043856\n",
            "batch 1889: loss 0.061405\n",
            "batch 1890: loss 0.119525\n",
            "batch 1891: loss 0.279695\n",
            "batch 1892: loss 0.147454\n",
            "batch 1893: loss 0.048105\n",
            "batch 1894: loss 0.029305\n",
            "batch 1895: loss 0.235479\n",
            "batch 1896: loss 0.108721\n",
            "batch 1897: loss 0.033026\n",
            "batch 1898: loss 0.184413\n",
            "batch 1899: loss 0.080343\n",
            "batch 1900: loss 0.228297\n",
            "batch 1901: loss 0.219151\n",
            "batch 1902: loss 0.086458\n",
            "batch 1903: loss 0.082300\n",
            "batch 1904: loss 0.049029\n",
            "batch 1905: loss 0.188150\n",
            "batch 1906: loss 0.152199\n",
            "batch 1907: loss 0.204200\n",
            "batch 1908: loss 0.121275\n",
            "batch 1909: loss 0.082029\n",
            "batch 1910: loss 0.096854\n",
            "batch 1911: loss 0.064907\n",
            "batch 1912: loss 0.128162\n",
            "batch 1913: loss 0.053521\n",
            "batch 1914: loss 0.160468\n",
            "batch 1915: loss 0.160138\n",
            "batch 1916: loss 0.298420\n",
            "batch 1917: loss 0.067244\n",
            "batch 1918: loss 0.111431\n",
            "batch 1919: loss 0.057788\n",
            "batch 1920: loss 0.086877\n",
            "batch 1921: loss 0.026689\n",
            "batch 1922: loss 0.143996\n",
            "batch 1923: loss 0.036104\n",
            "batch 1924: loss 0.099000\n",
            "batch 1925: loss 0.130514\n",
            "batch 1926: loss 0.085371\n",
            "batch 1927: loss 0.189693\n",
            "batch 1928: loss 0.053435\n",
            "batch 1929: loss 0.186975\n",
            "batch 1930: loss 0.122076\n",
            "batch 1931: loss 0.054795\n",
            "batch 1932: loss 0.143972\n",
            "batch 1933: loss 0.085804\n",
            "batch 1934: loss 0.067456\n",
            "batch 1935: loss 0.068463\n",
            "batch 1936: loss 0.412745\n",
            "batch 1937: loss 0.084757\n",
            "batch 1938: loss 0.044461\n",
            "batch 1939: loss 0.248542\n",
            "batch 1940: loss 0.191211\n",
            "batch 1941: loss 0.076495\n",
            "batch 1942: loss 0.222553\n",
            "batch 1943: loss 0.086139\n",
            "batch 1944: loss 0.116962\n",
            "batch 1945: loss 0.067327\n",
            "batch 1946: loss 0.123188\n",
            "batch 1947: loss 0.240018\n",
            "batch 1948: loss 0.092810\n",
            "batch 1949: loss 0.063325\n",
            "batch 1950: loss 0.215384\n",
            "batch 1951: loss 0.033257\n",
            "batch 1952: loss 0.127587\n",
            "batch 1953: loss 0.085196\n",
            "batch 1954: loss 0.061258\n",
            "batch 1955: loss 0.250780\n",
            "batch 1956: loss 0.070754\n",
            "batch 1957: loss 0.172638\n",
            "batch 1958: loss 0.067010\n",
            "batch 1959: loss 0.107885\n",
            "batch 1960: loss 0.093517\n",
            "batch 1961: loss 0.070049\n",
            "batch 1962: loss 0.063186\n",
            "batch 1963: loss 0.197601\n",
            "batch 1964: loss 0.068876\n",
            "batch 1965: loss 0.201429\n",
            "batch 1966: loss 0.052027\n",
            "batch 1967: loss 0.157700\n",
            "batch 1968: loss 0.161814\n",
            "batch 1969: loss 0.122713\n",
            "batch 1970: loss 0.203331\n",
            "batch 1971: loss 0.063241\n",
            "batch 1972: loss 0.273144\n",
            "batch 1973: loss 0.082575\n",
            "batch 1974: loss 0.116005\n",
            "batch 1975: loss 0.079685\n",
            "batch 1976: loss 0.019193\n",
            "batch 1977: loss 0.056945\n",
            "batch 1978: loss 0.069579\n",
            "batch 1979: loss 0.356343\n",
            "batch 1980: loss 0.061590\n",
            "batch 1981: loss 0.069247\n",
            "batch 1982: loss 0.098069\n",
            "batch 1983: loss 0.033094\n",
            "batch 1984: loss 0.211290\n",
            "batch 1985: loss 0.104330\n",
            "batch 1986: loss 0.068493\n",
            "batch 1987: loss 0.049151\n",
            "batch 1988: loss 0.141433\n",
            "batch 1989: loss 0.025286\n",
            "batch 1990: loss 0.098565\n",
            "batch 1991: loss 0.165832\n",
            "batch 1992: loss 0.245913\n",
            "batch 1993: loss 0.112875\n",
            "batch 1994: loss 0.175952\n",
            "batch 1995: loss 0.183623\n",
            "batch 1996: loss 0.040100\n",
            "batch 1997: loss 0.240534\n",
            "batch 1998: loss 0.057736\n",
            "batch 1999: loss 0.082604\n",
            "batch 2000: loss 0.120885\n",
            "batch 2001: loss 0.099729\n",
            "batch 2002: loss 0.066592\n",
            "batch 2003: loss 0.036739\n",
            "batch 2004: loss 0.259542\n",
            "batch 2005: loss 0.193800\n",
            "batch 2006: loss 0.111475\n",
            "batch 2007: loss 0.139812\n",
            "batch 2008: loss 0.201624\n",
            "batch 2009: loss 0.081350\n",
            "batch 2010: loss 0.320068\n",
            "batch 2011: loss 0.073525\n",
            "batch 2012: loss 0.145103\n",
            "batch 2013: loss 0.130065\n",
            "batch 2014: loss 0.178815\n",
            "batch 2015: loss 0.185803\n",
            "batch 2016: loss 0.089496\n",
            "batch 2017: loss 0.039378\n",
            "batch 2018: loss 0.130908\n",
            "batch 2019: loss 0.129564\n",
            "batch 2020: loss 0.151535\n",
            "batch 2021: loss 0.029101\n",
            "batch 2022: loss 0.175862\n",
            "batch 2023: loss 0.097613\n",
            "batch 2024: loss 0.075312\n",
            "batch 2025: loss 0.104817\n",
            "batch 2026: loss 0.066019\n",
            "batch 2027: loss 0.120806\n",
            "batch 2028: loss 0.224342\n",
            "batch 2029: loss 0.248162\n",
            "batch 2030: loss 0.045346\n",
            "batch 2031: loss 0.069641\n",
            "batch 2032: loss 0.123686\n",
            "batch 2033: loss 0.086741\n",
            "batch 2034: loss 0.122349\n",
            "batch 2035: loss 0.064582\n",
            "batch 2036: loss 0.150313\n",
            "batch 2037: loss 0.076574\n",
            "batch 2038: loss 0.075657\n",
            "batch 2039: loss 0.083110\n",
            "batch 2040: loss 0.225211\n",
            "batch 2041: loss 0.108545\n",
            "batch 2042: loss 0.097057\n",
            "batch 2043: loss 0.030689\n",
            "batch 2044: loss 0.112471\n",
            "batch 2045: loss 0.027052\n",
            "batch 2046: loss 0.175578\n",
            "batch 2047: loss 0.119684\n",
            "batch 2048: loss 0.110754\n",
            "batch 2049: loss 0.124552\n",
            "batch 2050: loss 0.186069\n",
            "batch 2051: loss 0.065671\n",
            "batch 2052: loss 0.057556\n",
            "batch 2053: loss 0.167677\n",
            "batch 2054: loss 0.064499\n",
            "batch 2055: loss 0.119431\n",
            "batch 2056: loss 0.066393\n",
            "batch 2057: loss 0.203053\n",
            "batch 2058: loss 0.093452\n",
            "batch 2059: loss 0.140836\n",
            "batch 2060: loss 0.022473\n",
            "batch 2061: loss 0.089363\n",
            "batch 2062: loss 0.068862\n",
            "batch 2063: loss 0.030807\n",
            "batch 2064: loss 0.137574\n",
            "batch 2065: loss 0.207215\n",
            "batch 2066: loss 0.056148\n",
            "batch 2067: loss 0.091679\n",
            "batch 2068: loss 0.154286\n",
            "batch 2069: loss 0.036511\n",
            "batch 2070: loss 0.118912\n",
            "batch 2071: loss 0.273641\n",
            "batch 2072: loss 0.067054\n",
            "batch 2073: loss 0.248666\n",
            "batch 2074: loss 0.043827\n",
            "batch 2075: loss 0.145887\n",
            "batch 2076: loss 0.152117\n",
            "batch 2077: loss 0.071909\n",
            "batch 2078: loss 0.026397\n",
            "batch 2079: loss 0.023478\n",
            "batch 2080: loss 0.054944\n",
            "batch 2081: loss 0.072440\n",
            "batch 2082: loss 0.122139\n",
            "batch 2083: loss 0.080853\n",
            "batch 2084: loss 0.044852\n",
            "batch 2085: loss 0.099120\n",
            "batch 2086: loss 0.026573\n",
            "batch 2087: loss 0.179797\n",
            "batch 2088: loss 0.051611\n",
            "batch 2089: loss 0.068549\n",
            "batch 2090: loss 0.196242\n",
            "batch 2091: loss 0.061054\n",
            "batch 2092: loss 0.110761\n",
            "batch 2093: loss 0.118925\n",
            "batch 2094: loss 0.047385\n",
            "batch 2095: loss 0.071021\n",
            "batch 2096: loss 0.065111\n",
            "batch 2097: loss 0.146821\n",
            "batch 2098: loss 0.045078\n",
            "batch 2099: loss 0.137597\n",
            "batch 2100: loss 0.100649\n",
            "batch 2101: loss 0.207057\n",
            "batch 2102: loss 0.247951\n",
            "batch 2103: loss 0.079669\n",
            "batch 2104: loss 0.056860\n",
            "batch 2105: loss 0.047823\n",
            "batch 2106: loss 0.053512\n",
            "batch 2107: loss 0.192968\n",
            "batch 2108: loss 0.116216\n",
            "batch 2109: loss 0.120762\n",
            "batch 2110: loss 0.152117\n",
            "batch 2111: loss 0.160181\n",
            "batch 2112: loss 0.287908\n",
            "batch 2113: loss 0.307743\n",
            "batch 2114: loss 0.045442\n",
            "batch 2115: loss 0.178419\n",
            "batch 2116: loss 0.190053\n",
            "batch 2117: loss 0.123536\n",
            "batch 2118: loss 0.080614\n",
            "batch 2119: loss 0.074704\n",
            "batch 2120: loss 0.089430\n",
            "batch 2121: loss 0.142233\n",
            "batch 2122: loss 0.180499\n",
            "batch 2123: loss 0.062061\n",
            "batch 2124: loss 0.084339\n",
            "batch 2125: loss 0.190734\n",
            "batch 2126: loss 0.122533\n",
            "batch 2127: loss 0.026303\n",
            "batch 2128: loss 0.121272\n",
            "batch 2129: loss 0.151968\n",
            "batch 2130: loss 0.085808\n",
            "batch 2131: loss 0.023842\n",
            "batch 2132: loss 0.121087\n",
            "batch 2133: loss 0.090952\n",
            "batch 2134: loss 0.071558\n",
            "batch 2135: loss 0.132596\n",
            "batch 2136: loss 0.158555\n",
            "batch 2137: loss 0.193065\n",
            "batch 2138: loss 0.163015\n",
            "batch 2139: loss 0.090049\n",
            "batch 2140: loss 0.119169\n",
            "batch 2141: loss 0.148938\n",
            "batch 2142: loss 0.119393\n",
            "batch 2143: loss 0.118317\n",
            "batch 2144: loss 0.123342\n",
            "batch 2145: loss 0.133021\n",
            "batch 2146: loss 0.126747\n",
            "batch 2147: loss 0.037587\n",
            "batch 2148: loss 0.146277\n",
            "batch 2149: loss 0.189011\n",
            "batch 2150: loss 0.100423\n",
            "batch 2151: loss 0.022670\n",
            "batch 2152: loss 0.061112\n",
            "batch 2153: loss 0.054941\n",
            "batch 2154: loss 0.048027\n",
            "batch 2155: loss 0.171933\n",
            "batch 2156: loss 0.079915\n",
            "batch 2157: loss 0.022306\n",
            "batch 2158: loss 0.035323\n",
            "batch 2159: loss 0.058125\n",
            "batch 2160: loss 0.149079\n",
            "batch 2161: loss 0.087497\n",
            "batch 2162: loss 0.122840\n",
            "batch 2163: loss 0.284726\n",
            "batch 2164: loss 0.075185\n",
            "batch 2165: loss 0.046286\n",
            "batch 2166: loss 0.091722\n",
            "batch 2167: loss 0.054851\n",
            "batch 2168: loss 0.089951\n",
            "batch 2169: loss 0.149336\n",
            "batch 2170: loss 0.025239\n",
            "batch 2171: loss 0.053300\n",
            "batch 2172: loss 0.029419\n",
            "batch 2173: loss 0.172734\n",
            "batch 2174: loss 0.259947\n",
            "batch 2175: loss 0.030477\n",
            "batch 2176: loss 0.090135\n",
            "batch 2177: loss 0.161665\n",
            "batch 2178: loss 0.050649\n",
            "batch 2179: loss 0.117791\n",
            "batch 2180: loss 0.128095\n",
            "batch 2181: loss 0.032950\n",
            "batch 2182: loss 0.071729\n",
            "batch 2183: loss 0.109271\n",
            "batch 2184: loss 0.208088\n",
            "batch 2185: loss 0.074179\n",
            "batch 2186: loss 0.149475\n",
            "batch 2187: loss 0.104233\n",
            "batch 2188: loss 0.176395\n",
            "batch 2189: loss 0.208154\n",
            "batch 2190: loss 0.184086\n",
            "batch 2191: loss 0.244642\n",
            "batch 2192: loss 0.118890\n",
            "batch 2193: loss 0.037702\n",
            "batch 2194: loss 0.292887\n",
            "batch 2195: loss 0.065625\n",
            "batch 2196: loss 0.051620\n",
            "batch 2197: loss 0.166678\n",
            "batch 2198: loss 0.042678\n",
            "batch 2199: loss 0.234107\n",
            "batch 2200: loss 0.128441\n",
            "batch 2201: loss 0.136624\n",
            "batch 2202: loss 0.038807\n",
            "batch 2203: loss 0.061041\n",
            "batch 2204: loss 0.059435\n",
            "batch 2205: loss 0.110113\n",
            "batch 2206: loss 0.033587\n",
            "batch 2207: loss 0.105730\n",
            "batch 2208: loss 0.095006\n",
            "batch 2209: loss 0.066485\n",
            "batch 2210: loss 0.205392\n",
            "batch 2211: loss 0.061315\n",
            "batch 2212: loss 0.038506\n",
            "batch 2213: loss 0.124494\n",
            "batch 2214: loss 0.095140\n",
            "batch 2215: loss 0.130245\n",
            "batch 2216: loss 0.065039\n",
            "batch 2217: loss 0.056436\n",
            "batch 2218: loss 0.091600\n",
            "batch 2219: loss 0.044789\n",
            "batch 2220: loss 0.298238\n",
            "batch 2221: loss 0.185181\n",
            "batch 2222: loss 0.177283\n",
            "batch 2223: loss 0.099300\n",
            "batch 2224: loss 0.117214\n",
            "batch 2225: loss 0.075204\n",
            "batch 2226: loss 0.037427\n",
            "batch 2227: loss 0.140096\n",
            "batch 2228: loss 0.073313\n",
            "batch 2229: loss 0.118868\n",
            "batch 2230: loss 0.099896\n",
            "batch 2231: loss 0.102894\n",
            "batch 2232: loss 0.091988\n",
            "batch 2233: loss 0.040140\n",
            "batch 2234: loss 0.089580\n",
            "batch 2235: loss 0.102402\n",
            "batch 2236: loss 0.381345\n",
            "batch 2237: loss 0.092051\n",
            "batch 2238: loss 0.119950\n",
            "batch 2239: loss 0.153877\n",
            "batch 2240: loss 0.125002\n",
            "batch 2241: loss 0.120535\n",
            "batch 2242: loss 0.131096\n",
            "batch 2243: loss 0.072759\n",
            "batch 2244: loss 0.097261\n",
            "batch 2245: loss 0.210420\n",
            "batch 2246: loss 0.066397\n",
            "batch 2247: loss 0.061995\n",
            "batch 2248: loss 0.021810\n",
            "batch 2249: loss 0.250387\n",
            "batch 2250: loss 0.126919\n",
            "batch 2251: loss 0.104632\n",
            "batch 2252: loss 0.138523\n",
            "batch 2253: loss 0.290885\n",
            "batch 2254: loss 0.066551\n",
            "batch 2255: loss 0.067654\n",
            "batch 2256: loss 0.019768\n",
            "batch 2257: loss 0.142730\n",
            "batch 2258: loss 0.129486\n",
            "batch 2259: loss 0.063754\n",
            "batch 2260: loss 0.016834\n",
            "batch 2261: loss 0.150034\n",
            "batch 2262: loss 0.237483\n",
            "batch 2263: loss 0.112432\n",
            "batch 2264: loss 0.199799\n",
            "batch 2265: loss 0.098801\n",
            "batch 2266: loss 0.190750\n",
            "batch 2267: loss 0.102967\n",
            "batch 2268: loss 0.223904\n",
            "batch 2269: loss 0.085945\n",
            "batch 2270: loss 0.043898\n",
            "batch 2271: loss 0.034168\n",
            "batch 2272: loss 0.135449\n",
            "batch 2273: loss 0.087962\n",
            "batch 2274: loss 0.064071\n",
            "batch 2275: loss 0.068426\n",
            "batch 2276: loss 0.161541\n",
            "batch 2277: loss 0.052010\n",
            "batch 2278: loss 0.117045\n",
            "batch 2279: loss 0.102201\n",
            "batch 2280: loss 0.059973\n",
            "batch 2281: loss 0.093646\n",
            "batch 2282: loss 0.113069\n",
            "batch 2283: loss 0.195949\n",
            "batch 2284: loss 0.113338\n",
            "batch 2285: loss 0.079771\n",
            "batch 2286: loss 0.097446\n",
            "batch 2287: loss 0.098374\n",
            "batch 2288: loss 0.205148\n",
            "batch 2289: loss 0.234654\n",
            "batch 2290: loss 0.029972\n",
            "batch 2291: loss 0.111903\n",
            "batch 2292: loss 0.153975\n",
            "batch 2293: loss 0.113531\n",
            "batch 2294: loss 0.154435\n",
            "batch 2295: loss 0.046850\n",
            "batch 2296: loss 0.140083\n",
            "batch 2297: loss 0.126411\n",
            "batch 2298: loss 0.054637\n",
            "batch 2299: loss 0.096455\n",
            "batch 2300: loss 0.118338\n",
            "batch 2301: loss 0.128113\n",
            "batch 2302: loss 0.124556\n",
            "batch 2303: loss 0.028292\n",
            "batch 2304: loss 0.171159\n",
            "batch 2305: loss 0.029826\n",
            "batch 2306: loss 0.140303\n",
            "batch 2307: loss 0.036086\n",
            "batch 2308: loss 0.168113\n",
            "batch 2309: loss 0.037943\n",
            "batch 2310: loss 0.096908\n",
            "batch 2311: loss 0.157655\n",
            "batch 2312: loss 0.139566\n",
            "batch 2313: loss 0.086202\n",
            "batch 2314: loss 0.120089\n",
            "batch 2315: loss 0.281969\n",
            "batch 2316: loss 0.223302\n",
            "batch 2317: loss 0.198316\n",
            "batch 2318: loss 0.232512\n",
            "batch 2319: loss 0.071937\n",
            "batch 2320: loss 0.061905\n",
            "batch 2321: loss 0.108584\n",
            "batch 2322: loss 0.149864\n",
            "batch 2323: loss 0.127430\n",
            "batch 2324: loss 0.131278\n",
            "batch 2325: loss 0.160725\n",
            "batch 2326: loss 0.095550\n",
            "batch 2327: loss 0.050070\n",
            "batch 2328: loss 0.100547\n",
            "batch 2329: loss 0.045181\n",
            "batch 2330: loss 0.212267\n",
            "batch 2331: loss 0.265552\n",
            "batch 2332: loss 0.053136\n",
            "batch 2333: loss 0.234229\n",
            "batch 2334: loss 0.088068\n",
            "batch 2335: loss 0.041249\n",
            "batch 2336: loss 0.081507\n",
            "batch 2337: loss 0.049548\n",
            "batch 2338: loss 0.051006\n",
            "batch 2339: loss 0.052385\n",
            "batch 2340: loss 0.028490\n",
            "batch 2341: loss 0.068185\n",
            "batch 2342: loss 0.106726\n",
            "batch 2343: loss 0.123307\n",
            "batch 2344: loss 0.091896\n",
            "batch 2345: loss 0.146368\n",
            "batch 2346: loss 0.139967\n",
            "batch 2347: loss 0.034263\n",
            "batch 2348: loss 0.106157\n",
            "batch 2349: loss 0.037737\n",
            "batch 2350: loss 0.066324\n",
            "batch 2351: loss 0.048111\n",
            "batch 2352: loss 0.155661\n",
            "batch 2353: loss 0.155976\n",
            "batch 2354: loss 0.097932\n",
            "batch 2355: loss 0.113857\n",
            "batch 2356: loss 0.103680\n",
            "batch 2357: loss 0.081857\n",
            "batch 2358: loss 0.080119\n",
            "batch 2359: loss 0.053618\n",
            "batch 2360: loss 0.319992\n",
            "batch 2361: loss 0.089990\n",
            "batch 2362: loss 0.056615\n",
            "batch 2363: loss 0.125369\n",
            "batch 2364: loss 0.059428\n",
            "batch 2365: loss 0.148700\n",
            "batch 2366: loss 0.117164\n",
            "batch 2367: loss 0.083177\n",
            "batch 2368: loss 0.109223\n",
            "batch 2369: loss 0.168305\n",
            "batch 2370: loss 0.159444\n",
            "batch 2371: loss 0.030423\n",
            "batch 2372: loss 0.129099\n",
            "batch 2373: loss 0.155020\n",
            "batch 2374: loss 0.145532\n",
            "batch 2375: loss 0.180992\n",
            "batch 2376: loss 0.056559\n",
            "batch 2377: loss 0.224438\n",
            "batch 2378: loss 0.072513\n",
            "batch 2379: loss 0.048703\n",
            "batch 2380: loss 0.071871\n",
            "batch 2381: loss 0.036591\n",
            "batch 2382: loss 0.152849\n",
            "batch 2383: loss 0.116703\n",
            "batch 2384: loss 0.033603\n",
            "batch 2385: loss 0.048432\n",
            "batch 2386: loss 0.028874\n",
            "batch 2387: loss 0.057936\n",
            "batch 2388: loss 0.231121\n",
            "batch 2389: loss 0.051479\n",
            "batch 2390: loss 0.047879\n",
            "batch 2391: loss 0.063571\n",
            "batch 2392: loss 0.063158\n",
            "batch 2393: loss 0.132022\n",
            "batch 2394: loss 0.044674\n",
            "batch 2395: loss 0.166910\n",
            "batch 2396: loss 0.071183\n",
            "batch 2397: loss 0.081620\n",
            "batch 2398: loss 0.037915\n",
            "batch 2399: loss 0.170816\n",
            "batch 2400: loss 0.021384\n",
            "batch 2401: loss 0.162471\n",
            "batch 2402: loss 0.125002\n",
            "batch 2403: loss 0.037417\n",
            "batch 2404: loss 0.289700\n",
            "batch 2405: loss 0.159351\n",
            "batch 2406: loss 0.170268\n",
            "batch 2407: loss 0.065396\n",
            "batch 2408: loss 0.135860\n",
            "batch 2409: loss 0.063603\n",
            "batch 2410: loss 0.057538\n",
            "batch 2411: loss 0.123137\n",
            "batch 2412: loss 0.026880\n",
            "batch 2413: loss 0.057125\n",
            "batch 2414: loss 0.107702\n",
            "batch 2415: loss 0.052730\n",
            "batch 2416: loss 0.108978\n",
            "batch 2417: loss 0.073514\n",
            "batch 2418: loss 0.093062\n",
            "batch 2419: loss 0.033016\n",
            "batch 2420: loss 0.029687\n",
            "batch 2421: loss 0.044955\n",
            "batch 2422: loss 0.189633\n",
            "batch 2423: loss 0.036452\n",
            "batch 2424: loss 0.118840\n",
            "batch 2425: loss 0.086188\n",
            "batch 2426: loss 0.017439\n",
            "batch 2427: loss 0.065722\n",
            "batch 2428: loss 0.051692\n",
            "batch 2429: loss 0.156873\n",
            "batch 2430: loss 0.154197\n",
            "batch 2431: loss 0.106088\n",
            "batch 2432: loss 0.115970\n",
            "batch 2433: loss 0.083255\n",
            "batch 2434: loss 0.081839\n",
            "batch 2435: loss 0.126743\n",
            "batch 2436: loss 0.179453\n",
            "batch 2437: loss 0.147390\n",
            "batch 2438: loss 0.048109\n",
            "batch 2439: loss 0.100549\n",
            "batch 2440: loss 0.191241\n",
            "batch 2441: loss 0.118708\n",
            "batch 2442: loss 0.152792\n",
            "batch 2443: loss 0.027990\n",
            "batch 2444: loss 0.120624\n",
            "batch 2445: loss 0.042245\n",
            "batch 2446: loss 0.063162\n",
            "batch 2447: loss 0.071725\n",
            "batch 2448: loss 0.157673\n",
            "batch 2449: loss 0.025569\n",
            "batch 2450: loss 0.047851\n",
            "batch 2451: loss 0.052462\n",
            "batch 2452: loss 0.293540\n",
            "batch 2453: loss 0.096671\n",
            "batch 2454: loss 0.070091\n",
            "batch 2455: loss 0.084133\n",
            "batch 2456: loss 0.151158\n",
            "batch 2457: loss 0.146815\n",
            "batch 2458: loss 0.145155\n",
            "batch 2459: loss 0.169448\n",
            "batch 2460: loss 0.051627\n",
            "batch 2461: loss 0.110853\n",
            "batch 2462: loss 0.163778\n",
            "batch 2463: loss 0.043972\n",
            "batch 2464: loss 0.078394\n",
            "batch 2465: loss 0.084274\n",
            "batch 2466: loss 0.139336\n",
            "batch 2467: loss 0.153865\n",
            "batch 2468: loss 0.182141\n",
            "batch 2469: loss 0.080884\n",
            "batch 2470: loss 0.062389\n",
            "batch 2471: loss 0.065866\n",
            "batch 2472: loss 0.088889\n",
            "batch 2473: loss 0.141052\n",
            "batch 2474: loss 0.104551\n",
            "batch 2475: loss 0.299832\n",
            "batch 2476: loss 0.072993\n",
            "batch 2477: loss 0.029268\n",
            "batch 2478: loss 0.021918\n",
            "batch 2479: loss 0.099113\n",
            "batch 2480: loss 0.229834\n",
            "batch 2481: loss 0.121135\n",
            "batch 2482: loss 0.043240\n",
            "batch 2483: loss 0.087680\n",
            "batch 2484: loss 0.146153\n",
            "batch 2485: loss 0.181219\n",
            "batch 2486: loss 0.172586\n",
            "batch 2487: loss 0.108721\n",
            "batch 2488: loss 0.050373\n",
            "batch 2489: loss 0.175811\n",
            "batch 2490: loss 0.079739\n",
            "batch 2491: loss 0.089556\n",
            "batch 2492: loss 0.137481\n",
            "batch 2493: loss 0.031375\n",
            "batch 2494: loss 0.166932\n",
            "batch 2495: loss 0.112982\n",
            "batch 2496: loss 0.042996\n",
            "batch 2497: loss 0.078801\n",
            "batch 2498: loss 0.031735\n",
            "batch 2499: loss 0.162634\n",
            "batch 2500: loss 0.060267\n",
            "batch 2501: loss 0.082199\n",
            "batch 2502: loss 0.126127\n",
            "batch 2503: loss 0.026502\n",
            "batch 2504: loss 0.054358\n",
            "batch 2505: loss 0.016762\n",
            "batch 2506: loss 0.188120\n",
            "batch 2507: loss 0.109970\n",
            "batch 2508: loss 0.215533\n",
            "batch 2509: loss 0.061820\n",
            "batch 2510: loss 0.033590\n",
            "batch 2511: loss 0.192423\n",
            "batch 2512: loss 0.059000\n",
            "batch 2513: loss 0.046843\n",
            "batch 2514: loss 0.027002\n",
            "batch 2515: loss 0.083506\n",
            "batch 2516: loss 0.046746\n",
            "batch 2517: loss 0.174653\n",
            "batch 2518: loss 0.061622\n",
            "batch 2519: loss 0.161036\n",
            "batch 2520: loss 0.086720\n",
            "batch 2521: loss 0.152181\n",
            "batch 2522: loss 0.190795\n",
            "batch 2523: loss 0.047306\n",
            "batch 2524: loss 0.221772\n",
            "batch 2525: loss 0.172881\n",
            "batch 2526: loss 0.148635\n",
            "batch 2527: loss 0.091958\n",
            "batch 2528: loss 0.050415\n",
            "batch 2529: loss 0.053096\n",
            "batch 2530: loss 0.108204\n",
            "batch 2531: loss 0.140315\n",
            "batch 2532: loss 0.118280\n",
            "batch 2533: loss 0.399473\n",
            "batch 2534: loss 0.076843\n",
            "batch 2535: loss 0.174287\n",
            "batch 2536: loss 0.144651\n",
            "batch 2537: loss 0.115429\n",
            "batch 2538: loss 0.031085\n",
            "batch 2539: loss 0.052044\n",
            "batch 2540: loss 0.068622\n",
            "batch 2541: loss 0.027374\n",
            "batch 2542: loss 0.048669\n",
            "batch 2543: loss 0.053062\n",
            "batch 2544: loss 0.033354\n",
            "batch 2545: loss 0.079402\n",
            "batch 2546: loss 0.175045\n",
            "batch 2547: loss 0.037375\n",
            "batch 2548: loss 0.089583\n",
            "batch 2549: loss 0.082475\n",
            "batch 2550: loss 0.086065\n",
            "batch 2551: loss 0.120713\n",
            "batch 2552: loss 0.147337\n",
            "batch 2553: loss 0.046114\n",
            "batch 2554: loss 0.278577\n",
            "batch 2555: loss 0.075242\n",
            "batch 2556: loss 0.035768\n",
            "batch 2557: loss 0.016825\n",
            "batch 2558: loss 0.091256\n",
            "batch 2559: loss 0.042870\n",
            "batch 2560: loss 0.164611\n",
            "batch 2561: loss 0.067251\n",
            "batch 2562: loss 0.146902\n",
            "batch 2563: loss 0.056729\n",
            "batch 2564: loss 0.040306\n",
            "batch 2565: loss 0.134640\n",
            "batch 2566: loss 0.198478\n",
            "batch 2567: loss 0.037474\n",
            "batch 2568: loss 0.161607\n",
            "batch 2569: loss 0.114715\n",
            "batch 2570: loss 0.033543\n",
            "batch 2571: loss 0.093744\n",
            "batch 2572: loss 0.102917\n",
            "batch 2573: loss 0.056829\n",
            "batch 2574: loss 0.082063\n",
            "batch 2575: loss 0.030428\n",
            "batch 2576: loss 0.135510\n",
            "batch 2577: loss 0.047985\n",
            "batch 2578: loss 0.094335\n",
            "batch 2579: loss 0.032267\n",
            "batch 2580: loss 0.171626\n",
            "batch 2581: loss 0.099817\n",
            "batch 2582: loss 0.082525\n",
            "batch 2583: loss 0.084984\n",
            "batch 2584: loss 0.260707\n",
            "batch 2585: loss 0.081388\n",
            "batch 2586: loss 0.028058\n",
            "batch 2587: loss 0.044642\n",
            "batch 2588: loss 0.038526\n",
            "batch 2589: loss 0.097365\n",
            "batch 2590: loss 0.081842\n",
            "batch 2591: loss 0.049798\n",
            "batch 2592: loss 0.093504\n",
            "batch 2593: loss 0.114776\n",
            "batch 2594: loss 0.056291\n",
            "batch 2595: loss 0.147913\n",
            "batch 2596: loss 0.026896\n",
            "batch 2597: loss 0.157056\n",
            "batch 2598: loss 0.042225\n",
            "batch 2599: loss 0.067843\n",
            "batch 2600: loss 0.063759\n",
            "batch 2601: loss 0.064784\n",
            "batch 2602: loss 0.042583\n",
            "batch 2603: loss 0.059372\n",
            "batch 2604: loss 0.073805\n",
            "batch 2605: loss 0.199851\n",
            "batch 2606: loss 0.126120\n",
            "batch 2607: loss 0.032196\n",
            "batch 2608: loss 0.082399\n",
            "batch 2609: loss 0.105698\n",
            "batch 2610: loss 0.029329\n",
            "batch 2611: loss 0.130803\n",
            "batch 2612: loss 0.096374\n",
            "batch 2613: loss 0.095537\n",
            "batch 2614: loss 0.151959\n",
            "batch 2615: loss 0.113581\n",
            "batch 2616: loss 0.039666\n",
            "batch 2617: loss 0.126634\n",
            "batch 2618: loss 0.120700\n",
            "batch 2619: loss 0.198946\n",
            "batch 2620: loss 0.065167\n",
            "batch 2621: loss 0.181482\n",
            "batch 2622: loss 0.145529\n",
            "batch 2623: loss 0.217849\n",
            "batch 2624: loss 0.121823\n",
            "batch 2625: loss 0.083274\n",
            "batch 2626: loss 0.100143\n",
            "batch 2627: loss 0.165887\n",
            "batch 2628: loss 0.046127\n",
            "batch 2629: loss 0.126069\n",
            "batch 2630: loss 0.016493\n",
            "batch 2631: loss 0.091556\n",
            "batch 2632: loss 0.064085\n",
            "batch 2633: loss 0.128426\n",
            "batch 2634: loss 0.076672\n",
            "batch 2635: loss 0.094313\n",
            "batch 2636: loss 0.041731\n",
            "batch 2637: loss 0.057723\n",
            "batch 2638: loss 0.248183\n",
            "batch 2639: loss 0.074246\n",
            "batch 2640: loss 0.212490\n",
            "batch 2641: loss 0.084337\n",
            "batch 2642: loss 0.197579\n",
            "batch 2643: loss 0.063886\n",
            "batch 2644: loss 0.074788\n",
            "batch 2645: loss 0.032499\n",
            "batch 2646: loss 0.047849\n",
            "batch 2647: loss 0.048943\n",
            "batch 2648: loss 0.188854\n",
            "batch 2649: loss 0.128427\n",
            "batch 2650: loss 0.266644\n",
            "batch 2651: loss 0.101060\n",
            "batch 2652: loss 0.078837\n",
            "batch 2653: loss 0.154285\n",
            "batch 2654: loss 0.081770\n",
            "batch 2655: loss 0.184112\n",
            "batch 2656: loss 0.039865\n",
            "batch 2657: loss 0.121960\n",
            "batch 2658: loss 0.160266\n",
            "batch 2659: loss 0.176695\n",
            "batch 2660: loss 0.068595\n",
            "batch 2661: loss 0.132894\n",
            "batch 2662: loss 0.056515\n",
            "batch 2663: loss 0.143375\n",
            "batch 2664: loss 0.019191\n",
            "batch 2665: loss 0.083971\n",
            "batch 2666: loss 0.281826\n",
            "batch 2667: loss 0.033542\n",
            "batch 2668: loss 0.114660\n",
            "batch 2669: loss 0.041875\n",
            "batch 2670: loss 0.216703\n",
            "batch 2671: loss 0.059894\n",
            "batch 2672: loss 0.245970\n",
            "batch 2673: loss 0.109768\n",
            "batch 2674: loss 0.092064\n",
            "batch 2675: loss 0.038069\n",
            "batch 2676: loss 0.052847\n",
            "batch 2677: loss 0.134656\n",
            "batch 2678: loss 0.078539\n",
            "batch 2679: loss 0.068212\n",
            "batch 2680: loss 0.070177\n",
            "batch 2681: loss 0.049819\n",
            "batch 2682: loss 0.234550\n",
            "batch 2683: loss 0.045533\n",
            "batch 2684: loss 0.101151\n",
            "batch 2685: loss 0.048473\n",
            "batch 2686: loss 0.074374\n",
            "batch 2687: loss 0.136680\n",
            "batch 2688: loss 0.033956\n",
            "batch 2689: loss 0.165626\n",
            "batch 2690: loss 0.126653\n",
            "batch 2691: loss 0.061936\n",
            "batch 2692: loss 0.146345\n",
            "batch 2693: loss 0.091345\n",
            "batch 2694: loss 0.040055\n",
            "batch 2695: loss 0.038298\n",
            "batch 2696: loss 0.062238\n",
            "batch 2697: loss 0.047538\n",
            "batch 2698: loss 0.030821\n",
            "batch 2699: loss 0.099216\n",
            "batch 2700: loss 0.092934\n",
            "batch 2701: loss 0.107419\n",
            "batch 2702: loss 0.058839\n",
            "batch 2703: loss 0.114224\n",
            "batch 2704: loss 0.048041\n",
            "batch 2705: loss 0.135340\n",
            "batch 2706: loss 0.223972\n",
            "batch 2707: loss 0.021056\n",
            "batch 2708: loss 0.096250\n",
            "batch 2709: loss 0.201465\n",
            "batch 2710: loss 0.094665\n",
            "batch 2711: loss 0.076331\n",
            "batch 2712: loss 0.089858\n",
            "batch 2713: loss 0.083219\n",
            "batch 2714: loss 0.126529\n",
            "batch 2715: loss 0.124359\n",
            "batch 2716: loss 0.045910\n",
            "batch 2717: loss 0.021190\n",
            "batch 2718: loss 0.123681\n",
            "batch 2719: loss 0.053295\n",
            "batch 2720: loss 0.034621\n",
            "batch 2721: loss 0.043983\n",
            "batch 2722: loss 0.110221\n",
            "batch 2723: loss 0.102120\n",
            "batch 2724: loss 0.166469\n",
            "batch 2725: loss 0.070726\n",
            "batch 2726: loss 0.077287\n",
            "batch 2727: loss 0.107773\n",
            "batch 2728: loss 0.104160\n",
            "batch 2729: loss 0.159431\n",
            "batch 2730: loss 0.068410\n",
            "batch 2731: loss 0.053591\n",
            "batch 2732: loss 0.137978\n",
            "batch 2733: loss 0.130130\n",
            "batch 2734: loss 0.158508\n",
            "batch 2735: loss 0.180124\n",
            "batch 2736: loss 0.039633\n",
            "batch 2737: loss 0.076871\n",
            "batch 2738: loss 0.095451\n",
            "batch 2739: loss 0.061801\n",
            "batch 2740: loss 0.125795\n",
            "batch 2741: loss 0.032114\n",
            "batch 2742: loss 0.100115\n",
            "batch 2743: loss 0.023325\n",
            "batch 2744: loss 0.065123\n",
            "batch 2745: loss 0.056192\n",
            "batch 2746: loss 0.183518\n",
            "batch 2747: loss 0.047169\n",
            "batch 2748: loss 0.097742\n",
            "batch 2749: loss 0.092603\n",
            "batch 2750: loss 0.044148\n",
            "batch 2751: loss 0.033705\n",
            "batch 2752: loss 0.101586\n",
            "batch 2753: loss 0.078345\n",
            "batch 2754: loss 0.072071\n",
            "batch 2755: loss 0.056476\n",
            "batch 2756: loss 0.138045\n",
            "batch 2757: loss 0.052393\n",
            "batch 2758: loss 0.034814\n",
            "batch 2759: loss 0.051840\n",
            "batch 2760: loss 0.096496\n",
            "batch 2761: loss 0.019686\n",
            "batch 2762: loss 0.031238\n",
            "batch 2763: loss 0.104229\n",
            "batch 2764: loss 0.037573\n",
            "batch 2765: loss 0.154068\n",
            "batch 2766: loss 0.207089\n",
            "batch 2767: loss 0.115054\n",
            "batch 2768: loss 0.056716\n",
            "batch 2769: loss 0.066810\n",
            "batch 2770: loss 0.279254\n",
            "batch 2771: loss 0.083113\n",
            "batch 2772: loss 0.157694\n",
            "batch 2773: loss 0.100168\n",
            "batch 2774: loss 0.146198\n",
            "batch 2775: loss 0.039675\n",
            "batch 2776: loss 0.199643\n",
            "batch 2777: loss 0.028027\n",
            "batch 2778: loss 0.069363\n",
            "batch 2779: loss 0.055176\n",
            "batch 2780: loss 0.125297\n",
            "batch 2781: loss 0.107178\n",
            "batch 2782: loss 0.164404\n",
            "batch 2783: loss 0.097576\n",
            "batch 2784: loss 0.027848\n",
            "batch 2785: loss 0.092040\n",
            "batch 2786: loss 0.060204\n",
            "batch 2787: loss 0.073186\n",
            "batch 2788: loss 0.060971\n",
            "batch 2789: loss 0.132835\n",
            "batch 2790: loss 0.009831\n",
            "batch 2791: loss 0.073550\n",
            "batch 2792: loss 0.039678\n",
            "batch 2793: loss 0.162021\n",
            "batch 2794: loss 0.072925\n",
            "batch 2795: loss 0.249300\n",
            "batch 2796: loss 0.044000\n",
            "batch 2797: loss 0.040500\n",
            "batch 2798: loss 0.052661\n",
            "batch 2799: loss 0.025019\n",
            "batch 2800: loss 0.192571\n",
            "batch 2801: loss 0.072641\n",
            "batch 2802: loss 0.053427\n",
            "batch 2803: loss 0.067466\n",
            "batch 2804: loss 0.023671\n",
            "batch 2805: loss 0.120439\n",
            "batch 2806: loss 0.088383\n",
            "batch 2807: loss 0.034144\n",
            "batch 2808: loss 0.036986\n",
            "batch 2809: loss 0.134748\n",
            "batch 2810: loss 0.091747\n",
            "batch 2811: loss 0.114362\n",
            "batch 2812: loss 0.067807\n",
            "batch 2813: loss 0.176300\n",
            "batch 2814: loss 0.040346\n",
            "batch 2815: loss 0.170418\n",
            "batch 2816: loss 0.123580\n",
            "batch 2817: loss 0.153853\n",
            "batch 2818: loss 0.073476\n",
            "batch 2819: loss 0.095435\n",
            "batch 2820: loss 0.055158\n",
            "batch 2821: loss 0.153026\n",
            "batch 2822: loss 0.048311\n",
            "batch 2823: loss 0.042312\n",
            "batch 2824: loss 0.087849\n",
            "batch 2825: loss 0.011779\n",
            "batch 2826: loss 0.019011\n",
            "batch 2827: loss 0.086730\n",
            "batch 2828: loss 0.062196\n",
            "batch 2829: loss 0.109350\n",
            "batch 2830: loss 0.108536\n",
            "batch 2831: loss 0.049231\n",
            "batch 2832: loss 0.102997\n",
            "batch 2833: loss 0.179290\n",
            "batch 2834: loss 0.145536\n",
            "batch 2835: loss 0.021175\n",
            "batch 2836: loss 0.066597\n",
            "batch 2837: loss 0.045942\n",
            "batch 2838: loss 0.066319\n",
            "batch 2839: loss 0.021629\n",
            "batch 2840: loss 0.029329\n",
            "batch 2841: loss 0.075200\n",
            "batch 2842: loss 0.030351\n",
            "batch 2843: loss 0.140413\n",
            "batch 2844: loss 0.159970\n",
            "batch 2845: loss 0.031273\n",
            "batch 2846: loss 0.047074\n",
            "batch 2847: loss 0.119744\n",
            "batch 2848: loss 0.035162\n",
            "batch 2849: loss 0.076093\n",
            "batch 2850: loss 0.290054\n",
            "batch 2851: loss 0.031069\n",
            "batch 2852: loss 0.109188\n",
            "batch 2853: loss 0.021767\n",
            "batch 2854: loss 0.246337\n",
            "batch 2855: loss 0.027377\n",
            "batch 2856: loss 0.101588\n",
            "batch 2857: loss 0.177258\n",
            "batch 2858: loss 0.009346\n",
            "batch 2859: loss 0.196206\n",
            "batch 2860: loss 0.077632\n",
            "batch 2861: loss 0.183803\n",
            "batch 2862: loss 0.052068\n",
            "batch 2863: loss 0.035753\n",
            "batch 2864: loss 0.115005\n",
            "batch 2865: loss 0.119031\n",
            "batch 2866: loss 0.062854\n",
            "batch 2867: loss 0.123962\n",
            "batch 2868: loss 0.041871\n",
            "batch 2869: loss 0.027577\n",
            "batch 2870: loss 0.044954\n",
            "batch 2871: loss 0.060887\n",
            "batch 2872: loss 0.053090\n",
            "batch 2873: loss 0.114739\n",
            "batch 2874: loss 0.109124\n",
            "batch 2875: loss 0.115380\n",
            "batch 2876: loss 0.064033\n",
            "batch 2877: loss 0.047434\n",
            "batch 2878: loss 0.179023\n",
            "batch 2879: loss 0.024902\n",
            "batch 2880: loss 0.140528\n",
            "batch 2881: loss 0.126889\n",
            "batch 2882: loss 0.053665\n",
            "batch 2883: loss 0.129662\n",
            "batch 2884: loss 0.093735\n",
            "batch 2885: loss 0.062416\n",
            "batch 2886: loss 0.123473\n",
            "batch 2887: loss 0.046120\n",
            "batch 2888: loss 0.175281\n",
            "batch 2889: loss 0.087616\n",
            "batch 2890: loss 0.073109\n",
            "batch 2891: loss 0.126487\n",
            "batch 2892: loss 0.047705\n",
            "batch 2893: loss 0.062009\n",
            "batch 2894: loss 0.024094\n",
            "batch 2895: loss 0.044404\n",
            "batch 2896: loss 0.082678\n",
            "batch 2897: loss 0.070191\n",
            "batch 2898: loss 0.260919\n",
            "batch 2899: loss 0.198842\n",
            "batch 2900: loss 0.083569\n",
            "batch 2901: loss 0.103249\n",
            "batch 2902: loss 0.159801\n",
            "batch 2903: loss 0.075193\n",
            "batch 2904: loss 0.103597\n",
            "batch 2905: loss 0.292430\n",
            "batch 2906: loss 0.019164\n",
            "batch 2907: loss 0.130562\n",
            "batch 2908: loss 0.101513\n",
            "batch 2909: loss 0.094017\n",
            "batch 2910: loss 0.032543\n",
            "batch 2911: loss 0.036399\n",
            "batch 2912: loss 0.067557\n",
            "batch 2913: loss 0.137464\n",
            "batch 2914: loss 0.114412\n",
            "batch 2915: loss 0.058572\n",
            "batch 2916: loss 0.089958\n",
            "batch 2917: loss 0.053007\n",
            "batch 2918: loss 0.016225\n",
            "batch 2919: loss 0.138795\n",
            "batch 2920: loss 0.042169\n",
            "batch 2921: loss 0.120820\n",
            "batch 2922: loss 0.179776\n",
            "batch 2923: loss 0.028903\n",
            "batch 2924: loss 0.061386\n",
            "batch 2925: loss 0.218868\n",
            "batch 2926: loss 0.049727\n",
            "batch 2927: loss 0.045254\n",
            "batch 2928: loss 0.084588\n",
            "batch 2929: loss 0.080725\n",
            "batch 2930: loss 0.084187\n",
            "batch 2931: loss 0.163725\n",
            "batch 2932: loss 0.154020\n",
            "batch 2933: loss 0.057814\n",
            "batch 2934: loss 0.026956\n",
            "batch 2935: loss 0.143002\n",
            "batch 2936: loss 0.021865\n",
            "batch 2937: loss 0.048469\n",
            "batch 2938: loss 0.221476\n",
            "batch 2939: loss 0.090166\n",
            "batch 2940: loss 0.087979\n",
            "batch 2941: loss 0.183076\n",
            "batch 2942: loss 0.063286\n",
            "batch 2943: loss 0.188722\n",
            "batch 2944: loss 0.059534\n",
            "batch 2945: loss 0.039830\n",
            "batch 2946: loss 0.044866\n",
            "batch 2947: loss 0.136219\n",
            "batch 2948: loss 0.066911\n",
            "batch 2949: loss 0.110950\n",
            "batch 2950: loss 0.041498\n",
            "batch 2951: loss 0.145411\n",
            "batch 2952: loss 0.024499\n",
            "batch 2953: loss 0.112766\n",
            "batch 2954: loss 0.037553\n",
            "batch 2955: loss 0.139355\n",
            "batch 2956: loss 0.132175\n",
            "batch 2957: loss 0.075902\n",
            "batch 2958: loss 0.103355\n",
            "batch 2959: loss 0.085119\n",
            "batch 2960: loss 0.033455\n",
            "batch 2961: loss 0.116247\n",
            "batch 2962: loss 0.094469\n",
            "batch 2963: loss 0.073011\n",
            "batch 2964: loss 0.158687\n",
            "batch 2965: loss 0.041624\n",
            "batch 2966: loss 0.128661\n",
            "batch 2967: loss 0.094034\n",
            "batch 2968: loss 0.045346\n",
            "batch 2969: loss 0.020829\n",
            "batch 2970: loss 0.109068\n",
            "batch 2971: loss 0.035984\n",
            "batch 2972: loss 0.025915\n",
            "batch 2973: loss 0.101666\n",
            "batch 2974: loss 0.031906\n",
            "batch 2975: loss 0.053480\n",
            "batch 2976: loss 0.136864\n",
            "batch 2977: loss 0.069045\n",
            "batch 2978: loss 0.028735\n",
            "batch 2979: loss 0.079489\n",
            "batch 2980: loss 0.084517\n",
            "batch 2981: loss 0.040332\n",
            "batch 2982: loss 0.071109\n",
            "batch 2983: loss 0.053934\n",
            "batch 2984: loss 0.056638\n",
            "batch 2985: loss 0.025693\n",
            "batch 2986: loss 0.158675\n",
            "batch 2987: loss 0.033443\n",
            "batch 2988: loss 0.192300\n",
            "batch 2989: loss 0.071271\n",
            "batch 2990: loss 0.025129\n",
            "batch 2991: loss 0.016835\n",
            "batch 2992: loss 0.065343\n",
            "batch 2993: loss 0.177781\n",
            "batch 2994: loss 0.049400\n",
            "batch 2995: loss 0.073310\n",
            "batch 2996: loss 0.058519\n",
            "batch 2997: loss 0.075478\n",
            "batch 2998: loss 0.064733\n",
            "batch 2999: loss 0.042357\n",
            "batch 3000: loss 0.051087\n",
            "batch 3001: loss 0.059929\n",
            "batch 3002: loss 0.295283\n",
            "batch 3003: loss 0.083021\n",
            "batch 3004: loss 0.073165\n",
            "batch 3005: loss 0.206771\n",
            "batch 3006: loss 0.075774\n",
            "batch 3007: loss 0.134740\n",
            "batch 3008: loss 0.043890\n",
            "batch 3009: loss 0.119538\n",
            "batch 3010: loss 0.056959\n",
            "batch 3011: loss 0.060974\n",
            "batch 3012: loss 0.069412\n",
            "batch 3013: loss 0.037116\n",
            "batch 3014: loss 0.069922\n",
            "batch 3015: loss 0.044497\n",
            "batch 3016: loss 0.070549\n",
            "batch 3017: loss 0.169713\n",
            "batch 3018: loss 0.033418\n",
            "batch 3019: loss 0.018083\n",
            "batch 3020: loss 0.061358\n",
            "batch 3021: loss 0.024047\n",
            "batch 3022: loss 0.120031\n",
            "batch 3023: loss 0.061385\n",
            "batch 3024: loss 0.086820\n",
            "batch 3025: loss 0.081181\n",
            "batch 3026: loss 0.378747\n",
            "batch 3027: loss 0.043580\n",
            "batch 3028: loss 0.052220\n",
            "batch 3029: loss 0.033878\n",
            "batch 3030: loss 0.077773\n",
            "batch 3031: loss 0.050171\n",
            "batch 3032: loss 0.047881\n",
            "batch 3033: loss 0.048182\n",
            "batch 3034: loss 0.046526\n",
            "batch 3035: loss 0.134936\n",
            "batch 3036: loss 0.100951\n",
            "batch 3037: loss 0.084063\n",
            "batch 3038: loss 0.159504\n",
            "batch 3039: loss 0.051854\n",
            "batch 3040: loss 0.038020\n",
            "batch 3041: loss 0.102387\n",
            "batch 3042: loss 0.084932\n",
            "batch 3043: loss 0.084906\n",
            "batch 3044: loss 0.020834\n",
            "batch 3045: loss 0.096650\n",
            "batch 3046: loss 0.052896\n",
            "batch 3047: loss 0.078415\n",
            "batch 3048: loss 0.097404\n",
            "batch 3049: loss 0.124489\n",
            "batch 3050: loss 0.171520\n",
            "batch 3051: loss 0.059294\n",
            "batch 3052: loss 0.038020\n",
            "batch 3053: loss 0.024635\n",
            "batch 3054: loss 0.048542\n",
            "batch 3055: loss 0.179437\n",
            "batch 3056: loss 0.115333\n",
            "batch 3057: loss 0.071319\n",
            "batch 3058: loss 0.070161\n",
            "batch 3059: loss 0.110103\n",
            "batch 3060: loss 0.067239\n",
            "batch 3061: loss 0.013335\n",
            "batch 3062: loss 0.054958\n",
            "batch 3063: loss 0.052251\n",
            "batch 3064: loss 0.099505\n",
            "batch 3065: loss 0.082687\n",
            "batch 3066: loss 0.041248\n",
            "batch 3067: loss 0.017835\n",
            "batch 3068: loss 0.092615\n",
            "batch 3069: loss 0.015974\n",
            "batch 3070: loss 0.094949\n",
            "batch 3071: loss 0.177731\n",
            "batch 3072: loss 0.059269\n",
            "batch 3073: loss 0.068420\n",
            "batch 3074: loss 0.107535\n",
            "batch 3075: loss 0.037099\n",
            "batch 3076: loss 0.173772\n",
            "batch 3077: loss 0.029027\n",
            "batch 3078: loss 0.038649\n",
            "batch 3079: loss 0.038482\n",
            "batch 3080: loss 0.056626\n",
            "batch 3081: loss 0.033747\n",
            "batch 3082: loss 0.039934\n",
            "batch 3083: loss 0.025688\n",
            "batch 3084: loss 0.028897\n",
            "batch 3085: loss 0.126416\n",
            "batch 3086: loss 0.049036\n",
            "batch 3087: loss 0.042586\n",
            "batch 3088: loss 0.057922\n",
            "batch 3089: loss 0.039947\n",
            "batch 3090: loss 0.161120\n",
            "batch 3091: loss 0.117557\n",
            "batch 3092: loss 0.074124\n",
            "batch 3093: loss 0.115933\n",
            "batch 3094: loss 0.027484\n",
            "batch 3095: loss 0.175629\n",
            "batch 3096: loss 0.025406\n",
            "batch 3097: loss 0.019491\n",
            "batch 3098: loss 0.097732\n",
            "batch 3099: loss 0.052357\n",
            "batch 3100: loss 0.167794\n",
            "batch 3101: loss 0.056778\n",
            "batch 3102: loss 0.164913\n",
            "batch 3103: loss 0.034592\n",
            "batch 3104: loss 0.113053\n",
            "batch 3105: loss 0.129257\n",
            "batch 3106: loss 0.050439\n",
            "batch 3107: loss 0.067542\n",
            "batch 3108: loss 0.069353\n",
            "batch 3109: loss 0.021649\n",
            "batch 3110: loss 0.166061\n",
            "batch 3111: loss 0.028412\n",
            "batch 3112: loss 0.104643\n",
            "batch 3113: loss 0.045761\n",
            "batch 3114: loss 0.066698\n",
            "batch 3115: loss 0.060218\n",
            "batch 3116: loss 0.109709\n",
            "batch 3117: loss 0.049371\n",
            "batch 3118: loss 0.115446\n",
            "batch 3119: loss 0.136408\n",
            "batch 3120: loss 0.083813\n",
            "batch 3121: loss 0.008213\n",
            "batch 3122: loss 0.025346\n",
            "batch 3123: loss 0.029005\n",
            "batch 3124: loss 0.115099\n",
            "batch 3125: loss 0.016319\n",
            "batch 3126: loss 0.024905\n",
            "batch 3127: loss 0.159974\n",
            "batch 3128: loss 0.029476\n",
            "batch 3129: loss 0.128800\n",
            "batch 3130: loss 0.027805\n",
            "batch 3131: loss 0.112667\n",
            "batch 3132: loss 0.069857\n",
            "batch 3133: loss 0.070199\n",
            "batch 3134: loss 0.044762\n",
            "batch 3135: loss 0.135911\n",
            "batch 3136: loss 0.036933\n",
            "batch 3137: loss 0.031486\n",
            "batch 3138: loss 0.103906\n",
            "batch 3139: loss 0.093991\n",
            "batch 3140: loss 0.068317\n",
            "batch 3141: loss 0.025305\n",
            "batch 3142: loss 0.095090\n",
            "batch 3143: loss 0.033527\n",
            "batch 3144: loss 0.055977\n",
            "batch 3145: loss 0.187246\n",
            "batch 3146: loss 0.260934\n",
            "batch 3147: loss 0.250335\n",
            "batch 3148: loss 0.059387\n",
            "batch 3149: loss 0.253609\n",
            "batch 3150: loss 0.039974\n",
            "batch 3151: loss 0.012389\n",
            "batch 3152: loss 0.060142\n",
            "batch 3153: loss 0.109641\n",
            "batch 3154: loss 0.324119\n",
            "batch 3155: loss 0.044101\n",
            "batch 3156: loss 0.058672\n",
            "batch 3157: loss 0.034748\n",
            "batch 3158: loss 0.053674\n",
            "batch 3159: loss 0.156633\n",
            "batch 3160: loss 0.060545\n",
            "batch 3161: loss 0.274638\n",
            "batch 3162: loss 0.106286\n",
            "batch 3163: loss 0.182469\n",
            "batch 3164: loss 0.056973\n",
            "batch 3165: loss 0.043640\n",
            "batch 3166: loss 0.124831\n",
            "batch 3167: loss 0.052675\n",
            "batch 3168: loss 0.081505\n",
            "batch 3169: loss 0.116616\n",
            "batch 3170: loss 0.117408\n",
            "batch 3171: loss 0.040711\n",
            "batch 3172: loss 0.140642\n",
            "batch 3173: loss 0.055053\n",
            "batch 3174: loss 0.161152\n",
            "batch 3175: loss 0.047344\n",
            "batch 3176: loss 0.130367\n",
            "batch 3177: loss 0.050432\n",
            "batch 3178: loss 0.033556\n",
            "batch 3179: loss 0.031346\n",
            "batch 3180: loss 0.083760\n",
            "batch 3181: loss 0.087135\n",
            "batch 3182: loss 0.108367\n",
            "batch 3183: loss 0.053820\n",
            "batch 3184: loss 0.044508\n",
            "batch 3185: loss 0.119635\n",
            "batch 3186: loss 0.061268\n",
            "batch 3187: loss 0.169921\n",
            "batch 3188: loss 0.072585\n",
            "batch 3189: loss 0.080503\n",
            "batch 3190: loss 0.033999\n",
            "batch 3191: loss 0.037896\n",
            "batch 3192: loss 0.133540\n",
            "batch 3193: loss 0.276155\n",
            "batch 3194: loss 0.057947\n",
            "batch 3195: loss 0.030364\n",
            "batch 3196: loss 0.130394\n",
            "batch 3197: loss 0.120385\n",
            "batch 3198: loss 0.187067\n",
            "batch 3199: loss 0.092527\n",
            "batch 3200: loss 0.037724\n",
            "batch 3201: loss 0.019528\n",
            "batch 3202: loss 0.019698\n",
            "batch 3203: loss 0.030254\n",
            "batch 3204: loss 0.057403\n",
            "batch 3205: loss 0.049139\n",
            "batch 3206: loss 0.150700\n",
            "batch 3207: loss 0.065347\n",
            "batch 3208: loss 0.200985\n",
            "batch 3209: loss 0.076475\n",
            "batch 3210: loss 0.295440\n",
            "batch 3211: loss 0.034063\n",
            "batch 3212: loss 0.056128\n",
            "batch 3213: loss 0.025114\n",
            "batch 3214: loss 0.117573\n",
            "batch 3215: loss 0.161207\n",
            "batch 3216: loss 0.242090\n",
            "batch 3217: loss 0.086722\n",
            "batch 3218: loss 0.221712\n",
            "batch 3219: loss 0.051258\n",
            "batch 3220: loss 0.049344\n",
            "batch 3221: loss 0.083491\n",
            "batch 3222: loss 0.021035\n",
            "batch 3223: loss 0.136489\n",
            "batch 3224: loss 0.049641\n",
            "batch 3225: loss 0.053535\n",
            "batch 3226: loss 0.041596\n",
            "batch 3227: loss 0.150787\n",
            "batch 3228: loss 0.054891\n",
            "batch 3229: loss 0.145328\n",
            "batch 3230: loss 0.017790\n",
            "batch 3231: loss 0.142287\n",
            "batch 3232: loss 0.131249\n",
            "batch 3233: loss 0.065210\n",
            "batch 3234: loss 0.092670\n",
            "batch 3235: loss 0.084848\n",
            "batch 3236: loss 0.071753\n",
            "batch 3237: loss 0.033044\n",
            "batch 3238: loss 0.187632\n",
            "batch 3239: loss 0.018182\n",
            "batch 3240: loss 0.074219\n",
            "batch 3241: loss 0.104429\n",
            "batch 3242: loss 0.054888\n",
            "batch 3243: loss 0.076191\n",
            "batch 3244: loss 0.240306\n",
            "batch 3245: loss 0.067632\n",
            "batch 3246: loss 0.073698\n",
            "batch 3247: loss 0.145465\n",
            "batch 3248: loss 0.223554\n",
            "batch 3249: loss 0.023878\n",
            "batch 3250: loss 0.025499\n",
            "batch 3251: loss 0.019549\n",
            "batch 3252: loss 0.060577\n",
            "batch 3253: loss 0.044519\n",
            "batch 3254: loss 0.019404\n",
            "batch 3255: loss 0.087957\n",
            "batch 3256: loss 0.128236\n",
            "batch 3257: loss 0.038186\n",
            "batch 3258: loss 0.018753\n",
            "batch 3259: loss 0.072257\n",
            "batch 3260: loss 0.168787\n",
            "batch 3261: loss 0.055646\n",
            "batch 3262: loss 0.033644\n",
            "batch 3263: loss 0.152070\n",
            "batch 3264: loss 0.121345\n",
            "batch 3265: loss 0.127443\n",
            "batch 3266: loss 0.181850\n",
            "batch 3267: loss 0.050780\n",
            "batch 3268: loss 0.100651\n",
            "batch 3269: loss 0.071250\n",
            "batch 3270: loss 0.080076\n",
            "batch 3271: loss 0.032160\n",
            "batch 3272: loss 0.038019\n",
            "batch 3273: loss 0.038179\n",
            "batch 3274: loss 0.055357\n",
            "batch 3275: loss 0.130528\n",
            "batch 3276: loss 0.103261\n",
            "batch 3277: loss 0.173659\n",
            "batch 3278: loss 0.083234\n",
            "batch 3279: loss 0.024164\n",
            "batch 3280: loss 0.101102\n",
            "batch 3281: loss 0.027150\n",
            "batch 3282: loss 0.200107\n",
            "batch 3283: loss 0.087682\n",
            "batch 3284: loss 0.061886\n",
            "batch 3285: loss 0.040483\n",
            "batch 3286: loss 0.055532\n",
            "batch 3287: loss 0.103372\n",
            "batch 3288: loss 0.050896\n",
            "batch 3289: loss 0.137247\n",
            "batch 3290: loss 0.024700\n",
            "batch 3291: loss 0.166678\n",
            "batch 3292: loss 0.009680\n",
            "batch 3293: loss 0.009055\n",
            "batch 3294: loss 0.201640\n",
            "batch 3295: loss 0.030528\n",
            "batch 3296: loss 0.037950\n",
            "batch 3297: loss 0.052199\n",
            "batch 3298: loss 0.025988\n",
            "batch 3299: loss 0.066782\n",
            "batch 3300: loss 0.033965\n",
            "batch 3301: loss 0.098691\n",
            "batch 3302: loss 0.100524\n",
            "batch 3303: loss 0.106784\n",
            "batch 3304: loss 0.052381\n",
            "batch 3305: loss 0.066536\n",
            "batch 3306: loss 0.085806\n",
            "batch 3307: loss 0.070367\n",
            "batch 3308: loss 0.029130\n",
            "batch 3309: loss 0.029241\n",
            "batch 3310: loss 0.147822\n",
            "batch 3311: loss 0.131999\n",
            "batch 3312: loss 0.022587\n",
            "batch 3313: loss 0.083030\n",
            "batch 3314: loss 0.131795\n",
            "batch 3315: loss 0.115168\n",
            "batch 3316: loss 0.022394\n",
            "batch 3317: loss 0.157217\n",
            "batch 3318: loss 0.161533\n",
            "batch 3319: loss 0.035846\n",
            "batch 3320: loss 0.082652\n",
            "batch 3321: loss 0.102212\n",
            "batch 3322: loss 0.070846\n",
            "batch 3323: loss 0.214655\n",
            "batch 3324: loss 0.082369\n",
            "batch 3325: loss 0.049309\n",
            "batch 3326: loss 0.044033\n",
            "batch 3327: loss 0.107648\n",
            "batch 3328: loss 0.076369\n",
            "batch 3329: loss 0.117097\n",
            "batch 3330: loss 0.136227\n",
            "batch 3331: loss 0.182570\n",
            "batch 3332: loss 0.032427\n",
            "batch 3333: loss 0.039889\n",
            "batch 3334: loss 0.129331\n",
            "batch 3335: loss 0.058154\n",
            "batch 3336: loss 0.061196\n",
            "batch 3337: loss 0.109441\n",
            "batch 3338: loss 0.129019\n",
            "batch 3339: loss 0.074862\n",
            "batch 3340: loss 0.152888\n",
            "batch 3341: loss 0.089050\n",
            "batch 3342: loss 0.219153\n",
            "batch 3343: loss 0.105856\n",
            "batch 3344: loss 0.047512\n",
            "batch 3345: loss 0.059279\n",
            "batch 3346: loss 0.097417\n",
            "batch 3347: loss 0.175216\n",
            "batch 3348: loss 0.104470\n",
            "batch 3349: loss 0.051461\n",
            "batch 3350: loss 0.060301\n",
            "batch 3351: loss 0.041748\n",
            "batch 3352: loss 0.084212\n",
            "batch 3353: loss 0.049691\n",
            "batch 3354: loss 0.042028\n",
            "batch 3355: loss 0.132775\n",
            "batch 3356: loss 0.110541\n",
            "batch 3357: loss 0.083739\n",
            "batch 3358: loss 0.010702\n",
            "batch 3359: loss 0.123883\n",
            "batch 3360: loss 0.043754\n",
            "batch 3361: loss 0.062841\n",
            "batch 3362: loss 0.085628\n",
            "batch 3363: loss 0.028342\n",
            "batch 3364: loss 0.061682\n",
            "batch 3365: loss 0.254152\n",
            "batch 3366: loss 0.106716\n",
            "batch 3367: loss 0.051584\n",
            "batch 3368: loss 0.096674\n",
            "batch 3369: loss 0.091797\n",
            "batch 3370: loss 0.108251\n",
            "batch 3371: loss 0.153631\n",
            "batch 3372: loss 0.164931\n",
            "batch 3373: loss 0.090448\n",
            "batch 3374: loss 0.011038\n",
            "batch 3375: loss 0.275416\n",
            "batch 3376: loss 0.050255\n",
            "batch 3377: loss 0.037855\n",
            "batch 3378: loss 0.168043\n",
            "batch 3379: loss 0.405101\n",
            "batch 3380: loss 0.068966\n",
            "batch 3381: loss 0.079264\n",
            "batch 3382: loss 0.115068\n",
            "batch 3383: loss 0.026432\n",
            "batch 3384: loss 0.135171\n",
            "batch 3385: loss 0.086955\n",
            "batch 3386: loss 0.023073\n",
            "batch 3387: loss 0.063791\n",
            "batch 3388: loss 0.091094\n",
            "batch 3389: loss 0.128001\n",
            "batch 3390: loss 0.186180\n",
            "batch 3391: loss 0.071137\n",
            "batch 3392: loss 0.151379\n",
            "batch 3393: loss 0.067110\n",
            "batch 3394: loss 0.034421\n",
            "batch 3395: loss 0.039088\n",
            "batch 3396: loss 0.053425\n",
            "batch 3397: loss 0.063034\n",
            "batch 3398: loss 0.014249\n",
            "batch 3399: loss 0.074124\n",
            "batch 3400: loss 0.200561\n",
            "batch 3401: loss 0.060739\n",
            "batch 3402: loss 0.082794\n",
            "batch 3403: loss 0.021105\n",
            "batch 3404: loss 0.025650\n",
            "batch 3405: loss 0.040212\n",
            "batch 3406: loss 0.056253\n",
            "batch 3407: loss 0.066426\n",
            "batch 3408: loss 0.143980\n",
            "batch 3409: loss 0.103145\n",
            "batch 3410: loss 0.077786\n",
            "batch 3411: loss 0.167586\n",
            "batch 3412: loss 0.067236\n",
            "batch 3413: loss 0.011599\n",
            "batch 3414: loss 0.035834\n",
            "batch 3415: loss 0.082820\n",
            "batch 3416: loss 0.057573\n",
            "batch 3417: loss 0.115052\n",
            "batch 3418: loss 0.053053\n",
            "batch 3419: loss 0.169303\n",
            "batch 3420: loss 0.132002\n",
            "batch 3421: loss 0.080509\n",
            "batch 3422: loss 0.014733\n",
            "batch 3423: loss 0.062746\n",
            "batch 3424: loss 0.193036\n",
            "batch 3425: loss 0.068204\n",
            "batch 3426: loss 0.024162\n",
            "batch 3427: loss 0.069778\n",
            "batch 3428: loss 0.052810\n",
            "batch 3429: loss 0.016853\n",
            "batch 3430: loss 0.305740\n",
            "batch 3431: loss 0.051590\n",
            "batch 3432: loss 0.034190\n",
            "batch 3433: loss 0.036013\n",
            "batch 3434: loss 0.138956\n",
            "batch 3435: loss 0.094054\n",
            "batch 3436: loss 0.035534\n",
            "batch 3437: loss 0.058381\n",
            "batch 3438: loss 0.078449\n",
            "batch 3439: loss 0.039230\n",
            "batch 3440: loss 0.054084\n",
            "batch 3441: loss 0.133409\n",
            "batch 3442: loss 0.081086\n",
            "batch 3443: loss 0.032288\n",
            "batch 3444: loss 0.060667\n",
            "batch 3445: loss 0.032288\n",
            "batch 3446: loss 0.039689\n",
            "batch 3447: loss 0.019689\n",
            "batch 3448: loss 0.021998\n",
            "batch 3449: loss 0.140370\n",
            "batch 3450: loss 0.052006\n",
            "batch 3451: loss 0.031525\n",
            "batch 3452: loss 0.028901\n",
            "batch 3453: loss 0.075620\n",
            "batch 3454: loss 0.041294\n",
            "batch 3455: loss 0.016182\n",
            "batch 3456: loss 0.138212\n",
            "batch 3457: loss 0.040166\n",
            "batch 3458: loss 0.097017\n",
            "batch 3459: loss 0.026845\n",
            "batch 3460: loss 0.050353\n",
            "batch 3461: loss 0.120540\n",
            "batch 3462: loss 0.022538\n",
            "batch 3463: loss 0.079672\n",
            "batch 3464: loss 0.090690\n",
            "batch 3465: loss 0.054907\n",
            "batch 3466: loss 0.059171\n",
            "batch 3467: loss 0.089580\n",
            "batch 3468: loss 0.046764\n",
            "batch 3469: loss 0.092652\n",
            "batch 3470: loss 0.100783\n",
            "batch 3471: loss 0.015202\n",
            "batch 3472: loss 0.111219\n",
            "batch 3473: loss 0.059922\n",
            "batch 3474: loss 0.022624\n",
            "batch 3475: loss 0.042636\n",
            "batch 3476: loss 0.213633\n",
            "batch 3477: loss 0.081218\n",
            "batch 3478: loss 0.021815\n",
            "batch 3479: loss 0.008662\n",
            "batch 3480: loss 0.068403\n",
            "batch 3481: loss 0.049023\n",
            "batch 3482: loss 0.288605\n",
            "batch 3483: loss 0.012290\n",
            "batch 3484: loss 0.017676\n",
            "batch 3485: loss 0.098248\n",
            "batch 3486: loss 0.096872\n",
            "batch 3487: loss 0.110101\n",
            "batch 3488: loss 0.034771\n",
            "batch 3489: loss 0.051419\n",
            "batch 3490: loss 0.064696\n",
            "batch 3491: loss 0.033321\n",
            "batch 3492: loss 0.067922\n",
            "batch 3493: loss 0.186162\n",
            "batch 3494: loss 0.098322\n",
            "batch 3495: loss 0.113661\n",
            "batch 3496: loss 0.118199\n",
            "batch 3497: loss 0.019256\n",
            "batch 3498: loss 0.020187\n",
            "batch 3499: loss 0.064206\n",
            "batch 3500: loss 0.084331\n",
            "batch 3501: loss 0.050279\n",
            "batch 3502: loss 0.263342\n",
            "batch 3503: loss 0.099527\n",
            "batch 3504: loss 0.043267\n",
            "batch 3505: loss 0.100427\n",
            "batch 3506: loss 0.024318\n",
            "batch 3507: loss 0.118598\n",
            "batch 3508: loss 0.164662\n",
            "batch 3509: loss 0.037972\n",
            "batch 3510: loss 0.229819\n",
            "batch 3511: loss 0.128316\n",
            "batch 3512: loss 0.113625\n",
            "batch 3513: loss 0.066363\n",
            "batch 3514: loss 0.011569\n",
            "batch 3515: loss 0.027135\n",
            "batch 3516: loss 0.038549\n",
            "batch 3517: loss 0.148295\n",
            "batch 3518: loss 0.057429\n",
            "batch 3519: loss 0.046094\n",
            "batch 3520: loss 0.127447\n",
            "batch 3521: loss 0.024844\n",
            "batch 3522: loss 0.403695\n",
            "batch 3523: loss 0.018710\n",
            "batch 3524: loss 0.010219\n",
            "batch 3525: loss 0.036025\n",
            "batch 3526: loss 0.038514\n",
            "batch 3527: loss 0.056777\n",
            "batch 3528: loss 0.024506\n",
            "batch 3529: loss 0.109590\n",
            "batch 3530: loss 0.029536\n",
            "batch 3531: loss 0.039438\n",
            "batch 3532: loss 0.118462\n",
            "batch 3533: loss 0.045262\n",
            "batch 3534: loss 0.054310\n",
            "batch 3535: loss 0.168510\n",
            "batch 3536: loss 0.072989\n",
            "batch 3537: loss 0.110551\n",
            "batch 3538: loss 0.101485\n",
            "batch 3539: loss 0.041066\n",
            "batch 3540: loss 0.018908\n",
            "batch 3541: loss 0.109150\n",
            "batch 3542: loss 0.091491\n",
            "batch 3543: loss 0.018633\n",
            "batch 3544: loss 0.073857\n",
            "batch 3545: loss 0.052347\n",
            "batch 3546: loss 0.122333\n",
            "batch 3547: loss 0.120602\n",
            "batch 3548: loss 0.025231\n",
            "batch 3549: loss 0.027332\n",
            "batch 3550: loss 0.050973\n",
            "batch 3551: loss 0.039640\n",
            "batch 3552: loss 0.101971\n",
            "batch 3553: loss 0.071403\n",
            "batch 3554: loss 0.119250\n",
            "batch 3555: loss 0.043070\n",
            "batch 3556: loss 0.072017\n",
            "batch 3557: loss 0.055277\n",
            "batch 3558: loss 0.048254\n",
            "batch 3559: loss 0.124145\n",
            "batch 3560: loss 0.122278\n",
            "batch 3561: loss 0.121957\n",
            "batch 3562: loss 0.077201\n",
            "batch 3563: loss 0.119332\n",
            "batch 3564: loss 0.089604\n",
            "batch 3565: loss 0.116145\n",
            "batch 3566: loss 0.043656\n",
            "batch 3567: loss 0.093904\n",
            "batch 3568: loss 0.037226\n",
            "batch 3569: loss 0.030778\n",
            "batch 3570: loss 0.012602\n",
            "batch 3571: loss 0.066075\n",
            "batch 3572: loss 0.144924\n",
            "batch 3573: loss 0.117494\n",
            "batch 3574: loss 0.206581\n",
            "batch 3575: loss 0.230868\n",
            "batch 3576: loss 0.072909\n",
            "batch 3577: loss 0.121680\n",
            "batch 3578: loss 0.032556\n",
            "batch 3579: loss 0.182968\n",
            "batch 3580: loss 0.055800\n",
            "batch 3581: loss 0.013192\n",
            "batch 3582: loss 0.085616\n",
            "batch 3583: loss 0.038008\n",
            "batch 3584: loss 0.029611\n",
            "batch 3585: loss 0.037245\n",
            "batch 3586: loss 0.089096\n",
            "batch 3587: loss 0.038208\n",
            "batch 3588: loss 0.039684\n",
            "batch 3589: loss 0.088045\n",
            "batch 3590: loss 0.064904\n",
            "batch 3591: loss 0.106122\n",
            "batch 3592: loss 0.020292\n",
            "batch 3593: loss 0.052262\n",
            "batch 3594: loss 0.051634\n",
            "batch 3595: loss 0.146123\n",
            "batch 3596: loss 0.143358\n",
            "batch 3597: loss 0.119738\n",
            "batch 3598: loss 0.014724\n",
            "batch 3599: loss 0.034303\n",
            "batch 3600: loss 0.043150\n",
            "batch 3601: loss 0.017368\n",
            "batch 3602: loss 0.008557\n",
            "batch 3603: loss 0.099840\n",
            "batch 3604: loss 0.034952\n",
            "batch 3605: loss 0.032421\n",
            "batch 3606: loss 0.026848\n",
            "batch 3607: loss 0.101468\n",
            "batch 3608: loss 0.054676\n",
            "batch 3609: loss 0.093998\n",
            "batch 3610: loss 0.076601\n",
            "batch 3611: loss 0.050279\n",
            "batch 3612: loss 0.159925\n",
            "batch 3613: loss 0.139859\n",
            "batch 3614: loss 0.145608\n",
            "batch 3615: loss 0.120293\n",
            "batch 3616: loss 0.049449\n",
            "batch 3617: loss 0.046501\n",
            "batch 3618: loss 0.015187\n",
            "batch 3619: loss 0.038254\n",
            "batch 3620: loss 0.084367\n",
            "batch 3621: loss 0.076176\n",
            "batch 3622: loss 0.072363\n",
            "batch 3623: loss 0.033654\n",
            "batch 3624: loss 0.022856\n",
            "batch 3625: loss 0.083379\n",
            "batch 3626: loss 0.060325\n",
            "batch 3627: loss 0.122700\n",
            "batch 3628: loss 0.069383\n",
            "batch 3629: loss 0.040144\n",
            "batch 3630: loss 0.038442\n",
            "batch 3631: loss 0.079753\n",
            "batch 3632: loss 0.062751\n",
            "batch 3633: loss 0.092539\n",
            "batch 3634: loss 0.047713\n",
            "batch 3635: loss 0.058006\n",
            "batch 3636: loss 0.089882\n",
            "batch 3637: loss 0.102742\n",
            "batch 3638: loss 0.023539\n",
            "batch 3639: loss 0.091547\n",
            "batch 3640: loss 0.031781\n",
            "batch 3641: loss 0.086684\n",
            "batch 3642: loss 0.141136\n",
            "batch 3643: loss 0.031117\n",
            "batch 3644: loss 0.021079\n",
            "batch 3645: loss 0.147612\n",
            "batch 3646: loss 0.028920\n",
            "batch 3647: loss 0.149186\n",
            "batch 3648: loss 0.041168\n",
            "batch 3649: loss 0.025789\n",
            "batch 3650: loss 0.020431\n",
            "batch 3651: loss 0.086003\n",
            "batch 3652: loss 0.029682\n",
            "batch 3653: loss 0.116383\n",
            "batch 3654: loss 0.098892\n",
            "batch 3655: loss 0.047951\n",
            "batch 3656: loss 0.049399\n",
            "batch 3657: loss 0.041334\n",
            "batch 3658: loss 0.118460\n",
            "batch 3659: loss 0.066329\n",
            "batch 3660: loss 0.034070\n",
            "batch 3661: loss 0.021745\n",
            "batch 3662: loss 0.109574\n",
            "batch 3663: loss 0.127328\n",
            "batch 3664: loss 0.014287\n",
            "batch 3665: loss 0.016269\n",
            "batch 3666: loss 0.055688\n",
            "batch 3667: loss 0.222326\n",
            "batch 3668: loss 0.086696\n",
            "batch 3669: loss 0.050925\n",
            "batch 3670: loss 0.072888\n",
            "batch 3671: loss 0.113141\n",
            "batch 3672: loss 0.063578\n",
            "batch 3673: loss 0.191813\n",
            "batch 3674: loss 0.025715\n",
            "batch 3675: loss 0.267622\n",
            "batch 3676: loss 0.043999\n",
            "batch 3677: loss 0.104964\n",
            "batch 3678: loss 0.016931\n",
            "batch 3679: loss 0.205929\n",
            "batch 3680: loss 0.016656\n",
            "batch 3681: loss 0.013073\n",
            "batch 3682: loss 0.049818\n",
            "batch 3683: loss 0.048740\n",
            "batch 3684: loss 0.073925\n",
            "batch 3685: loss 0.079679\n",
            "batch 3686: loss 0.090922\n",
            "batch 3687: loss 0.049674\n",
            "batch 3688: loss 0.051921\n",
            "batch 3689: loss 0.109655\n",
            "batch 3690: loss 0.156187\n",
            "batch 3691: loss 0.132412\n",
            "batch 3692: loss 0.168875\n",
            "batch 3693: loss 0.146688\n",
            "batch 3694: loss 0.126388\n",
            "batch 3695: loss 0.022035\n",
            "batch 3696: loss 0.084850\n",
            "batch 3697: loss 0.098265\n",
            "batch 3698: loss 0.024002\n",
            "batch 3699: loss 0.144796\n",
            "batch 3700: loss 0.069037\n",
            "batch 3701: loss 0.054975\n",
            "batch 3702: loss 0.107839\n",
            "batch 3703: loss 0.015737\n",
            "batch 3704: loss 0.117147\n",
            "batch 3705: loss 0.022723\n",
            "batch 3706: loss 0.012948\n",
            "batch 3707: loss 0.129209\n",
            "batch 3708: loss 0.102008\n",
            "batch 3709: loss 0.020559\n",
            "batch 3710: loss 0.039757\n",
            "batch 3711: loss 0.206092\n",
            "batch 3712: loss 0.205750\n",
            "batch 3713: loss 0.139876\n",
            "batch 3714: loss 0.116499\n",
            "batch 3715: loss 0.016386\n",
            "batch 3716: loss 0.128528\n",
            "batch 3717: loss 0.016614\n",
            "batch 3718: loss 0.076288\n",
            "batch 3719: loss 0.040854\n",
            "batch 3720: loss 0.082747\n",
            "batch 3721: loss 0.148808\n",
            "batch 3722: loss 0.089987\n",
            "batch 3723: loss 0.051373\n",
            "batch 3724: loss 0.067793\n",
            "batch 3725: loss 0.197504\n",
            "batch 3726: loss 0.090993\n",
            "batch 3727: loss 0.035492\n",
            "batch 3728: loss 0.032982\n",
            "batch 3729: loss 0.015715\n",
            "batch 3730: loss 0.037406\n",
            "batch 3731: loss 0.023772\n",
            "batch 3732: loss 0.063879\n",
            "batch 3733: loss 0.059779\n",
            "batch 3734: loss 0.122880\n",
            "batch 3735: loss 0.157448\n",
            "batch 3736: loss 0.026889\n",
            "batch 3737: loss 0.016859\n",
            "batch 3738: loss 0.121502\n",
            "batch 3739: loss 0.074032\n",
            "batch 3740: loss 0.066184\n",
            "batch 3741: loss 0.013504\n",
            "batch 3742: loss 0.009819\n",
            "batch 3743: loss 0.016746\n",
            "batch 3744: loss 0.143223\n",
            "batch 3745: loss 0.056668\n",
            "batch 3746: loss 0.019743\n",
            "batch 3747: loss 0.042239\n",
            "batch 3748: loss 0.091280\n",
            "batch 3749: loss 0.109364\n",
            "batch 3750: loss 0.049248\n",
            "batch 3751: loss 0.044953\n",
            "batch 3752: loss 0.076182\n",
            "batch 3753: loss 0.068137\n",
            "batch 3754: loss 0.135743\n",
            "batch 3755: loss 0.042432\n",
            "batch 3756: loss 0.039090\n",
            "batch 3757: loss 0.052692\n",
            "batch 3758: loss 0.046589\n",
            "batch 3759: loss 0.006734\n",
            "batch 3760: loss 0.034851\n",
            "batch 3761: loss 0.076661\n",
            "batch 3762: loss 0.015204\n",
            "batch 3763: loss 0.089727\n",
            "batch 3764: loss 0.065637\n",
            "batch 3765: loss 0.066718\n",
            "batch 3766: loss 0.023947\n",
            "batch 3767: loss 0.050297\n",
            "batch 3768: loss 0.006873\n",
            "batch 3769: loss 0.010936\n",
            "batch 3770: loss 0.077665\n",
            "batch 3771: loss 0.054276\n",
            "batch 3772: loss 0.049556\n",
            "batch 3773: loss 0.052674\n",
            "batch 3774: loss 0.050838\n",
            "batch 3775: loss 0.196864\n",
            "batch 3776: loss 0.022038\n",
            "batch 3777: loss 0.080378\n",
            "batch 3778: loss 0.066054\n",
            "batch 3779: loss 0.013715\n",
            "batch 3780: loss 0.024352\n",
            "batch 3781: loss 0.126372\n",
            "batch 3782: loss 0.134490\n",
            "batch 3783: loss 0.027669\n",
            "batch 3784: loss 0.076546\n",
            "batch 3785: loss 0.127929\n",
            "batch 3786: loss 0.138093\n",
            "batch 3787: loss 0.059735\n",
            "batch 3788: loss 0.033254\n",
            "batch 3789: loss 0.056459\n",
            "batch 3790: loss 0.043149\n",
            "batch 3791: loss 0.020202\n",
            "batch 3792: loss 0.008342\n",
            "batch 3793: loss 0.109725\n",
            "batch 3794: loss 0.176968\n",
            "batch 3795: loss 0.150584\n",
            "batch 3796: loss 0.077225\n",
            "batch 3797: loss 0.109498\n",
            "batch 3798: loss 0.270257\n",
            "batch 3799: loss 0.170779\n",
            "batch 3800: loss 0.070686\n",
            "batch 3801: loss 0.061271\n",
            "batch 3802: loss 0.057508\n",
            "batch 3803: loss 0.063837\n",
            "batch 3804: loss 0.032965\n",
            "batch 3805: loss 0.052576\n",
            "batch 3806: loss 0.037213\n",
            "batch 3807: loss 0.010534\n",
            "batch 3808: loss 0.085979\n",
            "batch 3809: loss 0.064526\n",
            "batch 3810: loss 0.028287\n",
            "batch 3811: loss 0.085494\n",
            "batch 3812: loss 0.044454\n",
            "batch 3813: loss 0.023129\n",
            "batch 3814: loss 0.136451\n",
            "batch 3815: loss 0.107743\n",
            "batch 3816: loss 0.077133\n",
            "batch 3817: loss 0.032393\n",
            "batch 3818: loss 0.038133\n",
            "batch 3819: loss 0.020673\n",
            "batch 3820: loss 0.010620\n",
            "batch 3821: loss 0.056013\n",
            "batch 3822: loss 0.016295\n",
            "batch 3823: loss 0.153697\n",
            "batch 3824: loss 0.012505\n",
            "batch 3825: loss 0.035714\n",
            "batch 3826: loss 0.071487\n",
            "batch 3827: loss 0.070502\n",
            "batch 3828: loss 0.213242\n",
            "batch 3829: loss 0.068976\n",
            "batch 3830: loss 0.039312\n",
            "batch 3831: loss 0.083107\n",
            "batch 3832: loss 0.021447\n",
            "batch 3833: loss 0.041187\n",
            "batch 3834: loss 0.014532\n",
            "batch 3835: loss 0.152171\n",
            "batch 3836: loss 0.120672\n",
            "batch 3837: loss 0.035296\n",
            "batch 3838: loss 0.034417\n",
            "batch 3839: loss 0.013258\n",
            "batch 3840: loss 0.107931\n",
            "batch 3841: loss 0.049762\n",
            "batch 3842: loss 0.011612\n",
            "batch 3843: loss 0.036263\n",
            "batch 3844: loss 0.164072\n",
            "batch 3845: loss 0.008344\n",
            "batch 3846: loss 0.050141\n",
            "batch 3847: loss 0.084694\n",
            "batch 3848: loss 0.046495\n",
            "batch 3849: loss 0.086496\n",
            "batch 3850: loss 0.015605\n",
            "batch 3851: loss 0.015195\n",
            "batch 3852: loss 0.064944\n",
            "batch 3853: loss 0.081994\n",
            "batch 3854: loss 0.063567\n",
            "batch 3855: loss 0.038825\n",
            "batch 3856: loss 0.160244\n",
            "batch 3857: loss 0.038105\n",
            "batch 3858: loss 0.056702\n",
            "batch 3859: loss 0.068096\n",
            "batch 3860: loss 0.091747\n",
            "batch 3861: loss 0.019974\n",
            "batch 3862: loss 0.113153\n",
            "batch 3863: loss 0.180000\n",
            "batch 3864: loss 0.026970\n",
            "batch 3865: loss 0.026284\n",
            "batch 3866: loss 0.097920\n",
            "batch 3867: loss 0.095454\n",
            "batch 3868: loss 0.010480\n",
            "batch 3869: loss 0.286708\n",
            "batch 3870: loss 0.090090\n",
            "batch 3871: loss 0.011888\n",
            "batch 3872: loss 0.093200\n",
            "batch 3873: loss 0.016735\n",
            "batch 3874: loss 0.020621\n",
            "batch 3875: loss 0.117724\n",
            "batch 3876: loss 0.152090\n",
            "batch 3877: loss 0.054957\n",
            "batch 3878: loss 0.007448\n",
            "batch 3879: loss 0.041503\n",
            "batch 3880: loss 0.018082\n",
            "batch 3881: loss 0.025925\n",
            "batch 3882: loss 0.086444\n",
            "batch 3883: loss 0.012480\n",
            "batch 3884: loss 0.077028\n",
            "batch 3885: loss 0.113379\n",
            "batch 3886: loss 0.091556\n",
            "batch 3887: loss 0.043000\n",
            "batch 3888: loss 0.046760\n",
            "batch 3889: loss 0.110295\n",
            "batch 3890: loss 0.104285\n",
            "batch 3891: loss 0.083973\n",
            "batch 3892: loss 0.163770\n",
            "batch 3893: loss 0.167513\n",
            "batch 3894: loss 0.028610\n",
            "batch 3895: loss 0.133025\n",
            "batch 3896: loss 0.051424\n",
            "batch 3897: loss 0.015177\n",
            "batch 3898: loss 0.075464\n",
            "batch 3899: loss 0.036406\n",
            "batch 3900: loss 0.036567\n",
            "batch 3901: loss 0.080560\n",
            "batch 3902: loss 0.110085\n",
            "batch 3903: loss 0.099222\n",
            "batch 3904: loss 0.017470\n",
            "batch 3905: loss 0.024255\n",
            "batch 3906: loss 0.103378\n",
            "batch 3907: loss 0.056489\n",
            "batch 3908: loss 0.028129\n",
            "batch 3909: loss 0.047686\n",
            "batch 3910: loss 0.176808\n",
            "batch 3911: loss 0.061520\n",
            "batch 3912: loss 0.047467\n",
            "batch 3913: loss 0.073079\n",
            "batch 3914: loss 0.024018\n",
            "batch 3915: loss 0.072002\n",
            "batch 3916: loss 0.048681\n",
            "batch 3917: loss 0.051619\n",
            "batch 3918: loss 0.037940\n",
            "batch 3919: loss 0.198726\n",
            "batch 3920: loss 0.036598\n",
            "batch 3921: loss 0.042179\n",
            "batch 3922: loss 0.070329\n",
            "batch 3923: loss 0.025757\n",
            "batch 3924: loss 0.016431\n",
            "batch 3925: loss 0.036921\n",
            "batch 3926: loss 0.044287\n",
            "batch 3927: loss 0.244463\n",
            "batch 3928: loss 0.060742\n",
            "batch 3929: loss 0.046612\n",
            "batch 3930: loss 0.034804\n",
            "batch 3931: loss 0.139822\n",
            "batch 3932: loss 0.231675\n",
            "batch 3933: loss 0.092820\n",
            "batch 3934: loss 0.059298\n",
            "batch 3935: loss 0.025476\n",
            "batch 3936: loss 0.065156\n",
            "batch 3937: loss 0.059496\n",
            "batch 3938: loss 0.187209\n",
            "batch 3939: loss 0.194090\n",
            "batch 3940: loss 0.022454\n",
            "batch 3941: loss 0.071857\n",
            "batch 3942: loss 0.040552\n",
            "batch 3943: loss 0.011231\n",
            "batch 3944: loss 0.033333\n",
            "batch 3945: loss 0.038093\n",
            "batch 3946: loss 0.042565\n",
            "batch 3947: loss 0.065615\n",
            "batch 3948: loss 0.021315\n",
            "batch 3949: loss 0.018422\n",
            "batch 3950: loss 0.056256\n",
            "batch 3951: loss 0.090903\n",
            "batch 3952: loss 0.031412\n",
            "batch 3953: loss 0.091571\n",
            "batch 3954: loss 0.034720\n",
            "batch 3955: loss 0.044412\n",
            "batch 3956: loss 0.122721\n",
            "batch 3957: loss 0.078972\n",
            "batch 3958: loss 0.009678\n",
            "batch 3959: loss 0.088790\n",
            "batch 3960: loss 0.071821\n",
            "batch 3961: loss 0.022722\n",
            "batch 3962: loss 0.172909\n",
            "batch 3963: loss 0.085969\n",
            "batch 3964: loss 0.035942\n",
            "batch 3965: loss 0.127694\n",
            "batch 3966: loss 0.061609\n",
            "batch 3967: loss 0.052288\n",
            "batch 3968: loss 0.028669\n",
            "batch 3969: loss 0.052766\n",
            "batch 3970: loss 0.056812\n",
            "batch 3971: loss 0.054207\n",
            "batch 3972: loss 0.035299\n",
            "batch 3973: loss 0.105532\n",
            "batch 3974: loss 0.021086\n",
            "batch 3975: loss 0.027073\n",
            "batch 3976: loss 0.110263\n",
            "batch 3977: loss 0.176940\n",
            "batch 3978: loss 0.036922\n",
            "batch 3979: loss 0.095613\n",
            "batch 3980: loss 0.084374\n",
            "batch 3981: loss 0.184690\n",
            "batch 3982: loss 0.036925\n",
            "batch 3983: loss 0.123985\n",
            "batch 3984: loss 0.184924\n",
            "batch 3985: loss 0.084915\n",
            "batch 3986: loss 0.036809\n",
            "batch 3987: loss 0.124257\n",
            "batch 3988: loss 0.024802\n",
            "batch 3989: loss 0.049218\n",
            "batch 3990: loss 0.066228\n",
            "batch 3991: loss 0.092066\n",
            "batch 3992: loss 0.072532\n",
            "batch 3993: loss 0.037678\n",
            "batch 3994: loss 0.045747\n",
            "batch 3995: loss 0.027952\n",
            "batch 3996: loss 0.062791\n",
            "batch 3997: loss 0.059014\n",
            "batch 3998: loss 0.107462\n",
            "batch 3999: loss 0.035578\n",
            "batch 4000: loss 0.107232\n",
            "batch 4001: loss 0.116026\n",
            "batch 4002: loss 0.047100\n",
            "batch 4003: loss 0.033166\n",
            "batch 4004: loss 0.106690\n",
            "batch 4005: loss 0.022417\n",
            "batch 4006: loss 0.033479\n",
            "batch 4007: loss 0.043800\n",
            "batch 4008: loss 0.084436\n",
            "batch 4009: loss 0.035046\n",
            "batch 4010: loss 0.033509\n",
            "batch 4011: loss 0.143149\n",
            "batch 4012: loss 0.080790\n",
            "batch 4013: loss 0.060055\n",
            "batch 4014: loss 0.047074\n",
            "batch 4015: loss 0.023823\n",
            "batch 4016: loss 0.031398\n",
            "batch 4017: loss 0.066459\n",
            "batch 4018: loss 0.256165\n",
            "batch 4019: loss 0.056469\n",
            "batch 4020: loss 0.024879\n",
            "batch 4021: loss 0.081161\n",
            "batch 4022: loss 0.011657\n",
            "batch 4023: loss 0.093613\n",
            "batch 4024: loss 0.033275\n",
            "batch 4025: loss 0.031446\n",
            "batch 4026: loss 0.189262\n",
            "batch 4027: loss 0.067021\n",
            "batch 4028: loss 0.091907\n",
            "batch 4029: loss 0.033082\n",
            "batch 4030: loss 0.015818\n",
            "batch 4031: loss 0.041486\n",
            "batch 4032: loss 0.138606\n",
            "batch 4033: loss 0.046473\n",
            "batch 4034: loss 0.192326\n",
            "batch 4035: loss 0.070837\n",
            "batch 4036: loss 0.072887\n",
            "batch 4037: loss 0.018612\n",
            "batch 4038: loss 0.034506\n",
            "batch 4039: loss 0.113300\n",
            "batch 4040: loss 0.026560\n",
            "batch 4041: loss 0.140579\n",
            "batch 4042: loss 0.085831\n",
            "batch 4043: loss 0.088274\n",
            "batch 4044: loss 0.042918\n",
            "batch 4045: loss 0.121165\n",
            "batch 4046: loss 0.230838\n",
            "batch 4047: loss 0.084947\n",
            "batch 4048: loss 0.050948\n",
            "batch 4049: loss 0.057720\n",
            "batch 4050: loss 0.015107\n",
            "batch 4051: loss 0.017049\n",
            "batch 4052: loss 0.039622\n",
            "batch 4053: loss 0.053588\n",
            "batch 4054: loss 0.035118\n",
            "batch 4055: loss 0.235356\n",
            "batch 4056: loss 0.045991\n",
            "batch 4057: loss 0.062692\n",
            "batch 4058: loss 0.066513\n",
            "batch 4059: loss 0.099082\n",
            "batch 4060: loss 0.008963\n",
            "batch 4061: loss 0.004825\n",
            "batch 4062: loss 0.030018\n",
            "batch 4063: loss 0.101433\n",
            "batch 4064: loss 0.021344\n",
            "batch 4065: loss 0.054938\n",
            "batch 4066: loss 0.126136\n",
            "batch 4067: loss 0.175765\n",
            "batch 4068: loss 0.020742\n",
            "batch 4069: loss 0.011942\n",
            "batch 4070: loss 0.010211\n",
            "batch 4071: loss 0.099729\n",
            "batch 4072: loss 0.063712\n",
            "batch 4073: loss 0.143394\n",
            "batch 4074: loss 0.017216\n",
            "batch 4075: loss 0.065352\n",
            "batch 4076: loss 0.041749\n",
            "batch 4077: loss 0.061338\n",
            "batch 4078: loss 0.071278\n",
            "batch 4079: loss 0.040061\n",
            "batch 4080: loss 0.072265\n",
            "batch 4081: loss 0.140191\n",
            "batch 4082: loss 0.083773\n",
            "batch 4083: loss 0.145537\n",
            "batch 4084: loss 0.125237\n",
            "batch 4085: loss 0.062860\n",
            "batch 4086: loss 0.045976\n",
            "batch 4087: loss 0.072375\n",
            "batch 4088: loss 0.043844\n",
            "batch 4089: loss 0.031785\n",
            "batch 4090: loss 0.081290\n",
            "batch 4091: loss 0.033592\n",
            "batch 4092: loss 0.265349\n",
            "batch 4093: loss 0.026908\n",
            "batch 4094: loss 0.020957\n",
            "batch 4095: loss 0.025135\n",
            "batch 4096: loss 0.157831\n",
            "batch 4097: loss 0.015069\n",
            "batch 4098: loss 0.034700\n",
            "batch 4099: loss 0.087556\n",
            "batch 4100: loss 0.078732\n",
            "batch 4101: loss 0.020811\n",
            "batch 4102: loss 0.017269\n",
            "batch 4103: loss 0.028289\n",
            "batch 4104: loss 0.059889\n",
            "batch 4105: loss 0.015422\n",
            "batch 4106: loss 0.011838\n",
            "batch 4107: loss 0.015070\n",
            "batch 4108: loss 0.087902\n",
            "batch 4109: loss 0.108665\n",
            "batch 4110: loss 0.080069\n",
            "batch 4111: loss 0.077847\n",
            "batch 4112: loss 0.048460\n",
            "batch 4113: loss 0.186636\n",
            "batch 4114: loss 0.033421\n",
            "batch 4115: loss 0.055947\n",
            "batch 4116: loss 0.034591\n",
            "batch 4117: loss 0.037201\n",
            "batch 4118: loss 0.084961\n",
            "batch 4119: loss 0.017543\n",
            "batch 4120: loss 0.045060\n",
            "batch 4121: loss 0.011965\n",
            "batch 4122: loss 0.012854\n",
            "batch 4123: loss 0.135032\n",
            "batch 4124: loss 0.063088\n",
            "batch 4125: loss 0.023116\n",
            "batch 4126: loss 0.198408\n",
            "batch 4127: loss 0.085419\n",
            "batch 4128: loss 0.031743\n",
            "batch 4129: loss 0.054741\n",
            "batch 4130: loss 0.067923\n",
            "batch 4131: loss 0.079382\n",
            "batch 4132: loss 0.042012\n",
            "batch 4133: loss 0.037460\n",
            "batch 4134: loss 0.095724\n",
            "batch 4135: loss 0.030936\n",
            "batch 4136: loss 0.011771\n",
            "batch 4137: loss 0.022714\n",
            "batch 4138: loss 0.020871\n",
            "batch 4139: loss 0.065974\n",
            "batch 4140: loss 0.049195\n",
            "batch 4141: loss 0.322520\n",
            "batch 4142: loss 0.012520\n",
            "batch 4143: loss 0.061393\n",
            "batch 4144: loss 0.069327\n",
            "batch 4145: loss 0.149372\n",
            "batch 4146: loss 0.040032\n",
            "batch 4147: loss 0.261529\n",
            "batch 4148: loss 0.073269\n",
            "batch 4149: loss 0.040905\n",
            "batch 4150: loss 0.052919\n",
            "batch 4151: loss 0.025026\n",
            "batch 4152: loss 0.131951\n",
            "batch 4153: loss 0.042865\n",
            "batch 4154: loss 0.027416\n",
            "batch 4155: loss 0.036281\n",
            "batch 4156: loss 0.064661\n",
            "batch 4157: loss 0.047465\n",
            "batch 4158: loss 0.082695\n",
            "batch 4159: loss 0.037278\n",
            "batch 4160: loss 0.060981\n",
            "batch 4161: loss 0.063733\n",
            "batch 4162: loss 0.109879\n",
            "batch 4163: loss 0.019934\n",
            "batch 4164: loss 0.211133\n",
            "batch 4165: loss 0.055382\n",
            "batch 4166: loss 0.184242\n",
            "batch 4167: loss 0.052754\n",
            "batch 4168: loss 0.255780\n",
            "batch 4169: loss 0.052329\n",
            "batch 4170: loss 0.036492\n",
            "batch 4171: loss 0.139433\n",
            "batch 4172: loss 0.054568\n",
            "batch 4173: loss 0.137065\n",
            "batch 4174: loss 0.005801\n",
            "batch 4175: loss 0.085646\n",
            "batch 4176: loss 0.057923\n",
            "batch 4177: loss 0.194539\n",
            "batch 4178: loss 0.083553\n",
            "batch 4179: loss 0.090365\n",
            "batch 4180: loss 0.096302\n",
            "batch 4181: loss 0.087123\n",
            "batch 4182: loss 0.029644\n",
            "batch 4183: loss 0.149782\n",
            "batch 4184: loss 0.032913\n",
            "batch 4185: loss 0.112686\n",
            "batch 4186: loss 0.049282\n",
            "batch 4187: loss 0.024842\n",
            "batch 4188: loss 0.071587\n",
            "batch 4189: loss 0.058494\n",
            "batch 4190: loss 0.088606\n",
            "batch 4191: loss 0.019430\n",
            "batch 4192: loss 0.077354\n",
            "batch 4193: loss 0.047301\n",
            "batch 4194: loss 0.043589\n",
            "batch 4195: loss 0.041727\n",
            "batch 4196: loss 0.076448\n",
            "batch 4197: loss 0.089862\n",
            "batch 4198: loss 0.042669\n",
            "batch 4199: loss 0.017055\n",
            "batch 4200: loss 0.015644\n",
            "batch 4201: loss 0.030285\n",
            "batch 4202: loss 0.030671\n",
            "batch 4203: loss 0.134659\n",
            "batch 4204: loss 0.053693\n",
            "batch 4205: loss 0.055497\n",
            "batch 4206: loss 0.053547\n",
            "batch 4207: loss 0.200881\n",
            "batch 4208: loss 0.033524\n",
            "batch 4209: loss 0.016517\n",
            "batch 4210: loss 0.081831\n",
            "batch 4211: loss 0.058766\n",
            "batch 4212: loss 0.101325\n",
            "batch 4213: loss 0.017128\n",
            "batch 4214: loss 0.061544\n",
            "batch 4215: loss 0.020028\n",
            "batch 4216: loss 0.050705\n",
            "batch 4217: loss 0.010306\n",
            "batch 4218: loss 0.027145\n",
            "batch 4219: loss 0.057093\n",
            "batch 4220: loss 0.108026\n",
            "batch 4221: loss 0.041670\n",
            "batch 4222: loss 0.061815\n",
            "batch 4223: loss 0.046968\n",
            "batch 4224: loss 0.234539\n",
            "batch 4225: loss 0.046995\n",
            "batch 4226: loss 0.033187\n",
            "batch 4227: loss 0.033102\n",
            "batch 4228: loss 0.041703\n",
            "batch 4229: loss 0.034413\n",
            "batch 4230: loss 0.032714\n",
            "batch 4231: loss 0.084704\n",
            "batch 4232: loss 0.041056\n",
            "batch 4233: loss 0.024987\n",
            "batch 4234: loss 0.133050\n",
            "batch 4235: loss 0.025759\n",
            "batch 4236: loss 0.042257\n",
            "batch 4237: loss 0.154838\n",
            "batch 4238: loss 0.021468\n",
            "batch 4239: loss 0.056365\n",
            "batch 4240: loss 0.078128\n",
            "batch 4241: loss 0.131859\n",
            "batch 4242: loss 0.020576\n",
            "batch 4243: loss 0.051875\n",
            "batch 4244: loss 0.028116\n",
            "batch 4245: loss 0.062089\n",
            "batch 4246: loss 0.104551\n",
            "batch 4247: loss 0.127460\n",
            "batch 4248: loss 0.040506\n",
            "batch 4249: loss 0.088225\n",
            "batch 4250: loss 0.036767\n",
            "batch 4251: loss 0.281460\n",
            "batch 4252: loss 0.074528\n",
            "batch 4253: loss 0.017540\n",
            "batch 4254: loss 0.011848\n",
            "batch 4255: loss 0.043258\n",
            "batch 4256: loss 0.080850\n",
            "batch 4257: loss 0.063238\n",
            "batch 4258: loss 0.072228\n",
            "batch 4259: loss 0.102196\n",
            "batch 4260: loss 0.014775\n",
            "batch 4261: loss 0.033216\n",
            "batch 4262: loss 0.072546\n",
            "batch 4263: loss 0.044862\n",
            "batch 4264: loss 0.097694\n",
            "batch 4265: loss 0.052739\n",
            "batch 4266: loss 0.059563\n",
            "batch 4267: loss 0.006887\n",
            "batch 4268: loss 0.066250\n",
            "batch 4269: loss 0.052238\n",
            "batch 4270: loss 0.064366\n",
            "batch 4271: loss 0.026213\n",
            "batch 4272: loss 0.168496\n",
            "batch 4273: loss 0.030240\n",
            "batch 4274: loss 0.052470\n",
            "batch 4275: loss 0.029533\n",
            "batch 4276: loss 0.179021\n",
            "batch 4277: loss 0.106830\n",
            "batch 4278: loss 0.092410\n",
            "batch 4279: loss 0.017013\n",
            "batch 4280: loss 0.077686\n",
            "batch 4281: loss 0.090449\n",
            "batch 4282: loss 0.138476\n",
            "batch 4283: loss 0.041114\n",
            "batch 4284: loss 0.027382\n",
            "batch 4285: loss 0.050809\n",
            "batch 4286: loss 0.025891\n",
            "batch 4287: loss 0.138765\n",
            "batch 4288: loss 0.037924\n",
            "batch 4289: loss 0.020893\n",
            "batch 4290: loss 0.029773\n",
            "batch 4291: loss 0.150135\n",
            "batch 4292: loss 0.014606\n",
            "batch 4293: loss 0.035551\n",
            "batch 4294: loss 0.074194\n",
            "batch 4295: loss 0.025533\n",
            "batch 4296: loss 0.043029\n",
            "batch 4297: loss 0.090718\n",
            "batch 4298: loss 0.015389\n",
            "batch 4299: loss 0.046400\n",
            "batch 4300: loss 0.016584\n",
            "batch 4301: loss 0.055091\n",
            "batch 4302: loss 0.024157\n",
            "batch 4303: loss 0.037868\n",
            "batch 4304: loss 0.145051\n",
            "batch 4305: loss 0.056723\n",
            "batch 4306: loss 0.016583\n",
            "batch 4307: loss 0.029302\n",
            "batch 4308: loss 0.017656\n",
            "batch 4309: loss 0.067814\n",
            "batch 4310: loss 0.013246\n",
            "batch 4311: loss 0.386761\n",
            "batch 4312: loss 0.054399\n",
            "batch 4313: loss 0.028803\n",
            "batch 4314: loss 0.018034\n",
            "batch 4315: loss 0.021078\n",
            "batch 4316: loss 0.100831\n",
            "batch 4317: loss 0.154309\n",
            "batch 4318: loss 0.018795\n",
            "batch 4319: loss 0.053414\n",
            "batch 4320: loss 0.043844\n",
            "batch 4321: loss 0.040727\n",
            "batch 4322: loss 0.101451\n",
            "batch 4323: loss 0.048974\n",
            "batch 4324: loss 0.038520\n",
            "batch 4325: loss 0.044662\n",
            "batch 4326: loss 0.019632\n",
            "batch 4327: loss 0.143200\n",
            "batch 4328: loss 0.058081\n",
            "batch 4329: loss 0.022037\n",
            "batch 4330: loss 0.026939\n",
            "batch 4331: loss 0.110131\n",
            "batch 4332: loss 0.126110\n",
            "batch 4333: loss 0.044169\n",
            "batch 4334: loss 0.118359\n",
            "batch 4335: loss 0.157838\n",
            "batch 4336: loss 0.044724\n",
            "batch 4337: loss 0.031660\n",
            "batch 4338: loss 0.035127\n",
            "batch 4339: loss 0.010377\n",
            "batch 4340: loss 0.076980\n",
            "batch 4341: loss 0.014203\n",
            "batch 4342: loss 0.055467\n",
            "batch 4343: loss 0.014023\n",
            "batch 4344: loss 0.084572\n",
            "batch 4345: loss 0.044121\n",
            "batch 4346: loss 0.122888\n",
            "batch 4347: loss 0.060588\n",
            "batch 4348: loss 0.114830\n",
            "batch 4349: loss 0.339000\n",
            "batch 4350: loss 0.026190\n",
            "batch 4351: loss 0.093162\n",
            "batch 4352: loss 0.168205\n",
            "batch 4353: loss 0.039932\n",
            "batch 4354: loss 0.023276\n",
            "batch 4355: loss 0.021282\n",
            "batch 4356: loss 0.014613\n",
            "batch 4357: loss 0.071403\n",
            "batch 4358: loss 0.016225\n",
            "batch 4359: loss 0.141216\n",
            "batch 4360: loss 0.038532\n",
            "batch 4361: loss 0.136062\n",
            "batch 4362: loss 0.073016\n",
            "batch 4363: loss 0.044633\n",
            "batch 4364: loss 0.038068\n",
            "batch 4365: loss 0.134962\n",
            "batch 4366: loss 0.050689\n",
            "batch 4367: loss 0.043506\n",
            "batch 4368: loss 0.100354\n",
            "batch 4369: loss 0.036757\n",
            "batch 4370: loss 0.076291\n",
            "batch 4371: loss 0.118874\n",
            "batch 4372: loss 0.017481\n",
            "batch 4373: loss 0.143378\n",
            "batch 4374: loss 0.124133\n",
            "batch 4375: loss 0.028568\n",
            "batch 4376: loss 0.012521\n",
            "batch 4377: loss 0.022108\n",
            "batch 4378: loss 0.085773\n",
            "batch 4379: loss 0.016325\n",
            "batch 4380: loss 0.066382\n",
            "batch 4381: loss 0.027794\n",
            "batch 4382: loss 0.029588\n",
            "batch 4383: loss 0.014657\n",
            "batch 4384: loss 0.027964\n",
            "batch 4385: loss 0.021443\n",
            "batch 4386: loss 0.012315\n",
            "batch 4387: loss 0.027543\n",
            "batch 4388: loss 0.013177\n",
            "batch 4389: loss 0.165696\n",
            "batch 4390: loss 0.076878\n",
            "batch 4391: loss 0.007716\n",
            "batch 4392: loss 0.031215\n",
            "batch 4393: loss 0.067455\n",
            "batch 4394: loss 0.027641\n",
            "batch 4395: loss 0.084785\n",
            "batch 4396: loss 0.206962\n",
            "batch 4397: loss 0.033777\n",
            "batch 4398: loss 0.008968\n",
            "batch 4399: loss 0.026501\n",
            "batch 4400: loss 0.051516\n",
            "batch 4401: loss 0.034765\n",
            "batch 4402: loss 0.049221\n",
            "batch 4403: loss 0.053565\n",
            "batch 4404: loss 0.036620\n",
            "batch 4405: loss 0.056743\n",
            "batch 4406: loss 0.157555\n",
            "batch 4407: loss 0.022110\n",
            "batch 4408: loss 0.114401\n",
            "batch 4409: loss 0.051016\n",
            "batch 4410: loss 0.021906\n",
            "batch 4411: loss 0.119149\n",
            "batch 4412: loss 0.039901\n",
            "batch 4413: loss 0.046462\n",
            "batch 4414: loss 0.051962\n",
            "batch 4415: loss 0.033579\n",
            "batch 4416: loss 0.084818\n",
            "batch 4417: loss 0.084823\n",
            "batch 4418: loss 0.029619\n",
            "batch 4419: loss 0.075695\n",
            "batch 4420: loss 0.183934\n",
            "batch 4421: loss 0.121693\n",
            "batch 4422: loss 0.109417\n",
            "batch 4423: loss 0.098992\n",
            "batch 4424: loss 0.037605\n",
            "batch 4425: loss 0.045886\n",
            "batch 4426: loss 0.079118\n",
            "batch 4427: loss 0.019710\n",
            "batch 4428: loss 0.093547\n",
            "batch 4429: loss 0.024999\n",
            "batch 4430: loss 0.103309\n",
            "batch 4431: loss 0.011918\n",
            "batch 4432: loss 0.025933\n",
            "batch 4433: loss 0.047497\n",
            "batch 4434: loss 0.152062\n",
            "batch 4435: loss 0.104695\n",
            "batch 4436: loss 0.167611\n",
            "batch 4437: loss 0.022446\n",
            "batch 4438: loss 0.055360\n",
            "batch 4439: loss 0.287885\n",
            "batch 4440: loss 0.056801\n",
            "batch 4441: loss 0.058986\n",
            "batch 4442: loss 0.040647\n",
            "batch 4443: loss 0.175533\n",
            "batch 4444: loss 0.075046\n",
            "batch 4445: loss 0.073561\n",
            "batch 4446: loss 0.152547\n",
            "batch 4447: loss 0.008975\n",
            "batch 4448: loss 0.108106\n",
            "batch 4449: loss 0.153809\n",
            "batch 4450: loss 0.161734\n",
            "batch 4451: loss 0.054967\n",
            "batch 4452: loss 0.130196\n",
            "batch 4453: loss 0.075566\n",
            "batch 4454: loss 0.101141\n",
            "batch 4455: loss 0.035500\n",
            "batch 4456: loss 0.137301\n",
            "batch 4457: loss 0.027211\n",
            "batch 4458: loss 0.069008\n",
            "batch 4459: loss 0.121657\n",
            "batch 4460: loss 0.248497\n",
            "batch 4461: loss 0.036545\n",
            "batch 4462: loss 0.027318\n",
            "batch 4463: loss 0.161719\n",
            "batch 4464: loss 0.070738\n",
            "batch 4465: loss 0.185689\n",
            "batch 4466: loss 0.037712\n",
            "batch 4467: loss 0.026881\n",
            "batch 4468: loss 0.038534\n",
            "batch 4469: loss 0.046141\n",
            "batch 4470: loss 0.099539\n",
            "batch 4471: loss 0.055916\n",
            "batch 4472: loss 0.047652\n",
            "batch 4473: loss 0.111641\n",
            "batch 4474: loss 0.017671\n",
            "batch 4475: loss 0.007044\n",
            "batch 4476: loss 0.145509\n",
            "batch 4477: loss 0.083133\n",
            "batch 4478: loss 0.058805\n",
            "batch 4479: loss 0.050433\n",
            "batch 4480: loss 0.034022\n",
            "batch 4481: loss 0.082004\n",
            "batch 4482: loss 0.031440\n",
            "batch 4483: loss 0.029010\n",
            "batch 4484: loss 0.236622\n",
            "batch 4485: loss 0.093133\n",
            "batch 4486: loss 0.045728\n",
            "batch 4487: loss 0.014628\n",
            "batch 4488: loss 0.007557\n",
            "batch 4489: loss 0.105164\n",
            "batch 4490: loss 0.082119\n",
            "batch 4491: loss 0.028467\n",
            "batch 4492: loss 0.046020\n",
            "batch 4493: loss 0.026436\n",
            "batch 4494: loss 0.021265\n",
            "batch 4495: loss 0.081528\n",
            "batch 4496: loss 0.270101\n",
            "batch 4497: loss 0.064143\n",
            "batch 4498: loss 0.045121\n",
            "batch 4499: loss 0.194438\n",
            "batch 4500: loss 0.035922\n",
            "batch 4501: loss 0.061882\n",
            "batch 4502: loss 0.138211\n",
            "batch 4503: loss 0.083951\n",
            "batch 4504: loss 0.056574\n",
            "batch 4505: loss 0.048958\n",
            "batch 4506: loss 0.014620\n",
            "batch 4507: loss 0.040269\n",
            "batch 4508: loss 0.052131\n",
            "batch 4509: loss 0.090929\n",
            "batch 4510: loss 0.106723\n",
            "batch 4511: loss 0.182370\n",
            "batch 4512: loss 0.058616\n",
            "batch 4513: loss 0.069968\n",
            "batch 4514: loss 0.155785\n",
            "batch 4515: loss 0.081859\n",
            "batch 4516: loss 0.068993\n",
            "batch 4517: loss 0.112578\n",
            "batch 4518: loss 0.025709\n",
            "batch 4519: loss 0.068369\n",
            "batch 4520: loss 0.031609\n",
            "batch 4521: loss 0.066357\n",
            "batch 4522: loss 0.041190\n",
            "batch 4523: loss 0.051741\n",
            "batch 4524: loss 0.146723\n",
            "batch 4525: loss 0.052956\n",
            "batch 4526: loss 0.064227\n",
            "batch 4527: loss 0.059517\n",
            "batch 4528: loss 0.067185\n",
            "batch 4529: loss 0.096150\n",
            "batch 4530: loss 0.063115\n",
            "batch 4531: loss 0.049739\n",
            "batch 4532: loss 0.124627\n",
            "batch 4533: loss 0.149832\n",
            "batch 4534: loss 0.039341\n",
            "batch 4535: loss 0.036166\n",
            "batch 4536: loss 0.017782\n",
            "batch 4537: loss 0.011515\n",
            "batch 4538: loss 0.051789\n",
            "batch 4539: loss 0.071311\n",
            "batch 4540: loss 0.067455\n",
            "batch 4541: loss 0.077556\n",
            "batch 4542: loss 0.036299\n",
            "batch 4543: loss 0.012881\n",
            "batch 4544: loss 0.038249\n",
            "batch 4545: loss 0.057446\n",
            "batch 4546: loss 0.013510\n",
            "batch 4547: loss 0.120317\n",
            "batch 4548: loss 0.047594\n",
            "batch 4549: loss 0.038793\n",
            "batch 4550: loss 0.152709\n",
            "batch 4551: loss 0.014604\n",
            "batch 4552: loss 0.031871\n",
            "batch 4553: loss 0.075832\n",
            "batch 4554: loss 0.028754\n",
            "batch 4555: loss 0.083313\n",
            "batch 4556: loss 0.057334\n",
            "batch 4557: loss 0.050564\n",
            "batch 4558: loss 0.180967\n",
            "batch 4559: loss 0.051594\n",
            "batch 4560: loss 0.162246\n",
            "batch 4561: loss 0.131188\n",
            "batch 4562: loss 0.025874\n",
            "batch 4563: loss 0.055473\n",
            "batch 4564: loss 0.071398\n",
            "batch 4565: loss 0.043686\n",
            "batch 4566: loss 0.023424\n",
            "batch 4567: loss 0.070354\n",
            "batch 4568: loss 0.051651\n",
            "batch 4569: loss 0.044775\n",
            "batch 4570: loss 0.024465\n",
            "batch 4571: loss 0.029706\n",
            "batch 4572: loss 0.077566\n",
            "batch 4573: loss 0.040266\n",
            "batch 4574: loss 0.069572\n",
            "batch 4575: loss 0.053225\n",
            "batch 4576: loss 0.028422\n",
            "batch 4577: loss 0.190524\n",
            "batch 4578: loss 0.042624\n",
            "batch 4579: loss 0.049798\n",
            "batch 4580: loss 0.281252\n",
            "batch 4581: loss 0.123485\n",
            "batch 4582: loss 0.006666\n",
            "batch 4583: loss 0.131079\n",
            "batch 4584: loss 0.030923\n",
            "batch 4585: loss 0.025420\n",
            "batch 4586: loss 0.044173\n",
            "batch 4587: loss 0.096339\n",
            "batch 4588: loss 0.018855\n",
            "batch 4589: loss 0.020423\n",
            "batch 4590: loss 0.044831\n",
            "batch 4591: loss 0.052645\n",
            "batch 4592: loss 0.043579\n",
            "batch 4593: loss 0.095597\n",
            "batch 4594: loss 0.087195\n",
            "batch 4595: loss 0.060075\n",
            "batch 4596: loss 0.095469\n",
            "batch 4597: loss 0.111307\n",
            "batch 4598: loss 0.011829\n",
            "batch 4599: loss 0.054949\n",
            "batch 4600: loss 0.047244\n",
            "batch 4601: loss 0.015704\n",
            "batch 4602: loss 0.068736\n",
            "batch 4603: loss 0.033211\n",
            "batch 4604: loss 0.035644\n",
            "batch 4605: loss 0.115542\n",
            "batch 4606: loss 0.067789\n",
            "batch 4607: loss 0.068624\n",
            "batch 4608: loss 0.026882\n",
            "batch 4609: loss 0.091383\n",
            "batch 4610: loss 0.028570\n",
            "batch 4611: loss 0.083693\n",
            "batch 4612: loss 0.028366\n",
            "batch 4613: loss 0.128263\n",
            "batch 4614: loss 0.232421\n",
            "batch 4615: loss 0.042962\n",
            "batch 4616: loss 0.030657\n",
            "batch 4617: loss 0.020235\n",
            "batch 4618: loss 0.019087\n",
            "batch 4619: loss 0.086422\n",
            "batch 4620: loss 0.043844\n",
            "batch 4621: loss 0.097361\n",
            "batch 4622: loss 0.115928\n",
            "batch 4623: loss 0.033873\n",
            "batch 4624: loss 0.014998\n",
            "batch 4625: loss 0.052262\n",
            "batch 4626: loss 0.138653\n",
            "batch 4627: loss 0.099130\n",
            "batch 4628: loss 0.065153\n",
            "batch 4629: loss 0.033328\n",
            "batch 4630: loss 0.048534\n",
            "batch 4631: loss 0.156002\n",
            "batch 4632: loss 0.097448\n",
            "batch 4633: loss 0.036397\n",
            "batch 4634: loss 0.049254\n",
            "batch 4635: loss 0.064235\n",
            "batch 4636: loss 0.112688\n",
            "batch 4637: loss 0.098071\n",
            "batch 4638: loss 0.049179\n",
            "batch 4639: loss 0.010708\n",
            "batch 4640: loss 0.126010\n",
            "batch 4641: loss 0.050402\n",
            "batch 4642: loss 0.095559\n",
            "batch 4643: loss 0.030433\n",
            "batch 4644: loss 0.074210\n",
            "batch 4645: loss 0.023962\n",
            "batch 4646: loss 0.034012\n",
            "batch 4647: loss 0.013571\n",
            "batch 4648: loss 0.069686\n",
            "batch 4649: loss 0.080619\n",
            "batch 4650: loss 0.098582\n",
            "batch 4651: loss 0.175964\n",
            "batch 4652: loss 0.089619\n",
            "batch 4653: loss 0.064296\n",
            "batch 4654: loss 0.014419\n",
            "batch 4655: loss 0.014955\n",
            "batch 4656: loss 0.056759\n",
            "batch 4657: loss 0.021993\n",
            "batch 4658: loss 0.017526\n",
            "batch 4659: loss 0.037200\n",
            "batch 4660: loss 0.038217\n",
            "batch 4661: loss 0.035349\n",
            "batch 4662: loss 0.044095\n",
            "batch 4663: loss 0.024041\n",
            "batch 4664: loss 0.008231\n",
            "batch 4665: loss 0.035212\n",
            "batch 4666: loss 0.064120\n",
            "batch 4667: loss 0.068722\n",
            "batch 4668: loss 0.050671\n",
            "batch 4669: loss 0.064132\n",
            "batch 4670: loss 0.011845\n",
            "batch 4671: loss 0.063455\n",
            "batch 4672: loss 0.060828\n",
            "batch 4673: loss 0.117658\n",
            "batch 4674: loss 0.121885\n",
            "batch 4675: loss 0.046078\n",
            "batch 4676: loss 0.087677\n",
            "batch 4677: loss 0.032944\n",
            "batch 4678: loss 0.010851\n",
            "batch 4679: loss 0.075256\n",
            "batch 4680: loss 0.004580\n",
            "batch 4681: loss 0.060294\n",
            "batch 4682: loss 0.067708\n",
            "batch 4683: loss 0.024664\n",
            "batch 4684: loss 0.057556\n",
            "batch 4685: loss 0.006188\n",
            "batch 4686: loss 0.017608\n",
            "batch 4687: loss 0.068319\n",
            "batch 4688: loss 0.076433\n",
            "batch 4689: loss 0.071162\n",
            "batch 4690: loss 0.025241\n",
            "batch 4691: loss 0.122609\n",
            "batch 4692: loss 0.060594\n",
            "batch 4693: loss 0.039632\n",
            "batch 4694: loss 0.060812\n",
            "batch 4695: loss 0.039405\n",
            "batch 4696: loss 0.012922\n",
            "batch 4697: loss 0.036331\n",
            "batch 4698: loss 0.044035\n",
            "batch 4699: loss 0.012054\n",
            "batch 4700: loss 0.048685\n",
            "batch 4701: loss 0.013955\n",
            "batch 4702: loss 0.036896\n",
            "batch 4703: loss 0.065406\n",
            "batch 4704: loss 0.134836\n",
            "batch 4705: loss 0.060465\n",
            "batch 4706: loss 0.153356\n",
            "batch 4707: loss 0.207175\n",
            "batch 4708: loss 0.011690\n",
            "batch 4709: loss 0.020001\n",
            "batch 4710: loss 0.166789\n",
            "batch 4711: loss 0.006999\n",
            "batch 4712: loss 0.038772\n",
            "batch 4713: loss 0.018539\n",
            "batch 4714: loss 0.079769\n",
            "batch 4715: loss 0.139597\n",
            "batch 4716: loss 0.009835\n",
            "batch 4717: loss 0.018348\n",
            "batch 4718: loss 0.148520\n",
            "batch 4719: loss 0.087114\n",
            "batch 4720: loss 0.121172\n",
            "batch 4721: loss 0.052711\n",
            "batch 4722: loss 0.044992\n",
            "batch 4723: loss 0.030456\n",
            "batch 4724: loss 0.063449\n",
            "batch 4725: loss 0.076058\n",
            "batch 4726: loss 0.016108\n",
            "batch 4727: loss 0.025183\n",
            "batch 4728: loss 0.055724\n",
            "batch 4729: loss 0.066329\n",
            "batch 4730: loss 0.007329\n",
            "batch 4731: loss 0.060787\n",
            "batch 4732: loss 0.066795\n",
            "batch 4733: loss 0.139786\n",
            "batch 4734: loss 0.049380\n",
            "batch 4735: loss 0.018188\n",
            "batch 4736: loss 0.058123\n",
            "batch 4737: loss 0.066963\n",
            "batch 4738: loss 0.041965\n",
            "batch 4739: loss 0.068580\n",
            "batch 4740: loss 0.059048\n",
            "batch 4741: loss 0.101867\n",
            "batch 4742: loss 0.038205\n",
            "batch 4743: loss 0.029158\n",
            "batch 4744: loss 0.034563\n",
            "batch 4745: loss 0.021816\n",
            "batch 4746: loss 0.016064\n",
            "batch 4747: loss 0.240684\n",
            "batch 4748: loss 0.026375\n",
            "batch 4749: loss 0.056455\n",
            "batch 4750: loss 0.057132\n",
            "batch 4751: loss 0.054464\n",
            "batch 4752: loss 0.023703\n",
            "batch 4753: loss 0.067594\n",
            "batch 4754: loss 0.052440\n",
            "batch 4755: loss 0.149702\n",
            "batch 4756: loss 0.112720\n",
            "batch 4757: loss 0.015863\n",
            "batch 4758: loss 0.164279\n",
            "batch 4759: loss 0.175135\n",
            "batch 4760: loss 0.018767\n",
            "batch 4761: loss 0.062515\n",
            "batch 4762: loss 0.026324\n",
            "batch 4763: loss 0.021691\n",
            "batch 4764: loss 0.063155\n",
            "batch 4765: loss 0.089357\n",
            "batch 4766: loss 0.216209\n",
            "batch 4767: loss 0.010187\n",
            "batch 4768: loss 0.030145\n",
            "batch 4769: loss 0.292385\n",
            "batch 4770: loss 0.063600\n",
            "batch 4771: loss 0.036295\n",
            "batch 4772: loss 0.049276\n",
            "batch 4773: loss 0.082310\n",
            "batch 4774: loss 0.017117\n",
            "batch 4775: loss 0.078894\n",
            "batch 4776: loss 0.045633\n",
            "batch 4777: loss 0.168236\n",
            "batch 4778: loss 0.063149\n",
            "batch 4779: loss 0.024942\n",
            "batch 4780: loss 0.014207\n",
            "batch 4781: loss 0.058464\n",
            "batch 4782: loss 0.052879\n",
            "batch 4783: loss 0.008657\n",
            "batch 4784: loss 0.045681\n",
            "batch 4785: loss 0.022591\n",
            "batch 4786: loss 0.064729\n",
            "batch 4787: loss 0.103205\n",
            "batch 4788: loss 0.058391\n",
            "batch 4789: loss 0.083846\n",
            "batch 4790: loss 0.117376\n",
            "batch 4791: loss 0.111446\n",
            "batch 4792: loss 0.148979\n",
            "batch 4793: loss 0.090770\n",
            "batch 4794: loss 0.050799\n",
            "batch 4795: loss 0.025505\n",
            "batch 4796: loss 0.015974\n",
            "batch 4797: loss 0.040044\n",
            "batch 4798: loss 0.026816\n",
            "batch 4799: loss 0.033375\n",
            "batch 4800: loss 0.112094\n",
            "batch 4801: loss 0.060892\n",
            "batch 4802: loss 0.257129\n",
            "batch 4803: loss 0.064861\n",
            "batch 4804: loss 0.038288\n",
            "batch 4805: loss 0.029236\n",
            "batch 4806: loss 0.011974\n",
            "batch 4807: loss 0.045057\n",
            "batch 4808: loss 0.024177\n",
            "batch 4809: loss 0.043574\n",
            "batch 4810: loss 0.013277\n",
            "batch 4811: loss 0.095548\n",
            "batch 4812: loss 0.139139\n",
            "batch 4813: loss 0.113724\n",
            "batch 4814: loss 0.010322\n",
            "batch 4815: loss 0.044725\n",
            "batch 4816: loss 0.100087\n",
            "batch 4817: loss 0.115179\n",
            "batch 4818: loss 0.044210\n",
            "batch 4819: loss 0.057660\n",
            "batch 4820: loss 0.043028\n",
            "batch 4821: loss 0.064454\n",
            "batch 4822: loss 0.147123\n",
            "batch 4823: loss 0.093811\n",
            "batch 4824: loss 0.031403\n",
            "batch 4825: loss 0.028312\n",
            "batch 4826: loss 0.021963\n",
            "batch 4827: loss 0.083891\n",
            "batch 4828: loss 0.083944\n",
            "batch 4829: loss 0.020873\n",
            "batch 4830: loss 0.022699\n",
            "batch 4831: loss 0.039614\n",
            "batch 4832: loss 0.080003\n",
            "batch 4833: loss 0.024194\n",
            "batch 4834: loss 0.060445\n",
            "batch 4835: loss 0.035368\n",
            "batch 4836: loss 0.141901\n",
            "batch 4837: loss 0.028939\n",
            "batch 4838: loss 0.041613\n",
            "batch 4839: loss 0.035369\n",
            "batch 4840: loss 0.024671\n",
            "batch 4841: loss 0.183554\n",
            "batch 4842: loss 0.032132\n",
            "batch 4843: loss 0.039150\n",
            "batch 4844: loss 0.056974\n",
            "batch 4845: loss 0.344896\n",
            "batch 4846: loss 0.087649\n",
            "batch 4847: loss 0.049737\n",
            "batch 4848: loss 0.034059\n",
            "batch 4849: loss 0.208219\n",
            "batch 4850: loss 0.064601\n",
            "batch 4851: loss 0.015227\n",
            "batch 4852: loss 0.010102\n",
            "batch 4853: loss 0.066534\n",
            "batch 4854: loss 0.018201\n",
            "batch 4855: loss 0.030213\n",
            "batch 4856: loss 0.008198\n",
            "batch 4857: loss 0.045107\n",
            "batch 4858: loss 0.027446\n",
            "batch 4859: loss 0.038399\n",
            "batch 4860: loss 0.086654\n",
            "batch 4861: loss 0.016077\n",
            "batch 4862: loss 0.026416\n",
            "batch 4863: loss 0.066186\n",
            "batch 4864: loss 0.013294\n",
            "batch 4865: loss 0.110153\n",
            "batch 4866: loss 0.026007\n",
            "batch 4867: loss 0.050921\n",
            "batch 4868: loss 0.053304\n",
            "batch 4869: loss 0.031426\n",
            "batch 4870: loss 0.122450\n",
            "batch 4871: loss 0.013316\n",
            "batch 4872: loss 0.048353\n",
            "batch 4873: loss 0.026768\n",
            "batch 4874: loss 0.045263\n",
            "batch 4875: loss 0.018997\n",
            "batch 4876: loss 0.128059\n",
            "batch 4877: loss 0.204019\n",
            "batch 4878: loss 0.039602\n",
            "batch 4879: loss 0.072282\n",
            "batch 4880: loss 0.274691\n",
            "batch 4881: loss 0.064298\n",
            "batch 4882: loss 0.028284\n",
            "batch 4883: loss 0.007325\n",
            "batch 4884: loss 0.045956\n",
            "batch 4885: loss 0.047933\n",
            "batch 4886: loss 0.065040\n",
            "batch 4887: loss 0.024980\n",
            "batch 4888: loss 0.052880\n",
            "batch 4889: loss 0.006347\n",
            "batch 4890: loss 0.104888\n",
            "batch 4891: loss 0.050626\n",
            "batch 4892: loss 0.005620\n",
            "batch 4893: loss 0.131380\n",
            "batch 4894: loss 0.121724\n",
            "batch 4895: loss 0.007940\n",
            "batch 4896: loss 0.027176\n",
            "batch 4897: loss 0.011120\n",
            "batch 4898: loss 0.042286\n",
            "batch 4899: loss 0.011794\n",
            "batch 4900: loss 0.184910\n",
            "batch 4901: loss 0.021152\n",
            "batch 4902: loss 0.050723\n",
            "batch 4903: loss 0.029924\n",
            "batch 4904: loss 0.038994\n",
            "batch 4905: loss 0.007858\n",
            "batch 4906: loss 0.046030\n",
            "batch 4907: loss 0.128555\n",
            "batch 4908: loss 0.070200\n",
            "batch 4909: loss 0.060558\n",
            "batch 4910: loss 0.042991\n",
            "batch 4911: loss 0.196992\n",
            "batch 4912: loss 0.015131\n",
            "batch 4913: loss 0.038218\n",
            "batch 4914: loss 0.076863\n",
            "batch 4915: loss 0.016503\n",
            "batch 4916: loss 0.008431\n",
            "batch 4917: loss 0.076634\n",
            "batch 4918: loss 0.022253\n",
            "batch 4919: loss 0.016448\n",
            "batch 4920: loss 0.050245\n",
            "batch 4921: loss 0.088998\n",
            "batch 4922: loss 0.030043\n",
            "batch 4923: loss 0.059259\n",
            "batch 4924: loss 0.093222\n",
            "batch 4925: loss 0.027423\n",
            "batch 4926: loss 0.133946\n",
            "batch 4927: loss 0.013734\n",
            "batch 4928: loss 0.056919\n",
            "batch 4929: loss 0.101595\n",
            "batch 4930: loss 0.036022\n",
            "batch 4931: loss 0.063638\n",
            "batch 4932: loss 0.026319\n",
            "batch 4933: loss 0.050674\n",
            "batch 4934: loss 0.153323\n",
            "batch 4935: loss 0.015931\n",
            "batch 4936: loss 0.023187\n",
            "batch 4937: loss 0.038911\n",
            "batch 4938: loss 0.111744\n",
            "batch 4939: loss 0.073457\n",
            "batch 4940: loss 0.018734\n",
            "batch 4941: loss 0.061884\n",
            "batch 4942: loss 0.019846\n",
            "batch 4943: loss 0.075399\n",
            "batch 4944: loss 0.014635\n",
            "batch 4945: loss 0.095938\n",
            "batch 4946: loss 0.047998\n",
            "batch 4947: loss 0.055883\n",
            "batch 4948: loss 0.078946\n",
            "batch 4949: loss 0.114654\n",
            "batch 4950: loss 0.081555\n",
            "batch 4951: loss 0.028647\n",
            "batch 4952: loss 0.012896\n",
            "batch 4953: loss 0.067759\n",
            "batch 4954: loss 0.038391\n",
            "batch 4955: loss 0.073718\n",
            "batch 4956: loss 0.028704\n",
            "batch 4957: loss 0.040391\n",
            "batch 4958: loss 0.068139\n",
            "batch 4959: loss 0.050110\n",
            "batch 4960: loss 0.130548\n",
            "batch 4961: loss 0.024426\n",
            "batch 4962: loss 0.007264\n",
            "batch 4963: loss 0.057392\n",
            "batch 4964: loss 0.030325\n",
            "batch 4965: loss 0.042001\n",
            "batch 4966: loss 0.033591\n",
            "batch 4967: loss 0.276047\n",
            "batch 4968: loss 0.049517\n",
            "batch 4969: loss 0.069440\n",
            "batch 4970: loss 0.026070\n",
            "batch 4971: loss 0.136999\n",
            "batch 4972: loss 0.022272\n",
            "batch 4973: loss 0.028330\n",
            "batch 4974: loss 0.045381\n",
            "batch 4975: loss 0.011552\n",
            "batch 4976: loss 0.022691\n",
            "batch 4977: loss 0.130630\n",
            "batch 4978: loss 0.197072\n",
            "batch 4979: loss 0.019129\n",
            "batch 4980: loss 0.111299\n",
            "batch 4981: loss 0.015226\n",
            "batch 4982: loss 0.025220\n",
            "batch 4983: loss 0.091435\n",
            "batch 4984: loss 0.141907\n",
            "batch 4985: loss 0.011413\n",
            "batch 4986: loss 0.028405\n",
            "batch 4987: loss 0.069626\n",
            "batch 4988: loss 0.092131\n",
            "batch 4989: loss 0.036421\n",
            "batch 4990: loss 0.034134\n",
            "batch 4991: loss 0.006032\n",
            "batch 4992: loss 0.076234\n",
            "batch 4993: loss 0.071303\n",
            "batch 4994: loss 0.006739\n",
            "batch 4995: loss 0.035821\n",
            "batch 4996: loss 0.130257\n",
            "batch 4997: loss 0.049398\n",
            "batch 4998: loss 0.095825\n",
            "batch 4999: loss 0.024017\n",
            "batch 5000: loss 0.017924\n",
            "batch 5001: loss 0.093063\n",
            "batch 5002: loss 0.042606\n",
            "batch 5003: loss 0.081753\n",
            "batch 5004: loss 0.054043\n",
            "batch 5005: loss 0.037882\n",
            "batch 5006: loss 0.055996\n",
            "batch 5007: loss 0.038437\n",
            "batch 5008: loss 0.178695\n",
            "batch 5009: loss 0.030992\n",
            "batch 5010: loss 0.070504\n",
            "batch 5011: loss 0.151599\n",
            "batch 5012: loss 0.029861\n",
            "batch 5013: loss 0.052956\n",
            "batch 5014: loss 0.071189\n",
            "batch 5015: loss 0.076696\n",
            "batch 5016: loss 0.016513\n",
            "batch 5017: loss 0.161075\n",
            "batch 5018: loss 0.070619\n",
            "batch 5019: loss 0.074341\n",
            "batch 5020: loss 0.060901\n",
            "batch 5021: loss 0.104290\n",
            "batch 5022: loss 0.019017\n",
            "batch 5023: loss 0.067450\n",
            "batch 5024: loss 0.057621\n",
            "batch 5025: loss 0.110958\n",
            "batch 5026: loss 0.024293\n",
            "batch 5027: loss 0.033089\n",
            "batch 5028: loss 0.018525\n",
            "batch 5029: loss 0.099877\n",
            "batch 5030: loss 0.009976\n",
            "batch 5031: loss 0.142291\n",
            "batch 5032: loss 0.041730\n",
            "batch 5033: loss 0.053791\n",
            "batch 5034: loss 0.016334\n",
            "batch 5035: loss 0.019553\n",
            "batch 5036: loss 0.048237\n",
            "batch 5037: loss 0.075343\n",
            "batch 5038: loss 0.059254\n",
            "batch 5039: loss 0.172367\n",
            "batch 5040: loss 0.016675\n",
            "batch 5041: loss 0.022316\n",
            "batch 5042: loss 0.157820\n",
            "batch 5043: loss 0.014846\n",
            "batch 5044: loss 0.061241\n",
            "batch 5045: loss 0.044173\n",
            "batch 5046: loss 0.030546\n",
            "batch 5047: loss 0.034777\n",
            "batch 5048: loss 0.012005\n",
            "batch 5049: loss 0.029195\n",
            "batch 5050: loss 0.079585\n",
            "batch 5051: loss 0.027798\n",
            "batch 5052: loss 0.320205\n",
            "batch 5053: loss 0.075072\n",
            "batch 5054: loss 0.008572\n",
            "batch 5055: loss 0.077321\n",
            "batch 5056: loss 0.064586\n",
            "batch 5057: loss 0.056288\n",
            "batch 5058: loss 0.091353\n",
            "batch 5059: loss 0.054239\n",
            "batch 5060: loss 0.030687\n",
            "batch 5061: loss 0.043547\n",
            "batch 5062: loss 0.024580\n",
            "batch 5063: loss 0.070939\n",
            "batch 5064: loss 0.043654\n",
            "batch 5065: loss 0.039968\n",
            "batch 5066: loss 0.024525\n",
            "batch 5067: loss 0.043803\n",
            "batch 5068: loss 0.066270\n",
            "batch 5069: loss 0.031327\n",
            "batch 5070: loss 0.110183\n",
            "batch 5071: loss 0.142277\n",
            "batch 5072: loss 0.018807\n",
            "batch 5073: loss 0.036930\n",
            "batch 5074: loss 0.020782\n",
            "batch 5075: loss 0.038332\n",
            "batch 5076: loss 0.063000\n",
            "batch 5077: loss 0.213523\n",
            "batch 5078: loss 0.005647\n",
            "batch 5079: loss 0.017377\n",
            "batch 5080: loss 0.314956\n",
            "batch 5081: loss 0.105342\n",
            "batch 5082: loss 0.057097\n",
            "batch 5083: loss 0.019549\n",
            "batch 5084: loss 0.006960\n",
            "batch 5085: loss 0.011308\n",
            "batch 5086: loss 0.039453\n",
            "batch 5087: loss 0.027857\n",
            "batch 5088: loss 0.042683\n",
            "batch 5089: loss 0.124082\n",
            "batch 5090: loss 0.067714\n",
            "batch 5091: loss 0.022325\n",
            "batch 5092: loss 0.177779\n",
            "batch 5093: loss 0.013879\n",
            "batch 5094: loss 0.039597\n",
            "batch 5095: loss 0.130148\n",
            "batch 5096: loss 0.024849\n",
            "batch 5097: loss 0.032877\n",
            "batch 5098: loss 0.025726\n",
            "batch 5099: loss 0.104243\n",
            "batch 5100: loss 0.108574\n",
            "batch 5101: loss 0.088709\n",
            "batch 5102: loss 0.040834\n",
            "batch 5103: loss 0.123636\n",
            "batch 5104: loss 0.033232\n",
            "batch 5105: loss 0.043846\n",
            "batch 5106: loss 0.035950\n",
            "batch 5107: loss 0.009852\n",
            "batch 5108: loss 0.175532\n",
            "batch 5109: loss 0.060865\n",
            "batch 5110: loss 0.029270\n",
            "batch 5111: loss 0.023414\n",
            "batch 5112: loss 0.041369\n",
            "batch 5113: loss 0.215982\n",
            "batch 5114: loss 0.096339\n",
            "batch 5115: loss 0.037275\n",
            "batch 5116: loss 0.016050\n",
            "batch 5117: loss 0.023438\n",
            "batch 5118: loss 0.039652\n",
            "batch 5119: loss 0.046004\n",
            "batch 5120: loss 0.054712\n",
            "batch 5121: loss 0.110511\n",
            "batch 5122: loss 0.030254\n",
            "batch 5123: loss 0.064766\n",
            "batch 5124: loss 0.035365\n",
            "batch 5125: loss 0.092692\n",
            "batch 5126: loss 0.015018\n",
            "batch 5127: loss 0.017391\n",
            "batch 5128: loss 0.031263\n",
            "batch 5129: loss 0.083744\n",
            "batch 5130: loss 0.051649\n",
            "batch 5131: loss 0.020067\n",
            "batch 5132: loss 0.033870\n",
            "batch 5133: loss 0.054690\n",
            "batch 5134: loss 0.060768\n",
            "batch 5135: loss 0.129707\n",
            "batch 5136: loss 0.018677\n",
            "batch 5137: loss 0.028248\n",
            "batch 5138: loss 0.020390\n",
            "batch 5139: loss 0.069805\n",
            "batch 5140: loss 0.044321\n",
            "batch 5141: loss 0.057301\n",
            "batch 5142: loss 0.070289\n",
            "batch 5143: loss 0.083056\n",
            "batch 5144: loss 0.022467\n",
            "batch 5145: loss 0.022675\n",
            "batch 5146: loss 0.026218\n",
            "batch 5147: loss 0.006571\n",
            "batch 5148: loss 0.093702\n",
            "batch 5149: loss 0.016568\n",
            "batch 5150: loss 0.011562\n",
            "batch 5151: loss 0.030710\n",
            "batch 5152: loss 0.032116\n",
            "batch 5153: loss 0.126154\n",
            "batch 5154: loss 0.043029\n",
            "batch 5155: loss 0.020399\n",
            "batch 5156: loss 0.069170\n",
            "batch 5157: loss 0.009174\n",
            "batch 5158: loss 0.033776\n",
            "batch 5159: loss 0.050042\n",
            "batch 5160: loss 0.020785\n",
            "batch 5161: loss 0.024873\n",
            "batch 5162: loss 0.100648\n",
            "batch 5163: loss 0.055059\n",
            "batch 5164: loss 0.146412\n",
            "batch 5165: loss 0.094946\n",
            "batch 5166: loss 0.115515\n",
            "batch 5167: loss 0.036686\n",
            "batch 5168: loss 0.007573\n",
            "batch 5169: loss 0.060752\n",
            "batch 5170: loss 0.068823\n",
            "batch 5171: loss 0.166419\n",
            "batch 5172: loss 0.167940\n",
            "batch 5173: loss 0.091134\n",
            "batch 5174: loss 0.038222\n",
            "batch 5175: loss 0.012665\n",
            "batch 5176: loss 0.066545\n",
            "batch 5177: loss 0.092250\n",
            "batch 5178: loss 0.072322\n",
            "batch 5179: loss 0.049522\n",
            "batch 5180: loss 0.324153\n",
            "batch 5181: loss 0.029708\n",
            "batch 5182: loss 0.025354\n",
            "batch 5183: loss 0.075851\n",
            "batch 5184: loss 0.161900\n",
            "batch 5185: loss 0.018169\n",
            "batch 5186: loss 0.025340\n",
            "batch 5187: loss 0.050595\n",
            "batch 5188: loss 0.018582\n",
            "batch 5189: loss 0.031709\n",
            "batch 5190: loss 0.023908\n",
            "batch 5191: loss 0.064031\n",
            "batch 5192: loss 0.033299\n",
            "batch 5193: loss 0.048349\n",
            "batch 5194: loss 0.015006\n",
            "batch 5195: loss 0.026542\n",
            "batch 5196: loss 0.100847\n",
            "batch 5197: loss 0.022316\n",
            "batch 5198: loss 0.013120\n",
            "batch 5199: loss 0.036102\n",
            "batch 5200: loss 0.009938\n",
            "batch 5201: loss 0.082355\n",
            "batch 5202: loss 0.076770\n",
            "batch 5203: loss 0.054996\n",
            "batch 5204: loss 0.013245\n",
            "batch 5205: loss 0.065845\n",
            "batch 5206: loss 0.101333\n",
            "batch 5207: loss 0.062735\n",
            "batch 5208: loss 0.067968\n",
            "batch 5209: loss 0.104252\n",
            "batch 5210: loss 0.028131\n",
            "batch 5211: loss 0.081181\n",
            "batch 5212: loss 0.020795\n",
            "batch 5213: loss 0.061521\n",
            "batch 5214: loss 0.203795\n",
            "batch 5215: loss 0.079080\n",
            "batch 5216: loss 0.055832\n",
            "batch 5217: loss 0.067401\n",
            "batch 5218: loss 0.051343\n",
            "batch 5219: loss 0.051920\n",
            "batch 5220: loss 0.007776\n",
            "batch 5221: loss 0.051516\n",
            "batch 5222: loss 0.022299\n",
            "batch 5223: loss 0.013455\n",
            "batch 5224: loss 0.012410\n",
            "batch 5225: loss 0.015540\n",
            "batch 5226: loss 0.040115\n",
            "batch 5227: loss 0.041859\n",
            "batch 5228: loss 0.008185\n",
            "batch 5229: loss 0.032141\n",
            "batch 5230: loss 0.019579\n",
            "batch 5231: loss 0.073287\n",
            "batch 5232: loss 0.050570\n",
            "batch 5233: loss 0.022767\n",
            "batch 5234: loss 0.076569\n",
            "batch 5235: loss 0.021403\n",
            "batch 5236: loss 0.009210\n",
            "batch 5237: loss 0.040233\n",
            "batch 5238: loss 0.056359\n",
            "batch 5239: loss 0.044314\n",
            "batch 5240: loss 0.042290\n",
            "batch 5241: loss 0.055963\n",
            "batch 5242: loss 0.041951\n",
            "batch 5243: loss 0.174514\n",
            "batch 5244: loss 0.008637\n",
            "batch 5245: loss 0.116038\n",
            "batch 5246: loss 0.050401\n",
            "batch 5247: loss 0.083701\n",
            "batch 5248: loss 0.128896\n",
            "batch 5249: loss 0.032547\n",
            "batch 5250: loss 0.086434\n",
            "batch 5251: loss 0.194100\n",
            "batch 5252: loss 0.069134\n",
            "batch 5253: loss 0.117034\n",
            "batch 5254: loss 0.038772\n",
            "batch 5255: loss 0.060423\n",
            "batch 5256: loss 0.011469\n",
            "batch 5257: loss 0.053740\n",
            "batch 5258: loss 0.012006\n",
            "batch 5259: loss 0.047918\n",
            "batch 5260: loss 0.049567\n",
            "batch 5261: loss 0.131396\n",
            "batch 5262: loss 0.030032\n",
            "batch 5263: loss 0.044864\n",
            "batch 5264: loss 0.046124\n",
            "batch 5265: loss 0.043511\n",
            "batch 5266: loss 0.101824\n",
            "batch 5267: loss 0.080757\n",
            "batch 5268: loss 0.076023\n",
            "batch 5269: loss 0.035078\n",
            "batch 5270: loss 0.033776\n",
            "batch 5271: loss 0.025997\n",
            "batch 5272: loss 0.100827\n",
            "batch 5273: loss 0.070181\n",
            "batch 5274: loss 0.128038\n",
            "batch 5275: loss 0.049981\n",
            "batch 5276: loss 0.009897\n",
            "batch 5277: loss 0.020240\n",
            "batch 5278: loss 0.063273\n",
            "batch 5279: loss 0.125061\n",
            "batch 5280: loss 0.063845\n",
            "batch 5281: loss 0.023641\n",
            "batch 5282: loss 0.023102\n",
            "batch 5283: loss 0.142208\n",
            "batch 5284: loss 0.016054\n",
            "batch 5285: loss 0.020102\n",
            "batch 5286: loss 0.061916\n",
            "batch 5287: loss 0.083301\n",
            "batch 5288: loss 0.078403\n",
            "batch 5289: loss 0.018424\n",
            "batch 5290: loss 0.130632\n",
            "batch 5291: loss 0.031050\n",
            "batch 5292: loss 0.021503\n",
            "batch 5293: loss 0.014961\n",
            "batch 5294: loss 0.022301\n",
            "batch 5295: loss 0.015669\n",
            "batch 5296: loss 0.022656\n",
            "batch 5297: loss 0.048836\n",
            "batch 5298: loss 0.026210\n",
            "batch 5299: loss 0.026344\n",
            "batch 5300: loss 0.024702\n",
            "batch 5301: loss 0.289780\n",
            "batch 5302: loss 0.047277\n",
            "batch 5303: loss 0.149070\n",
            "batch 5304: loss 0.079782\n",
            "batch 5305: loss 0.225472\n",
            "batch 5306: loss 0.019635\n",
            "batch 5307: loss 0.041321\n",
            "batch 5308: loss 0.082256\n",
            "batch 5309: loss 0.029542\n",
            "batch 5310: loss 0.103961\n",
            "batch 5311: loss 0.046995\n",
            "batch 5312: loss 0.017490\n",
            "batch 5313: loss 0.055689\n",
            "batch 5314: loss 0.005518\n",
            "batch 5315: loss 0.047810\n",
            "batch 5316: loss 0.010408\n",
            "batch 5317: loss 0.145460\n",
            "batch 5318: loss 0.053552\n",
            "batch 5319: loss 0.028550\n",
            "batch 5320: loss 0.001575\n",
            "batch 5321: loss 0.077585\n",
            "batch 5322: loss 0.047367\n",
            "batch 5323: loss 0.009001\n",
            "batch 5324: loss 0.035594\n",
            "batch 5325: loss 0.022730\n",
            "batch 5326: loss 0.107601\n",
            "batch 5327: loss 0.033590\n",
            "batch 5328: loss 0.068875\n",
            "batch 5329: loss 0.034401\n",
            "batch 5330: loss 0.042779\n",
            "batch 5331: loss 0.151839\n",
            "batch 5332: loss 0.016966\n",
            "batch 5333: loss 0.014180\n",
            "batch 5334: loss 0.037938\n",
            "batch 5335: loss 0.019658\n",
            "batch 5336: loss 0.065639\n",
            "batch 5337: loss 0.016034\n",
            "batch 5338: loss 0.074698\n",
            "batch 5339: loss 0.013143\n",
            "batch 5340: loss 0.010810\n",
            "batch 5341: loss 0.131966\n",
            "batch 5342: loss 0.008846\n",
            "batch 5343: loss 0.013813\n",
            "batch 5344: loss 0.050734\n",
            "batch 5345: loss 0.029994\n",
            "batch 5346: loss 0.020864\n",
            "batch 5347: loss 0.183045\n",
            "batch 5348: loss 0.022433\n",
            "batch 5349: loss 0.061521\n",
            "batch 5350: loss 0.037604\n",
            "batch 5351: loss 0.053892\n",
            "batch 5352: loss 0.064498\n",
            "batch 5353: loss 0.069023\n",
            "batch 5354: loss 0.016248\n",
            "batch 5355: loss 0.038022\n",
            "batch 5356: loss 0.108295\n",
            "batch 5357: loss 0.064064\n",
            "batch 5358: loss 0.021778\n",
            "batch 5359: loss 0.025806\n",
            "batch 5360: loss 0.021568\n",
            "batch 5361: loss 0.025132\n",
            "batch 5362: loss 0.049803\n",
            "batch 5363: loss 0.078821\n",
            "batch 5364: loss 0.043912\n",
            "batch 5365: loss 0.074284\n",
            "batch 5366: loss 0.018778\n",
            "batch 5367: loss 0.139096\n",
            "batch 5368: loss 0.096089\n",
            "batch 5369: loss 0.059292\n",
            "batch 5370: loss 0.036473\n",
            "batch 5371: loss 0.079716\n",
            "batch 5372: loss 0.035473\n",
            "batch 5373: loss 0.034262\n",
            "batch 5374: loss 0.163562\n",
            "batch 5375: loss 0.025431\n",
            "batch 5376: loss 0.020630\n",
            "batch 5377: loss 0.060068\n",
            "batch 5378: loss 0.019091\n",
            "batch 5379: loss 0.049263\n",
            "batch 5380: loss 0.075095\n",
            "batch 5381: loss 0.004141\n",
            "batch 5382: loss 0.046975\n",
            "batch 5383: loss 0.116699\n",
            "batch 5384: loss 0.068075\n",
            "batch 5385: loss 0.026783\n",
            "batch 5386: loss 0.045294\n",
            "batch 5387: loss 0.044359\n",
            "batch 5388: loss 0.023590\n",
            "batch 5389: loss 0.240061\n",
            "batch 5390: loss 0.109881\n",
            "batch 5391: loss 0.115198\n",
            "batch 5392: loss 0.015305\n",
            "batch 5393: loss 0.028618\n",
            "batch 5394: loss 0.105955\n",
            "batch 5395: loss 0.062087\n",
            "batch 5396: loss 0.035514\n",
            "batch 5397: loss 0.023169\n",
            "batch 5398: loss 0.049514\n",
            "batch 5399: loss 0.031730\n",
            "batch 5400: loss 0.085335\n",
            "batch 5401: loss 0.053556\n",
            "batch 5402: loss 0.033620\n",
            "batch 5403: loss 0.014600\n",
            "batch 5404: loss 0.017177\n",
            "batch 5405: loss 0.006981\n",
            "batch 5406: loss 0.013063\n",
            "batch 5407: loss 0.060594\n",
            "batch 5408: loss 0.061608\n",
            "batch 5409: loss 0.082522\n",
            "batch 5410: loss 0.023458\n",
            "batch 5411: loss 0.083918\n",
            "batch 5412: loss 0.089368\n",
            "batch 5413: loss 0.065868\n",
            "batch 5414: loss 0.159945\n",
            "batch 5415: loss 0.020385\n",
            "batch 5416: loss 0.012198\n",
            "batch 5417: loss 0.016732\n",
            "batch 5418: loss 0.015271\n",
            "batch 5419: loss 0.084387\n",
            "batch 5420: loss 0.080794\n",
            "batch 5421: loss 0.059226\n",
            "batch 5422: loss 0.010126\n",
            "batch 5423: loss 0.051626\n",
            "batch 5424: loss 0.029753\n",
            "batch 5425: loss 0.015866\n",
            "batch 5426: loss 0.126723\n",
            "batch 5427: loss 0.067517\n",
            "batch 5428: loss 0.062227\n",
            "batch 5429: loss 0.053440\n",
            "batch 5430: loss 0.054756\n",
            "batch 5431: loss 0.006590\n",
            "batch 5432: loss 0.020666\n",
            "batch 5433: loss 0.071523\n",
            "batch 5434: loss 0.095636\n",
            "batch 5435: loss 0.021930\n",
            "batch 5436: loss 0.109217\n",
            "batch 5437: loss 0.018526\n",
            "batch 5438: loss 0.146456\n",
            "batch 5439: loss 0.044293\n",
            "batch 5440: loss 0.126159\n",
            "batch 5441: loss 0.037585\n",
            "batch 5442: loss 0.039120\n",
            "batch 5443: loss 0.097651\n",
            "batch 5444: loss 0.045372\n",
            "batch 5445: loss 0.025070\n",
            "batch 5446: loss 0.095238\n",
            "batch 5447: loss 0.027190\n",
            "batch 5448: loss 0.051656\n",
            "batch 5449: loss 0.109733\n",
            "batch 5450: loss 0.011700\n",
            "batch 5451: loss 0.019339\n",
            "batch 5452: loss 0.036040\n",
            "batch 5453: loss 0.028526\n",
            "batch 5454: loss 0.080066\n",
            "batch 5455: loss 0.044414\n",
            "batch 5456: loss 0.024079\n",
            "batch 5457: loss 0.089851\n",
            "batch 5458: loss 0.018028\n",
            "batch 5459: loss 0.038628\n",
            "batch 5460: loss 0.027512\n",
            "batch 5461: loss 0.046738\n",
            "batch 5462: loss 0.075827\n",
            "batch 5463: loss 0.038083\n",
            "batch 5464: loss 0.145913\n",
            "batch 5465: loss 0.030670\n",
            "batch 5466: loss 0.113884\n",
            "batch 5467: loss 0.114239\n",
            "batch 5468: loss 0.027188\n",
            "batch 5469: loss 0.131665\n",
            "batch 5470: loss 0.045951\n",
            "batch 5471: loss 0.043696\n",
            "batch 5472: loss 0.046685\n",
            "batch 5473: loss 0.219282\n",
            "batch 5474: loss 0.046690\n",
            "batch 5475: loss 0.021476\n",
            "batch 5476: loss 0.024003\n",
            "batch 5477: loss 0.021570\n",
            "batch 5478: loss 0.032911\n",
            "batch 5479: loss 0.012468\n",
            "batch 5480: loss 0.029639\n",
            "batch 5481: loss 0.032399\n",
            "batch 5482: loss 0.026440\n",
            "batch 5483: loss 0.034833\n",
            "batch 5484: loss 0.024144\n",
            "batch 5485: loss 0.003827\n",
            "batch 5486: loss 0.014928\n",
            "batch 5487: loss 0.018851\n",
            "batch 5488: loss 0.060864\n",
            "batch 5489: loss 0.072923\n",
            "batch 5490: loss 0.029694\n",
            "batch 5491: loss 0.068399\n",
            "batch 5492: loss 0.037935\n",
            "batch 5493: loss 0.208918\n",
            "batch 5494: loss 0.057604\n",
            "batch 5495: loss 0.072699\n",
            "batch 5496: loss 0.022669\n",
            "batch 5497: loss 0.068197\n",
            "batch 5498: loss 0.015635\n",
            "batch 5499: loss 0.032412\n",
            "batch 5500: loss 0.048181\n",
            "batch 5501: loss 0.084053\n",
            "batch 5502: loss 0.081312\n",
            "batch 5503: loss 0.019537\n",
            "batch 5504: loss 0.017771\n",
            "batch 5505: loss 0.228474\n",
            "batch 5506: loss 0.029080\n",
            "batch 5507: loss 0.138987\n",
            "batch 5508: loss 0.011303\n",
            "batch 5509: loss 0.154616\n",
            "batch 5510: loss 0.039537\n",
            "batch 5511: loss 0.076092\n",
            "batch 5512: loss 0.040774\n",
            "batch 5513: loss 0.061329\n",
            "batch 5514: loss 0.026718\n",
            "batch 5515: loss 0.098560\n",
            "batch 5516: loss 0.042905\n",
            "batch 5517: loss 0.032644\n",
            "batch 5518: loss 0.171583\n",
            "batch 5519: loss 0.036477\n",
            "batch 5520: loss 0.092741\n",
            "batch 5521: loss 0.085043\n",
            "batch 5522: loss 0.024520\n",
            "batch 5523: loss 0.015092\n",
            "batch 5524: loss 0.017341\n",
            "batch 5525: loss 0.039712\n",
            "batch 5526: loss 0.114038\n",
            "batch 5527: loss 0.013628\n",
            "batch 5528: loss 0.016583\n",
            "batch 5529: loss 0.037616\n",
            "batch 5530: loss 0.019394\n",
            "batch 5531: loss 0.006797\n",
            "batch 5532: loss 0.026509\n",
            "batch 5533: loss 0.019284\n",
            "batch 5534: loss 0.031864\n",
            "batch 5535: loss 0.061204\n",
            "batch 5536: loss 0.018664\n",
            "batch 5537: loss 0.041303\n",
            "batch 5538: loss 0.116771\n",
            "batch 5539: loss 0.053330\n",
            "batch 5540: loss 0.059326\n",
            "batch 5541: loss 0.040116\n",
            "batch 5542: loss 0.012660\n",
            "batch 5543: loss 0.013265\n",
            "batch 5544: loss 0.027551\n",
            "batch 5545: loss 0.031737\n",
            "batch 5546: loss 0.012199\n",
            "batch 5547: loss 0.060702\n",
            "batch 5548: loss 0.006091\n",
            "batch 5549: loss 0.034072\n",
            "batch 5550: loss 0.007288\n",
            "batch 5551: loss 0.058489\n",
            "batch 5552: loss 0.023885\n",
            "batch 5553: loss 0.076059\n",
            "batch 5554: loss 0.062466\n",
            "batch 5555: loss 0.013979\n",
            "batch 5556: loss 0.030808\n",
            "batch 5557: loss 0.035344\n",
            "batch 5558: loss 0.021231\n",
            "batch 5559: loss 0.108108\n",
            "batch 5560: loss 0.076167\n",
            "batch 5561: loss 0.017676\n",
            "batch 5562: loss 0.011925\n",
            "batch 5563: loss 0.062720\n",
            "batch 5564: loss 0.020452\n",
            "batch 5565: loss 0.023608\n",
            "batch 5566: loss 0.029437\n",
            "batch 5567: loss 0.028711\n",
            "batch 5568: loss 0.043631\n",
            "batch 5569: loss 0.049336\n",
            "batch 5570: loss 0.063938\n",
            "batch 5571: loss 0.033370\n",
            "batch 5572: loss 0.006067\n",
            "batch 5573: loss 0.081709\n",
            "batch 5574: loss 0.046472\n",
            "batch 5575: loss 0.039382\n",
            "batch 5576: loss 0.111007\n",
            "batch 5577: loss 0.025529\n",
            "batch 5578: loss 0.011858\n",
            "batch 5579: loss 0.009035\n",
            "batch 5580: loss 0.021367\n",
            "batch 5581: loss 0.046402\n",
            "batch 5582: loss 0.048427\n",
            "batch 5583: loss 0.060602\n",
            "batch 5584: loss 0.045096\n",
            "batch 5585: loss 0.066889\n",
            "batch 5586: loss 0.007812\n",
            "batch 5587: loss 0.008491\n",
            "batch 5588: loss 0.079829\n",
            "batch 5589: loss 0.048178\n",
            "batch 5590: loss 0.114819\n",
            "batch 5591: loss 0.008140\n",
            "batch 5592: loss 0.020097\n",
            "batch 5593: loss 0.033349\n",
            "batch 5594: loss 0.090955\n",
            "batch 5595: loss 0.072050\n",
            "batch 5596: loss 0.083107\n",
            "batch 5597: loss 0.073232\n",
            "batch 5598: loss 0.033625\n",
            "batch 5599: loss 0.019860\n",
            "batch 5600: loss 0.130205\n",
            "batch 5601: loss 0.042441\n",
            "batch 5602: loss 0.011331\n",
            "batch 5603: loss 0.067185\n",
            "batch 5604: loss 0.063597\n",
            "batch 5605: loss 0.019766\n",
            "batch 5606: loss 0.113664\n",
            "batch 5607: loss 0.017848\n",
            "batch 5608: loss 0.055765\n",
            "batch 5609: loss 0.021314\n",
            "batch 5610: loss 0.052319\n",
            "batch 5611: loss 0.009438\n",
            "batch 5612: loss 0.352099\n",
            "batch 5613: loss 0.005575\n",
            "batch 5614: loss 0.012366\n",
            "batch 5615: loss 0.221506\n",
            "batch 5616: loss 0.025672\n",
            "batch 5617: loss 0.010260\n",
            "batch 5618: loss 0.017140\n",
            "batch 5619: loss 0.010211\n",
            "batch 5620: loss 0.018053\n",
            "batch 5621: loss 0.024063\n",
            "batch 5622: loss 0.004975\n",
            "batch 5623: loss 0.113916\n",
            "batch 5624: loss 0.039785\n",
            "batch 5625: loss 0.031044\n",
            "batch 5626: loss 0.071025\n",
            "batch 5627: loss 0.005126\n",
            "batch 5628: loss 0.086714\n",
            "batch 5629: loss 0.098852\n",
            "batch 5630: loss 0.018399\n",
            "batch 5631: loss 0.082975\n",
            "batch 5632: loss 0.052074\n",
            "batch 5633: loss 0.039017\n",
            "batch 5634: loss 0.021494\n",
            "batch 5635: loss 0.048901\n",
            "batch 5636: loss 0.036463\n",
            "batch 5637: loss 0.034650\n",
            "batch 5638: loss 0.021334\n",
            "batch 5639: loss 0.047234\n",
            "batch 5640: loss 0.018396\n",
            "batch 5641: loss 0.035307\n",
            "batch 5642: loss 0.040173\n",
            "batch 5643: loss 0.073962\n",
            "batch 5644: loss 0.012510\n",
            "batch 5645: loss 0.082671\n",
            "batch 5646: loss 0.169513\n",
            "batch 5647: loss 0.093602\n",
            "batch 5648: loss 0.100731\n",
            "batch 5649: loss 0.027852\n",
            "batch 5650: loss 0.006375\n",
            "batch 5651: loss 0.030199\n",
            "batch 5652: loss 0.016496\n",
            "batch 5653: loss 0.016692\n",
            "batch 5654: loss 0.047786\n",
            "batch 5655: loss 0.018863\n",
            "batch 5656: loss 0.064103\n",
            "batch 5657: loss 0.084988\n",
            "batch 5658: loss 0.056205\n",
            "batch 5659: loss 0.050464\n",
            "batch 5660: loss 0.065076\n",
            "batch 5661: loss 0.013436\n",
            "batch 5662: loss 0.005832\n",
            "batch 5663: loss 0.008214\n",
            "batch 5664: loss 0.027529\n",
            "batch 5665: loss 0.037835\n",
            "batch 5666: loss 0.013789\n",
            "batch 5667: loss 0.064836\n",
            "batch 5668: loss 0.194736\n",
            "batch 5669: loss 0.017656\n",
            "batch 5670: loss 0.056544\n",
            "batch 5671: loss 0.043892\n",
            "batch 5672: loss 0.029404\n",
            "batch 5673: loss 0.028391\n",
            "batch 5674: loss 0.008671\n",
            "batch 5675: loss 0.059109\n",
            "batch 5676: loss 0.034738\n",
            "batch 5677: loss 0.011628\n",
            "batch 5678: loss 0.011854\n",
            "batch 5679: loss 0.042806\n",
            "batch 5680: loss 0.019636\n",
            "batch 5681: loss 0.017096\n",
            "batch 5682: loss 0.007121\n",
            "batch 5683: loss 0.128911\n",
            "batch 5684: loss 0.027620\n",
            "batch 5685: loss 0.022168\n",
            "batch 5686: loss 0.034820\n",
            "batch 5687: loss 0.081497\n",
            "batch 5688: loss 0.088560\n",
            "batch 5689: loss 0.036095\n",
            "batch 5690: loss 0.022495\n",
            "batch 5691: loss 0.060162\n",
            "batch 5692: loss 0.018317\n",
            "batch 5693: loss 0.012936\n",
            "batch 5694: loss 0.031421\n",
            "batch 5695: loss 0.051410\n",
            "batch 5696: loss 0.130821\n",
            "batch 5697: loss 0.111052\n",
            "batch 5698: loss 0.080892\n",
            "batch 5699: loss 0.052548\n",
            "batch 5700: loss 0.022645\n",
            "batch 5701: loss 0.036465\n",
            "batch 5702: loss 0.036251\n",
            "batch 5703: loss 0.182343\n",
            "batch 5704: loss 0.015695\n",
            "batch 5705: loss 0.022692\n",
            "batch 5706: loss 0.014402\n",
            "batch 5707: loss 0.061695\n",
            "batch 5708: loss 0.096841\n",
            "batch 5709: loss 0.108858\n",
            "batch 5710: loss 0.025335\n",
            "batch 5711: loss 0.025933\n",
            "batch 5712: loss 0.013240\n",
            "batch 5713: loss 0.034016\n",
            "batch 5714: loss 0.040688\n",
            "batch 5715: loss 0.028606\n",
            "batch 5716: loss 0.019489\n",
            "batch 5717: loss 0.090592\n",
            "batch 5718: loss 0.153770\n",
            "batch 5719: loss 0.027428\n",
            "batch 5720: loss 0.012416\n",
            "batch 5721: loss 0.024625\n",
            "batch 5722: loss 0.029031\n",
            "batch 5723: loss 0.008222\n",
            "batch 5724: loss 0.012318\n",
            "batch 5725: loss 0.132861\n",
            "batch 5726: loss 0.005518\n",
            "batch 5727: loss 0.043459\n",
            "batch 5728: loss 0.074717\n",
            "batch 5729: loss 0.028105\n",
            "batch 5730: loss 0.114344\n",
            "batch 5731: loss 0.082742\n",
            "batch 5732: loss 0.020629\n",
            "batch 5733: loss 0.026190\n",
            "batch 5734: loss 0.031326\n",
            "batch 5735: loss 0.092159\n",
            "batch 5736: loss 0.010239\n",
            "batch 5737: loss 0.037917\n",
            "batch 5738: loss 0.024271\n",
            "batch 5739: loss 0.093545\n",
            "batch 5740: loss 0.025842\n",
            "batch 5741: loss 0.026317\n",
            "batch 5742: loss 0.018943\n",
            "batch 5743: loss 0.097243\n",
            "batch 5744: loss 0.036723\n",
            "batch 5745: loss 0.041669\n",
            "batch 5746: loss 0.051950\n",
            "batch 5747: loss 0.013400\n",
            "batch 5748: loss 0.019673\n",
            "batch 5749: loss 0.089641\n",
            "batch 5750: loss 0.027195\n",
            "batch 5751: loss 0.027008\n",
            "batch 5752: loss 0.005545\n",
            "batch 5753: loss 0.028337\n",
            "batch 5754: loss 0.044735\n",
            "batch 5755: loss 0.046882\n",
            "batch 5756: loss 0.025818\n",
            "batch 5757: loss 0.039897\n",
            "batch 5758: loss 0.128173\n",
            "batch 5759: loss 0.022471\n",
            "batch 5760: loss 0.059070\n",
            "batch 5761: loss 0.024286\n",
            "batch 5762: loss 0.038863\n",
            "batch 5763: loss 0.034059\n",
            "batch 5764: loss 0.017503\n",
            "batch 5765: loss 0.054475\n",
            "batch 5766: loss 0.028753\n",
            "batch 5767: loss 0.094549\n",
            "batch 5768: loss 0.048286\n",
            "batch 5769: loss 0.006497\n",
            "batch 5770: loss 0.004283\n",
            "batch 5771: loss 0.021447\n",
            "batch 5772: loss 0.053985\n",
            "batch 5773: loss 0.044174\n",
            "batch 5774: loss 0.021353\n",
            "batch 5775: loss 0.015722\n",
            "batch 5776: loss 0.021734\n",
            "batch 5777: loss 0.008131\n",
            "batch 5778: loss 0.057938\n",
            "batch 5779: loss 0.038104\n",
            "batch 5780: loss 0.129401\n",
            "batch 5781: loss 0.059769\n",
            "batch 5782: loss 0.014462\n",
            "batch 5783: loss 0.004032\n",
            "batch 5784: loss 0.086126\n",
            "batch 5785: loss 0.100688\n",
            "batch 5786: loss 0.016751\n",
            "batch 5787: loss 0.026355\n",
            "batch 5788: loss 0.006336\n",
            "batch 5789: loss 0.019613\n",
            "batch 5790: loss 0.056768\n",
            "batch 5791: loss 0.027083\n",
            "batch 5792: loss 0.031848\n",
            "batch 5793: loss 0.008423\n",
            "batch 5794: loss 0.044992\n",
            "batch 5795: loss 0.043876\n",
            "batch 5796: loss 0.030269\n",
            "batch 5797: loss 0.027475\n",
            "batch 5798: loss 0.044083\n",
            "batch 5799: loss 0.181091\n",
            "batch 5800: loss 0.049432\n",
            "batch 5801: loss 0.108536\n",
            "batch 5802: loss 0.067413\n",
            "batch 5803: loss 0.048381\n",
            "batch 5804: loss 0.013460\n",
            "batch 5805: loss 0.027259\n",
            "batch 5806: loss 0.059262\n",
            "batch 5807: loss 0.025342\n",
            "batch 5808: loss 0.051562\n",
            "batch 5809: loss 0.019527\n",
            "batch 5810: loss 0.017938\n",
            "batch 5811: loss 0.083449\n",
            "batch 5812: loss 0.030404\n",
            "batch 5813: loss 0.046391\n",
            "batch 5814: loss 0.029512\n",
            "batch 5815: loss 0.005822\n",
            "batch 5816: loss 0.011200\n",
            "batch 5817: loss 0.026465\n",
            "batch 5818: loss 0.049419\n",
            "batch 5819: loss 0.103061\n",
            "batch 5820: loss 0.188416\n",
            "batch 5821: loss 0.053326\n",
            "batch 5822: loss 0.024109\n",
            "batch 5823: loss 0.014687\n",
            "batch 5824: loss 0.010567\n",
            "batch 5825: loss 0.035903\n",
            "batch 5826: loss 0.021216\n",
            "batch 5827: loss 0.008467\n",
            "batch 5828: loss 0.009673\n",
            "batch 5829: loss 0.243799\n",
            "batch 5830: loss 0.062365\n",
            "batch 5831: loss 0.021211\n",
            "batch 5832: loss 0.146248\n",
            "batch 5833: loss 0.089232\n",
            "batch 5834: loss 0.016914\n",
            "batch 5835: loss 0.021892\n",
            "batch 5836: loss 0.132067\n",
            "batch 5837: loss 0.040083\n",
            "batch 5838: loss 0.028628\n",
            "batch 5839: loss 0.064286\n",
            "batch 5840: loss 0.060234\n",
            "batch 5841: loss 0.044177\n",
            "batch 5842: loss 0.036658\n",
            "batch 5843: loss 0.010394\n",
            "batch 5844: loss 0.020646\n",
            "batch 5845: loss 0.100689\n",
            "batch 5846: loss 0.221580\n",
            "batch 5847: loss 0.050098\n",
            "batch 5848: loss 0.060402\n",
            "batch 5849: loss 0.048491\n",
            "batch 5850: loss 0.026327\n",
            "batch 5851: loss 0.077465\n",
            "batch 5852: loss 0.028882\n",
            "batch 5853: loss 0.025661\n",
            "batch 5854: loss 0.019545\n",
            "batch 5855: loss 0.004220\n",
            "batch 5856: loss 0.028807\n",
            "batch 5857: loss 0.047665\n",
            "batch 5858: loss 0.065618\n",
            "batch 5859: loss 0.097123\n",
            "batch 5860: loss 0.121330\n",
            "batch 5861: loss 0.050013\n",
            "batch 5862: loss 0.004896\n",
            "batch 5863: loss 0.117526\n",
            "batch 5864: loss 0.013492\n",
            "batch 5865: loss 0.046629\n",
            "batch 5866: loss 0.013708\n",
            "batch 5867: loss 0.056688\n",
            "batch 5868: loss 0.033129\n",
            "batch 5869: loss 0.050583\n",
            "batch 5870: loss 0.131269\n",
            "batch 5871: loss 0.063564\n",
            "batch 5872: loss 0.197288\n",
            "batch 5873: loss 0.035460\n",
            "batch 5874: loss 0.036456\n",
            "batch 5875: loss 0.034070\n",
            "batch 5876: loss 0.041650\n",
            "batch 5877: loss 0.158964\n",
            "batch 5878: loss 0.033177\n",
            "batch 5879: loss 0.087395\n",
            "batch 5880: loss 0.048514\n",
            "batch 5881: loss 0.013510\n",
            "batch 5882: loss 0.225410\n",
            "batch 5883: loss 0.020204\n",
            "batch 5884: loss 0.039905\n",
            "batch 5885: loss 0.028532\n",
            "batch 5886: loss 0.070594\n",
            "batch 5887: loss 0.017344\n",
            "batch 5888: loss 0.079528\n",
            "batch 5889: loss 0.182376\n",
            "batch 5890: loss 0.069704\n",
            "batch 5891: loss 0.009727\n",
            "batch 5892: loss 0.081355\n",
            "batch 5893: loss 0.015768\n",
            "batch 5894: loss 0.022055\n",
            "batch 5895: loss 0.014845\n",
            "batch 5896: loss 0.035318\n",
            "batch 5897: loss 0.038488\n",
            "batch 5898: loss 0.064665\n",
            "batch 5899: loss 0.067387\n",
            "batch 5900: loss 0.018879\n",
            "batch 5901: loss 0.012241\n",
            "batch 5902: loss 0.101117\n",
            "batch 5903: loss 0.067482\n",
            "batch 5904: loss 0.010751\n",
            "batch 5905: loss 0.063048\n",
            "batch 5906: loss 0.042977\n",
            "batch 5907: loss 0.036345\n",
            "batch 5908: loss 0.028348\n",
            "batch 5909: loss 0.035307\n",
            "batch 5910: loss 0.011035\n",
            "batch 5911: loss 0.050571\n",
            "batch 5912: loss 0.022847\n",
            "batch 5913: loss 0.019459\n",
            "batch 5914: loss 0.026346\n",
            "batch 5915: loss 0.013767\n",
            "batch 5916: loss 0.019450\n",
            "batch 5917: loss 0.314955\n",
            "batch 5918: loss 0.016102\n",
            "batch 5919: loss 0.028082\n",
            "batch 5920: loss 0.006671\n",
            "batch 5921: loss 0.005009\n",
            "batch 5922: loss 0.168950\n",
            "batch 5923: loss 0.011633\n",
            "batch 5924: loss 0.081371\n",
            "batch 5925: loss 0.016151\n",
            "batch 5926: loss 0.009190\n",
            "batch 5927: loss 0.030994\n",
            "batch 5928: loss 0.016563\n",
            "batch 5929: loss 0.048978\n",
            "batch 5930: loss 0.004758\n",
            "batch 5931: loss 0.050491\n",
            "batch 5932: loss 0.015649\n",
            "batch 5933: loss 0.115016\n",
            "batch 5934: loss 0.034583\n",
            "batch 5935: loss 0.015225\n",
            "batch 5936: loss 0.132869\n",
            "batch 5937: loss 0.172909\n",
            "batch 5938: loss 0.033885\n",
            "batch 5939: loss 0.059844\n",
            "batch 5940: loss 0.046278\n",
            "batch 5941: loss 0.053201\n",
            "batch 5942: loss 0.047344\n",
            "batch 5943: loss 0.096227\n",
            "batch 5944: loss 0.012732\n",
            "batch 5945: loss 0.021431\n",
            "batch 5946: loss 0.025190\n",
            "batch 5947: loss 0.022732\n",
            "batch 5948: loss 0.037576\n",
            "batch 5949: loss 0.014058\n",
            "batch 5950: loss 0.061662\n",
            "batch 5951: loss 0.015048\n",
            "batch 5952: loss 0.127732\n",
            "batch 5953: loss 0.029840\n",
            "batch 5954: loss 0.015319\n",
            "batch 5955: loss 0.008933\n",
            "batch 5956: loss 0.082603\n",
            "batch 5957: loss 0.063689\n",
            "batch 5958: loss 0.030949\n",
            "batch 5959: loss 0.028574\n",
            "batch 5960: loss 0.016017\n",
            "batch 5961: loss 0.041576\n",
            "batch 5962: loss 0.130563\n",
            "batch 5963: loss 0.038395\n",
            "batch 5964: loss 0.021679\n",
            "batch 5965: loss 0.097075\n",
            "batch 5966: loss 0.015806\n",
            "batch 5967: loss 0.062564\n",
            "batch 5968: loss 0.145282\n",
            "batch 5969: loss 0.102917\n",
            "batch 5970: loss 0.004196\n",
            "batch 5971: loss 0.101594\n",
            "batch 5972: loss 0.027467\n",
            "batch 5973: loss 0.041625\n",
            "batch 5974: loss 0.279225\n",
            "batch 5975: loss 0.005879\n",
            "batch 5976: loss 0.006860\n",
            "batch 5977: loss 0.027751\n",
            "batch 5978: loss 0.058147\n",
            "batch 5979: loss 0.009973\n",
            "batch 5980: loss 0.026853\n",
            "batch 5981: loss 0.057345\n",
            "batch 5982: loss 0.013990\n",
            "batch 5983: loss 0.018954\n",
            "batch 5984: loss 0.081452\n",
            "batch 5985: loss 0.090390\n",
            "batch 5986: loss 0.028825\n",
            "batch 5987: loss 0.010995\n",
            "batch 5988: loss 0.021820\n",
            "batch 5989: loss 0.045579\n",
            "batch 5990: loss 0.010726\n",
            "batch 5991: loss 0.043033\n",
            "batch 5992: loss 0.099165\n",
            "batch 5993: loss 0.136735\n",
            "batch 5994: loss 0.029714\n",
            "batch 5995: loss 0.029681\n",
            "batch 5996: loss 0.058687\n",
            "batch 5997: loss 0.058215\n",
            "batch 5998: loss 0.051265\n",
            "batch 5999: loss 0.018232\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFWxzycAB7DL",
        "colab_type": "text"
      },
      "source": [
        "## 模型的评估： tf.keras.metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9zcMPPCCDzx",
        "colab_type": "text"
      },
      "source": [
        "最后，我们使用测试集评估模型的性能。这里，我们使用 **tf.keras.metrics** 中的 **SparseCategoricalAccuracy** 评估器来评估模型在测试集上的性能，该评估器能够对模型预测的结果与真实结果进行比较，并输出预测正确的样本数占总样本数的比例。我们迭代测试数据集，每次通过 **update_state()** 方法向评估器输入两个参数： y_pred 和 y_true ，即模型预测出的结果和真实结果。评估器具有内部变量来保存当前评估指标相关的参数数值（例如当前已传入的累计样本数和当前预测正确的样本数）。迭代结束后，我们使用 result() 方法输出最终的评估指标值（预测正确的样本数占总样本数的比例）。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCzxDC9aB9UD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e7023362-5b8c-4854-8834-16663b1a746a"
      },
      "source": [
        "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "num_batches = int(data_loader.num_test_data//batch_size)\n",
        "for batch_index in range(num_batches):\n",
        "    start_index,end_index = batch_index*batch_size,(batch_index+1)*batch_size\n",
        "    y_pred = model.predict(data_loader.test_data[start_index:end_index])\n",
        "    sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index:end_index],y_pred=y_pred)\n",
        "print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())   "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test accuracy: 0.972300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6YuQ6WrDa7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}